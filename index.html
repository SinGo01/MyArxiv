<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                SinGo's Arxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-07T00:00:00Z">2024-10-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">142</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Advisor: Dynamic Data Curation for Safety Alignment of Large
  Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Wang, Ninareh Mehrabi, Palash Goyal, Rahul Gupta, Kai-Wei Chang, Aram Galstyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data is a crucial element in large language model (LLM) alignment. Recent
studies have explored using LLMs for efficient data collection. However,
LLM-generated data often suffers from quality issues, with underrepresented or
absent aspects and low-quality datapoints. To address these problems, we
propose Data Advisor, an enhanced LLM-based method for generating data that
takes into account the characteristics of the desired dataset. Starting from a
set of pre-defined principles in hand, Data Advisor monitors the status of the
generated data, identifies weaknesses in the current dataset, and advises the
next iteration of data generation accordingly. Data Advisor can be easily
integrated into existing data generation methods to enhance data quality and
coverage. Experiments on safety alignment of three representative LLMs (i.e.,
Mistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in
enhancing model safety against various fine-grained safety issues without
sacrificing model utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference. Project website:
  https://feiwang96.github.io/DataAdvisor/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding Partially-Defined Events in Multimodal Data <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kate Sanders, Reno Kriz, David Etter, Hannah Recknor, Alexander Martin, Cameron Carpenter, Jingyang Lin, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How are we able to learn about complex current events just from short
snippets of video? While natural language enables straightforward ways to
represent under-specified, partially observable events, visual data does not
facilitate analogous methods and, consequently, introduces unique challenges in
event understanding. With the growing prevalence of vision-capable AI agents,
these systems must be able to model events from collections of unstructured
video data. To tackle robust event modeling in multimodal settings, we
introduce a multimodal formulation for partially-defined events and cast the
extraction of these events as a three-stage span retrieval task. We propose a
corresponding benchmark for this task, MultiVENT-G, that consists of 14.5 hours
of densely annotated current event videos and 1,168 text documents, containing
22.8K labeled event-centric entities. We propose a collection of LLM-driven
approaches to the task of multimodal event analysis, and evaluate them on
MultiVENT-G. Results illustrate the challenges that abstract event
understanding poses and demonstrates promise in event-centric video-language
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint; 9 pages; 2024 EMNLP Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers
  in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengzhao Chen, Yi Liu, Jiahao Wang, Yi Bin, Wenqi Shao, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization is essential for deploying Large Language Models (LLMs) by
enhancing memory efficiency and inference speed. Existing methods for
activation quantization mainly address channel-wise outliers, often neglecting
token-wise outliers, leading to reliance on costly per-token dynamic
quantization. To address this, we introduce PrefixQuant, a novel technique that
isolates outlier tokens offline without re-training. Specifically, PrefixQuant
identifies high-frequency outlier tokens and prefixes them in the KV cache,
preventing the generation of outlier tokens during inference and simplifying
quantization. To our knowledge, PrefixQuant is the first to enable efficient
per-tensor static quantization to outperform expensive per-token dynamic
quantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and
4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization
achieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5
common-sense reasoning tasks, outperforming previous per-token dynamic
quantization methods like QuaRot with 0.98 perplexity improvement and +5.98
points accuracy. Additionally, the inference speed of W4A4 quantized models
using PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot
models by 1.2x to 1.3x. Our code is available at
\url{https://github.com/ChenMnZ/PrefixQuant}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A PTQ method to significantly boost the performance of static
  activation quantization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TurtleBench: Evaluating Top Language Models via Real-World Yes/No
  Puzzles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingchen Yu, Shichao Song, Ke Fang, Yunfeng Shi, Zifan Zheng, Hanyu Wang, Simin Niu, Zhiyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the application of Large Language Models (LLMs) expands, the demand for
reliable evaluations increases. Existing LLM evaluation benchmarks primarily
rely on static datasets, making it challenging to assess model performance in
dynamic interactions with users. Moreover, these benchmarks often depend on
specific background knowledge, complicating the measurement of a model's
logical reasoning capabilities. Other dynamic evaluation methods based on
strong models or manual efforts may introduce biases and incur high costs and
time demands, hindering large-scale application. To address these issues, we
propose TurtleBench. TurtleBench collects real user guesses from our online
Turtle Soup Puzzle platform that we developed. This approach allows for the
relatively dynamic generation of evaluation datasets, mitigating the risk of
model cheating while aligning assessments more closely with genuine user needs
for reasoning capabilities, thus enhancing the reliability of evaluations.
TurtleBench includes 1,532 user guesses along with the correctness of guesses
after annotation. Using this dataset, we thoroughly evaluated nine of the most
advanced LLMs available today. Notably, the OpenAI o1 series models did not
achieve leading results in these evaluations. We propose several hypotheses for
further research, such as "the latent reasoning of o1 utilizes trivial
Chain-of-Thought (CoT) techniques" and "increasing CoT length not only provides
reasoning benefits but also incurs noise costs."
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differential <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer tends to overallocate attention to irrelevant context. In this
work, we introduce Diff Transformer, which amplifies attention to the relevant
context while canceling noise. Specifically, the differential attention
mechanism calculates attention scores as the difference between two separate
softmax attention maps. The subtraction cancels noise, promoting the emergence
of sparse attention patterns. Experimental results on language modeling show
that Diff Transformer outperforms Transformer in various settings of scaling up
model size and training tokens. More intriguingly, it offers notable advantages
in practical applications, such as long-context modeling, key information
retrieval, hallucination mitigation, in-context learning, and reduction of
activation outliers. By being less distracted by irrelevant context, Diff
Transformer can mitigate hallucination in question answering and text
summarization. For in-context learning, Diff Transformer not only enhances
accuracy but is also more robust to order permutation, which was considered as
a chronic robustness issue. The results position Diff Transformer as a highly
effective and promising architecture to advance large language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLEE: A Unified Framework and Benchmark for Language-based Economic
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eilam Shapira, Omer Madmon, Itamar Reinman, Samuel Joseph Amouyal, Roi Reichart, Moshe Tennenholtz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) show significant potential in economic and
strategic interactions, where communication via natural language is often
prevalent. This raises key questions: Do LLMs behave rationally? Can they mimic
human behavior? Do they tend to reach an efficient and fair outcome? What is
the role of natural language in the strategic interaction? How do
characteristics of the economic environment influence these dynamics? These
questions become crucial concerning the economic and societal implications of
integrating LLM-based agents into real-world data-driven systems, such as
online retail platforms and recommender systems. While the ML community has
been exploring the potential of LLMs in such multi-agent setups, varying
assumptions, design choices and evaluation criteria across studies make it
difficult to draw robust and meaningful conclusions. To address this, we
introduce a benchmark for standardizing research on two-player, sequential,
language-based games. Inspired by the economic literature, we define three base
families of games with consistent parameterization, degrees of freedom and
economic measures to evaluate agents' performance (self-gain), as well as the
game outcome (efficiency and fairness). We develop an open-source framework for
interaction simulation and analysis, and utilize it to collect a dataset of LLM
vs. LLM interactions across numerous game configurations and an additional
dataset of human vs. LLM interactions. Through extensive experimentation, we
demonstrate how our framework and dataset can be used to: (i) compare the
behavior of LLM-based agents to human players in various economic contexts;
(ii) evaluate agents in both individual and collective performance measures;
and (iii) quantify the effect of the economic characteristics of the
environments on the behavior of agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Micro-Narratives <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mourad Heddaya, Qingcheng Zeng, Chenhao Tan, Rob Voigt, Alexander Zentefis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to classify causal micro-narratives from text.
These narratives are sentence-level explanations of the cause(s) and/or
effect(s) of a target subject. The approach requires only a subject-specific
ontology of causes and effects, and we demonstrate it with an application to
inflation narratives. Using a human-annotated dataset spanning historical and
contemporary US news articles for training, we evaluate several large language
models (LLMs) on this multi-label classification task. The best-performing
model--a fine-tuned Llama 3.1 8B--achieves F1 scores of 0.87 on narrative
detection and 0.71 on narrative classification. Comprehensive error analysis
reveals challenges arising from linguistic ambiguity and highlights how model
errors often mirror human annotator disagreements. This research establishes a
framework for extracting causal micro-narratives from real-world data, with
wide-ranging applications to social science research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Workshop on Narrative Understanding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Xiao, Shujian Zhang, Wenxuan Zhou, Marzyeh Ghassemi, Sanqiang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To induce desired behaviors in large language models (LLMs) for
interaction-driven tasks, the instruction-tuning stage typically trains LLMs on
instruction-response pairs using the next-token prediction (NTP) loss. Previous
work aiming to improve instruction-tuning performance often emphasizes the need
for higher-quality supervised fine-tuning (SFT) datasets, which typically
involves expensive data filtering with proprietary LLMs or labor-intensive data
generation by human annotators. However, these approaches do not fully leverage
the datasets' intrinsic properties, resulting in high computational and labor
costs, thereby limiting scalability and performance gains. In this paper, we
propose SFTMix, a novel recipe that elevates instruction-tuning performance
beyond the conventional NTP paradigm, without the need for well-curated
datasets. Observing that LLMs exhibit uneven confidence across the semantic
representation space, we argue that examples with different confidence levels
should play distinct roles during the instruction-tuning process. Based on this
insight, SFTMix leverages training dynamics to identify examples with varying
confidence levels, then applies a Mixup-based regularization to mitigate
overfitting on confident examples while propagating supervision signals to
improve learning on relatively unconfident ones. This approach enables SFTMix
to significantly outperform NTP across a wide range of instruction-following
and healthcare domain-specific SFT tasks, demonstrating its adaptability to
diverse LLM families and scalability to datasets of any size. Comprehensive
ablation studies further verify the robustness of SFTMix's design choices,
underscoring its versatility in consistently enhancing performance across
different LLMs and datasets in broader natural language processing
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Navigating the Digital World as Humans Do: Universal Visual Grounding
  for GUI Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) are transforming the capabilities of
graphical user interface (GUI) agents, facilitating their transition from
controlled simulations to complex, real-world applications across various
platforms. However, the effectiveness of these agents hinges on the robustness
of their grounding capability. Current GUI agents predominantly utilize
text-based representations such as HTML or accessibility trees, which, despite
their utility, often introduce noise, incompleteness, and increased
computational overhead. In this paper, we advocate a human-like embodiment for
GUI agents that perceive the environment entirely visually and directly take
pixel-level operations on the GUI. The key is visual grounding models that can
accurately map diverse referring expressions of GUI elements to their
coordinates on the GUI across different platforms. We show that a simple
recipe, which includes web-based synthetic data and slight adaptation of the
LLaVA architecture, is surprisingly effective for training such visual
grounding models. We collect the largest dataset for GUI visual grounding so
far, containing 10M GUI elements and their referring expressions over 1.3M
screenshots, and use it to train UGround, a strong universal visual grounding
model for GUI agents. Empirical results on six benchmarks spanning three
categories (grounding, offline agent, and online agent) show that 1) UGround
substantially outperforms existing visual grounding models for GUI agents, by
up to 20% absolute, and 2) agents with UGround outperform state-of-the-art
agents, despite the fact that existing agents use additional text-based input
while ours only uses visual perception. These results provide strong support
for the feasibility and promises of GUI agents that navigate the digital world
as humans do.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TuneVLSeg: <span class="highlight-title">Prompt</span> Tuning Benchmark for Vision-Language Segmentation
  Models <span class="chip">ACCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rabin Adhikari, Safal Thapaliya, Manish Dhakal, Bishesh Khanal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) have shown impressive performance in vision
tasks, but adapting them to new domains often requires expensive fine-tuning.
Prompt tuning techniques, including textual, visual, and multimodal prompting,
offer efficient alternatives by leveraging learnable prompts. However, their
application to Vision-Language Segmentation Models (VLSMs) and evaluation under
significant domain shifts remain unexplored. This work presents an open-source
benchmarking framework, TuneVLSeg, to integrate various unimodal and multimodal
prompt tuning techniques into VLSMs, making prompt tuning usable for downstream
segmentation datasets with any number of classes. TuneVLSeg includes $6$ prompt
tuning strategies on various prompt depths used in $2$ VLSMs totaling of $8$
different combinations. We test various prompt tuning on $8$ diverse medical
datasets, including $3$ radiology datasets (breast tumor, echocardiograph,
chest X-ray pathologies) and $5$ non-radiology datasets (polyp, ulcer, skin
cancer), and two natural domain segmentation datasets. Our study found that
textual prompt tuning struggles under significant domain shifts, from
natural-domain images to medical data. Furthermore, visual prompt tuning, with
fewer hyperparameters than multimodal prompt tuning, often achieves performance
competitive to multimodal approaches, making it a valuable first attempt. Our
work advances the understanding and applicability of different prompt-tuning
techniques for robust domain-specific segmentation. The source code is
available at https://github.com/naamiinepal/tunevlseg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACCV 2024 (oral presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CasiMedicos-Arg: A Medical Question Answering <span class="highlight-title">Dataset</span> Annotated with
  Explanatory Argumentative Structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        katerina Sviridova, Anar Yeginbergen, Ainara Estarrona, Elena Cabrio, Serena Villata, Rodrigo Agerri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explaining Artificial Intelligence (AI) decisions is a major challenge
nowadays in AI, in particular when applied to sensitive scenarios like medicine
and law. However, the need to explain the rationale behind decisions is a main
issue also for human-based deliberation as it is important to justify
\textit{why} a certain decision has been taken. Resident medical doctors for
instance are required not only to provide a (possibly correct) diagnosis, but
also to explain how they reached a certain conclusion. Developing new tools to
aid residents to train their explanation skills is therefore a central
objective of AI in education. In this paper, we follow this direction, and we
present, to the best of our knowledge, the first multilingual dataset for
Medical Question Answering where correct and incorrect diagnoses for a clinical
case are enriched with a natural language explanation written by doctors. These
explanations have been manually annotated with argument components (i.e.,
premise, claim) and argument relations (i.e., attack, support), resulting in
the Multilingual CasiMedicos-Arg dataset which consists of 558 clinical cases
in four languages (English, Spanish, French, Italian) with explanations, where
we annotated 5021 claims, 2313 premises, 2431 support relations, and 1106
attack relations. We conclude by showing how competitive baselines perform over
this challenging dataset for the argument mining task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cookbook: A framework for improving LLM generative abilities via
  programmatic data generating templates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avanika Narayan, Mayee F. Chen, Kush Bhatia, Christopher Ré
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) on instruction datasets is a common
way to improve their generative capabilities. However, instruction datasets can
be expensive and time-consuming to manually curate, and while LLM-generated
data is less labor-intensive, it may violate user privacy agreements or terms
of service of LLM providers. Therefore, we seek a way of constructing
instruction datasets with samples that are not generated by humans or LLMs but
still improve LLM generative capabilities. In this work, we introduce Cookbook,
a framework that programmatically generates training data consisting of simple
patterns over random tokens, resulting in a scalable, cost-effective approach
that avoids legal and privacy issues. First, Cookbook uses a template -- a data
generating Python function -- to produce training data that encourages the
model to learn an explicit pattern-based rule that corresponds to a desired
task. We find that fine-tuning on Cookbook-generated data is able to improve
performance on its corresponding task by up to 52.7 accuracy points. Second,
since instruction datasets improve performance on multiple downstream tasks
simultaneously, Cookbook algorithmically learns how to mix data from various
templates to optimize performance on multiple tasks. On the standard multi-task
GPT4ALL evaluation suite, Mistral-7B fine-tuned using a Cookbook-generated
dataset attains the best accuracy on average compared to other 7B parameter
instruction-tuned models and is the best performing model on 3 out of 8 tasks.
Finally, we analyze when and why Cookbook improves performance and present a
metric that allows us to verify that the improvement is largely explained by
the model's generations adhering better to template rules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Precise Model Benchmarking with Only a Few Observations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Fogliato, Pratik Patil, Nil-Jana Akpinar, Mathew Monfort
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we precisely estimate a large language model's (LLM) accuracy on
questions belonging to a specific topic within a larger question-answering
dataset? The standard direct estimator, which averages the model's accuracy on
the questions in each subgroup, may exhibit high variance for subgroups
(topics) with small sample sizes. Synthetic regression modeling, which
leverages the model's accuracy on questions about other topics, may yield
biased estimates that are too unreliable for large subgroups. We prescribe a
simple yet effective solution: an empirical Bayes (EB) estimator that balances
direct and regression estimates for each subgroup separately, improving the
precision of subgroup-level estimates of model performance. Our experiments on
multiple datasets show that this approach consistently provides more precise
estimates of the LLM performance compared to the direct and regression
approaches, achieving substantial reductions in the mean squared error.
Confidence intervals for EB estimates also have near-nominal coverage and are
narrower compared to those for the direct estimator. Additional experiments on
tabular and vision data validate the benefits of this EB approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Density estimation with LLMs: a geometric investigation of in-context
  learning trajectories <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toni J. B. Liu, Nicolas Boullé, Raphaël Sarfati, Christopher J. Earls
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate remarkable emergent abilities to
perform in-context learning across various tasks, including time series
forecasting. This work investigates LLMs' ability to estimate probability
density functions (PDFs) from data observed in-context; such density estimation
(DE) is a fundamental task underlying many probabilistic modeling problems. We
leverage the Intensive Principal Component Analysis (InPCA) to visualize and
analyze the in-context learning dynamics of LLaMA-2 models. Our main finding is
that these LLMs all follow similar learning trajectories in a low-dimensional
InPCA space, which are distinct from those of traditional density estimation
methods like histograms and Gaussian kernel density estimation (KDE). We
interpret the LLaMA in-context DE process as a KDE with an adaptive kernel
width and shape. This custom kernel model captures a significant portion of
LLaMA's behavior despite having only two parameters. We further speculate on
why LLaMA's kernel width and shape differs from classical algorithms, providing
insights into the mechanism of in-context probabilistic reasoning in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preserving Multi-Modal Capabilities of <span class="highlight-title">Pre-train</span>ed VLMs for Improving
  Vision-Linguistic Compositionality <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngtaek Oh, Jae Won Cho, Dong-Jin Kim, In So Kweon, Junmo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new method to enhance compositional understanding
in pre-trained vision and language models (VLMs) without sacrificing
performance in zero-shot multi-modal tasks. Traditional fine-tuning approaches
often improve compositional reasoning at the cost of degrading multi-modal
capabilities, primarily due to the use of global hard negative (HN) loss, which
contrasts global representations of images and texts. This global HN loss
pushes HN texts that are highly similar to the original ones, damaging the
model's multi-modal representations. To overcome this limitation, we propose
Fine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard
negative loss and selective calibrated regularization. These innovations
provide fine-grained negative supervision while preserving the model's
representational integrity. Our extensive evaluations across diverse benchmarks
for both compositionality and multi-modal tasks show that FSC-CLIP not only
achieves compositionality on par with state-of-the-art models but also retains
strong multi-modal capabilities. Code is available at:
https://github.com/ytaek-oh/fsc-clip.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Long, Main). Project page:
  https://ytaek-oh.github.io/fsc-clip</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Studying and Mitigating Biases in Sign Language Understanding Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katherine Atwell, Danielle Bragg, Malihe Alikhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring that the benefits of sign language technologies are distributed
equitably among all community members is crucial. Thus, it is important to
address potential biases and inequities that may arise from the design or use
of these resources. Crowd-sourced sign language datasets, such as the ASL
Citizen dataset, are great resources for improving accessibility and preserving
linguistic diversity, but they must be used thoughtfully to avoid reinforcing
existing biases.
  In this work, we utilize the rich information about participant demographics
and lexical features present in the ASL Citizen dataset to study and document
the biases that may result from models trained on crowd-sourced sign datasets.
Further, we apply several bias mitigation techniques during model training, and
find that these techniques reduce performance disparities without decreasing
accuracy. With the publication of this work, we release the demographic
information about the participants in the ASL Citizen dataset to encourage
future bias mitigation work in this space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RevisEval: Improving LLM-as-a-Judge via Response-Adapted References 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyuan Zhang, Yufei Wang, Tiezheng YU, Yuxin Jiang, Chuhan Wu, Liangyou Li, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Fuyuan Lyu, Chen Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With significant efforts in recent studies, LLM-as-a-Judge has become a
cost-effective alternative to human evaluation for assessing the text
generation quality in a wide range of tasks. However, there still remains a
reliability gap between LLM-as-a-Judge and human evaluation. One important
reason is the lack of guided oracles in the evaluation process. Motivated by
the role of reference pervasively used in classic text evaluation, we introduce
RevisEval, a novel text generation evaluation paradigm via the response-adapted
references. RevisEval is driven by the key observation that an ideal reference
should maintain the necessary relevance to the response to be evaluated.
Specifically, RevisEval leverages the text revision capabilities of large
language models (LLMs) to adaptively revise the response, then treat the
revised text as the reference (response-adapted reference) for the subsequent
evaluation. Extensive experiments demonstrate that RevisEval outperforms
traditional reference-free and reference-based evaluation paradigms that use
LLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks.
More importantly, our response-adapted references can further boost the
classical text metrics, e.g., BLEU and BERTScore, compared to traditional
references and even rival the LLM-as-a-Judge. A detailed analysis is also
conducted to confirm RevisEval's effectiveness in bias reduction, the impact of
inference cost, and reference relevance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss
  Landscape Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyue Wen, Zhiyuan Li, Jason Wang, David Hall, Percy Liang, Tengyu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training language models currently requires pre-determining a fixed compute
budget because the typical cosine learning rate schedule depends on the total
number of steps. In contrast, the Warmup-Stable-Decay (WSD) schedule uses a
constant learning rate to produce a main branch of iterates that can in
principle continue indefinitely without a pre-specified compute budget. Then,
given any compute budget, one can branch out from the main branch at a proper
at any time with a rapidly decaying learning rate to produce a strong model.
Empirically, WSD generates a non-traditional loss curve: the loss remains
elevated during the stable phase but sharply declines during the decay phase.
Towards explaining this phenomenon, we conjecture that pretraining loss
exhibits a river valley landscape, which resembles a deep valley with a river
at its bottom. Under this assumption, we show that during the stable phase, the
iterate undergoes large oscillations due to the high learning rate, yet it
progresses swiftly along the river. During the decay phase, the rapidly
dropping learning rate minimizes the iterate's oscillations, moving it closer
to the river and revealing true optimization progress. Therefore, the sustained
high learning rate phase and fast decaying phase are responsible for progress
in the river and the mountain directions respectively, and are both critical.
Our analysis predicts phenomenons consistent with empirical observations and
shows that this landscape can emerge from pretraining on a simple bi-gram
dataset. Inspired by the theory, we introduce WSD-S, a variant of WSD that
reuses previous checkpoints' decay phases and keeps only one main branch, where
we resume from a decayed checkpoint. WSD-S empirically outperforms WSD and
Cyclic-Cosine in obtaining multiple language model checkpoints across various
compute budgets in a single run for parameters scaling from 0.1B to 1.2B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages,13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Correlation: Interpretable Evaluation of Machine Translation
  Metrics <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Perrella, Lorenzo Proietti, Pere-Lluís Huguet Cabot, Edoardo Barba, Roberto Navigli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Translation (MT) evaluation metrics assess translation quality
automatically. Recently, researchers have employed MT metrics for various new
use cases, such as data filtering and translation re-ranking. However, most MT
metrics return assessments as scalar scores that are difficult to interpret,
posing a challenge to making informed design choices. Moreover, MT metrics'
capabilities have historically been evaluated using correlation with human
judgment, which, despite its efficacy, falls short of providing intuitive
insights into metric performance, especially in terms of new metric use cases.
To address these issues, we introduce an interpretable evaluation framework for
MT metrics. Within this framework, we evaluate metrics in two scenarios that
serve as proxies for the data filtering and translation re-ranking use cases.
Furthermore, by measuring the performance of MT metrics using Precision,
Recall, and F-score, we offer clearer insights into their capabilities than
correlation with human judgments. Finally, we raise concerns regarding the
reliability of manually curated data following the Direct Assessments+Scalar
Quality Metrics (DA+SQM) guidelines, reporting a notably low agreement with
Multidimensional Quality Metrics (MQM) annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Main Conference. 26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Equity in Large Language Models for Medical Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuelyu Ji, Wenhe Ma, Sonish Sivarajkumar, Hang Zhang, Eugene Mathew Sadhu, Zhuochun Li, Xizhi Wu, Shyam Visweswaran, Yanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements have highlighted the potential of large language models
(LLMs) in medical applications, notably in automating Clinical Trial Matching
for translational research and providing medical question-answering for
clinical decision support. However, our study reveals significant inequities in
the use of LLMs, particularly for individuals from specific racial, gender, and
underrepresented groups influenced by social determinants of health. These
disparities could worsen existing health inequities if LLMs are broadly adopted
in healthcare. To address this, we propose and evaluate a novel framework,
EquityGuard, designed to detect and mitigate biases in LLM-based medical
applications. EquityGuard incorporates a Bias Detection Mechanism capable of
identifying and correcting unfair predictions, thus enhancing outcomes and
promoting equity across diverse population groups.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReasoningRank: Teaching Student Models to Rank through Reasoning-Based
  Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuelyu Ji, Zhuochun Li, Rui Meng, Daqing He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reranking documents based on their relevance to a given query is critical in
information retrieval. Traditional reranking methods often focus on improving
the initial rankings but lack transparency, failing to explain why one document
is ranked higher. In this paper, we introduce ReasoningRank, a novel reranking
approach that enhances clarity by generating two types of reasoning: explicit
reasoning, which explains how a document addresses the query, and comparison
reasoning, which justifies the relevance of one document over another. We
leverage large language models (LLMs) as teacher models to generate these
explanations and distill this knowledge into smaller, more resource-efficient
student models. While the student models may not outperform LLMs in speed, they
significantly reduce the computational burden by requiring fewer resources,
making them more suitable for large-scale or resource-constrained settings.
These student models are trained to both generate meaningful reasoning and
rerank documents, achieving competitive performance across multiple datasets,
including MSMARCO and BRIGHT. Experiments demonstrate that ReasoningRank
improves reranking accuracy and provides valuable insights into the
decision-making process, offering a structured and interpretable solution for
reranking tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Inference for Large Language Model-based Generative
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Lin, Chaoqun Yang, Wenjie Wang, Yongqi Li, Cunxiao Du, Fuli Feng, See-Kiong Ng, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM)-based generative recommendation has achieved
notable success, yet its practical deployment is costly particularly due to
excessive inference latency caused by autoregressive decoding. For lossless LLM
decoding acceleration, Speculative Decoding (SD) has emerged as a promising
solution. However, applying SD to generative recommendation presents unique
challenges due to the requirement of generating top-K items (i.e., K distinct
token sequences) as a recommendation list by beam search. This leads to more
stringent verification in SD, where all the top-K sequences from the target LLM
must be successfully drafted by the draft model at each decoding step. To
alleviate this, we consider 1) boosting top-K sequence alignment between the
draft model and the target LLM, and 2) relaxing the verification strategy to
reduce trivial LLM calls. To this end, we propose an alignment framework named
AtSpeed, which presents the AtSpeed-S optimization objective for top-K
alignment under the strict top-K verification. Moreover, we introduce a relaxed
sampling verification strategy that allows high-probability non-top-K drafted
sequences to be accepted, significantly reducing LLM calls. Correspondingly, we
propose AtSpeed-R for top-K alignment under this relaxed sampling verification.
Empirical results on two real-world datasets demonstrate that AtSpeed
significantly accelerates LLM-based generative recommendation, e.g., near 2x
speedup under strict top-K verification and up to 2.5 speedup under relaxed
sampling verification. The codes and datasets will be released in the near
future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deciphering the Interplay of Parametric and Non-parametric Memory in
  Retrieval-augmented Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehrdad Farahani, Richard Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative language models often struggle with specialized or less-discussed
knowledge. A potential solution is found in Retrieval-Augmented Generation
(RAG) models which act like retrieving information before generating responses.
In this study, we explore how the \textsc{Atlas} approach, a RAG model, decides
between what it already knows (parametric) and what it retrieves
(non-parametric). We use causal mediation analysis and controlled experiments
to examine how internal representations influence information processing. Our
findings disentangle the effects of parametric knowledge and the retrieved
context. They indicate that in cases where the model can choose between both
types of information (parametric and non-parametric), it relies more on the
context than the parametric knowledge. Furthermore, the analysis investigates
the computations involved in \emph{how} the model uses the information from the
context. We find that multiple mechanisms are active within the model and can
be detected with mediation analysis: first, the decision of \emph{whether the
context is relevant}, and second, how the encoder computes output
representations to support copying when relevant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLM2Vec: Training Vision-Language Models for Massive Multimodal
  Embedding Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding models have been crucial in enabling various downstream tasks such
as semantic similarity, information retrieval, and clustering. Recently, there
has been a surge of interest in developing universal text embedding models that
can generalize across tasks (e.g., MTEB). However, progress in learning
universal multimodal embedding models has been relatively slow despite their
importance. In this work, we aim to explore the potential for building
universal embeddings capable of handling a wide range of downstream tasks. Our
contributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark),
which covers 4 meta-tasks (i.e. classification, visual question answering,
multimodal retrieval, and visual grounding) and 36 datasets, including 20
training and 16 evaluation datasets, and (2) VLM2Vec (Vision-Language Model ->
Vector), a contrastive training framework that converts any state-of-the-art
vision-language model into an embedding model via training on MMEB. Unlike
previous models such as CLIP and BLIP, VLM2Vec can process any combination of
images and text to generate a fixed-dimensional vector based on task
instructions. We build a series of VLM2Vec models on Phi-3.5-V and evaluate
them on MMEB's evaluation split. Our results show that \model achieves an
absolute average improvement of 10% to 20% over existing multimodal embedding
models on both in-distribution and out-of-distribution datasets in MMEB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CTC-GMM: CTC guided modality matching for fast and accurate streaming
  speech translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Zhao, Jinyu Li, Ruchao Fan, Matt Post
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models for streaming speech translation (ST) can achieve high accuracy and
low latency if they're developed with vast amounts of paired audio in the
source language and written text in the target language. Yet, these text labels
for the target language are often pseudo labels due to the prohibitive cost of
manual ST data labeling. In this paper, we introduce a methodology named
Connectionist Temporal Classification guided modality matching (CTC-GMM) that
enhances the streaming ST model by leveraging extensive machine translation
(MT) text data. This technique employs CTC to compress the speech sequence into
a compact embedding sequence that matches the corresponding text sequence,
allowing us to utilize matched {source-target} language text pairs from the MT
corpora to refine the streaming ST model further. Our evaluations with FLEURS
and CoVoST2 show that the CTC-GMM approach can increase translation accuracy
relatively by 13.9% and 6.4% respectively, while also boosting decoding speed
by 59.7% on GPU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Spoken Language Technology Workshop (SLT 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SparsePO: Controlling Preference Alignment of LLMs via Sparse Token
  Masks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fenia Christopoulou, Ronald Cardenas, Gerasimos Lampouras, Haitham Bou-Ammar, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference Optimization (PO) has proven an effective step for aligning
language models to human-desired behaviors. Current variants, following the
offline Direct Preference Optimization objective, have focused on a strict
setting where all tokens are contributing signals of KL divergence and rewards
to the loss function. However, human preference is not affected by each word in
a sequence equally but is often dependent on specific words or phrases, e.g.
existence of toxic terms leads to non-preferred responses. Based on this
observation, we argue that not all tokens should be weighted equally during PO
and propose a flexible objective termed SparsePO, that aims to automatically
learn to weight the KL divergence and reward corresponding to each token during
PO training. We propose two different variants of weight-masks that can either
be derived from the reference model itself or learned on the fly. Notably, our
method induces sparsity in the learned masks, allowing the model to learn how
to best weight reward and KL divergence contributions at the token level,
learning an optimal level of mask sparsity. Extensive experiments on multiple
domains, including sentiment control, dialogue, text summarization and
text-to-code generation, illustrate that our approach assigns meaningful
weights to tokens according to the target task, generates more responses with
the desired preference and improves reasoning tasks by up to 2 percentage
points compared to other token- and response-level PO methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 papges, 9 figures, 5 tables. Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating large language models for their competence in extracting
  grammatically sound sentences from transcribed noisy utterances <span class="chip">CoNLL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alina Wróblewska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Selectively processing noisy utterances while effectively disregarding
speech-specific elements poses no considerable challenge for humans, as they
exhibit remarkable cognitive abilities to separate semantically significant
content from speech-specific noise (i.e. filled pauses, disfluencies, and
restarts). These abilities may be driven by mechanisms based on acquired
grammatical rules that compose abstract syntactic-semantic structures within
utterances. Segments without syntactic and semantic significance are
consistently disregarded in these structures. The structures, in tandem with
lexis, likely underpin language comprehension and thus facilitate effective
communication. In our study, grounded in linguistically motivated experiments,
we investigate whether large language models (LLMs) can effectively perform
analogical speech comprehension tasks. In particular, we examine the ability of
LLMs to extract well-structured utterances from transcriptions of noisy
dialogues. We conduct two evaluation experiments in the Polish language
scenario, using a~dataset presumably unfamiliar to LLMs to mitigate the risk of
data contamination. Our results show that not all extracted utterances are
correctly structured, indicating that either LLMs do not fully acquire
syntactic-semantic rules or they acquire them but cannot apply them
effectively. We conclude that the ability of LLMs to comprehend noisy
utterances is still relatively superficial compared to human proficiency in
processing them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CoNLL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explanation sensitivity to the randomness of large language models: the
  case of journalistic text classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremie Bogaert, Marie-Catherine de Marneffe, Antonin Descampe, Louis Escouflaire, Cedrick Fairon, Francois-Xavier Standaert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) perform very well in several natural language
processing tasks but raise explainability challenges. In this paper, we examine
the effect of random elements in the training of LLMs on the explainability of
their predictions. We do so on a task of opinionated journalistic text
classification in French. Using a fine-tuned CamemBERT model and an explanation
method based on relevance propagation, we find that training with different
random seeds produces models with similar accuracy but variable explanations.
We therefore claim that characterizing the explanations' statistical
distribution is needed for the explainability of LLMs. We then explore a
simpler model based on textual features which offers stable explanations but is
less accurate. Hence, this simpler model corresponds to a different tradeoff
between accuracy and explainability. We show that it can be improved by
inserting features derived from CamemBERT's explanations. We finally discuss
new research directions suggested by our results, in particular regarding the
origin of the sensitivity observed in the training randomness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is a faithful translation of a paper which was
  peer-reviewed and published in the French journal Traitement Automatique des
  Langues, n. 64</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ScienceAgentBench: Toward Rigorous Assessment of Language Agents for
  Data-Driven Scientific Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N. Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancements of language language models (LLMs) have piqued growing
interest in developing LLM-based language agents to automate scientific
discovery end-to-end, which has sparked both excitement and skepticism about
the true capabilities of such agents. In this work, we argue that for an agent
to fully automate scientific discovery, it must be able to complete all
essential tasks in the workflow. Thus, we call for rigorous assessment of
agents on individual tasks in a scientific workflow before making bold claims
on end-to-end automation. To this end, we present ScienceAgentBench, a new
benchmark for evaluating language agents for data-driven scientific discovery.
To ensure the scientific authenticity and real-world relevance of our
benchmark, we extract 102 tasks from 44 peer-reviewed publications in four
disciplines and engage nine subject matter experts to validate them. We unify
the target output for every task to a self-contained Python program file and
employ an array of evaluation metrics to examine the generated programs,
execution results, and costs. Each task goes through multiple rounds of manual
validation by annotators and subject matter experts to ensure its annotation
quality and scientific plausibility. We also propose two effective strategies
to mitigate data contamination concerns. Using our benchmark, we evaluate five
open-weight and proprietary LLMs, each with three frameworks: direct prompting,
OpenHands, and self-debug. Given three attempts for each task, the
best-performing agent can only solve 32.4% of the tasks independently and 34.3%
with expert-provided knowledge. These results underscore the limited capacities
of current language agents in generating code for data-driven discovery, let
alone end-to-end automation for scientific research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense
  Question Answering <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Maria Molfese, Simone Conia, Riccardo Orlando, Roberto Navigli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Large Language Models (LLMs) have shown strong reasoning capabilities
in commonsense question answering benchmarks, but the process underlying their
success remains largely opaque. As a consequence, recent approaches have
equipped LLMs with mechanisms for knowledge retrieval, reasoning and
introspection, not only to improve their capabilities but also to enhance the
interpretability of their outputs. However, these methods require additional
training, hand-crafted templates or human-written explanations. To address
these issues, we introduce ZEBRA, a zero-shot question answering framework that
combines retrieval, case-based reasoning and introspection and dispenses with
the need for additional training of the LLM. Given an input question, ZEBRA
retrieves relevant question-knowledge pairs from a knowledge base and generates
new knowledge by reasoning over the relationships in these pairs. This
generated knowledge is then used to answer the input question, improving the
model's performance and interpretability. We evaluate our approach across 8
well-established commonsense reasoning benchmarks, demonstrating that ZEBRA
consistently outperforms strong LLMs and previous knowledge integration
approaches, achieving an average accuracy improvement of up to 4.5 points.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent
  Sparse Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijie Yang, Zhihao Zhang, Zhuofu Chen, Zikun Li, Zhihao Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have driven significant advancements across
diverse NLP tasks, with long-context models gaining prominence for handling
extended inputs. However, the expanding key-value (KV) cache size required by
Transformer architectures intensifies the memory constraints, particularly
during the decoding phase, creating a significant bottleneck. Existing sparse
attention mechanisms designed to address this bottleneck have two limitations:
(1) they often fail to reliably identify the most relevant tokens for
attention, and (2) they overlook the spatial coherence of token selection
across consecutive Transformer layers, which can lead to performance
degradation and substantial overhead in token selection. This paper introduces
TidalDecode, a simple yet effective algorithm and system for fast and accurate
LLM decoding through position persistent sparse attention. TidalDecode
leverages the spatial coherence of tokens selected by existing sparse attention
methods and introduces a few token selection layers that perform full attention
to identify the tokens with the highest attention scores, while all other
layers perform sparse attention with the pre-selected tokens. This design
enables TidalDecode to substantially reduce the overhead of token selection for
sparse attention without sacrificing the quality of the generated results.
Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely
matches the generative performance of full attention methods while reducing the
LLM decoding latency by up to 2.1x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Initialization of Large Language Models via Reparameterization to
  Mitigate Loss Spikes <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kosuke Nishida, Kyosuke Nishida, Kuniko Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Loss spikes, a phenomenon in which the loss value diverges suddenly, is a
fundamental issue in the pre-training of large language models. This paper
supposes that the non-uniformity of the norm of the parameters is one of the
causes of loss spikes. Here, in training of neural networks, the scale of the
gradients is required to be kept constant throughout the layers to avoid the
vanishing and exploding gradients problem. However, to meet these requirements
in the Transformer model, the norm of the model parameters must be non-uniform,
and thus, parameters whose norm is smaller are more sensitive to the parameter
update. To address this issue, we propose a novel technique, weight scaling as
reparameterization (WeSaR). WeSaR introduces a gate parameter per parameter
matrix and adjusts it to the value satisfying the requirements. Because of the
gate parameter, WeSaR sets the norm of the original parameters uniformly, which
results in stable training. Experimental results with the Transformer decoders
consisting of 130 million, 1.3 billion, and 13 billion parameters showed that
WeSaR stabilizes and accelerates training and that it outperformed compared
methods including popular initialization methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP2024 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A test suite of <span class="highlight-title">prompt</span> injection attacks for LLM-based machine
  translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05047v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05047v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Valerio Miceli-Barone, Zhifan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-based NLP systems typically work by embedding their input data into
prompt templates which contain instructions and/or in-context examples,
creating queries which are submitted to a LLM, and then parsing the LLM
response in order to generate the system outputs. Prompt Injection Attacks
(PIAs) are a type of subversion of these systems where a malicious user crafts
special inputs which interfere with the prompt templates, causing the LLM to
respond in ways unintended by the system designer.
  Recently, Sun and Miceli-Barone proposed a class of PIAs against LLM-based
machine translation. Specifically, the task is to translate questions from the
TruthfulQA test suite, where an adversarial prompt is prepended to the
questions, instructing the system to ignore the translation instruction and
answer the questions instead.
  In this test suite, we extend this approach to all the language pairs of the
WMT 2024 General Machine Translation task. Moreover, we include additional
attack formats in addition to the one originally studied.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Named Clinical Entity Recognition Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wadood M Abdul, Marco AF Pimentel, Muhammad Umar Salman, Tathagata Raha, Clément Christophe, Praveen K Kanithi, Nasir Hayat, Ronnie Rajan, Shadab Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This technical report introduces a Named Clinical Entity Recognition
Benchmark for evaluating language models in healthcare, addressing the crucial
natural language processing (NLP) task of extracting structured information
from clinical narratives to support applications like automated coding,
clinical trial cohort identification, and clinical decision support.
  The leaderboard provides a standardized platform for assessing diverse
language models, including encoder and decoder architectures, on their ability
to identify and classify clinical entities across multiple medical domains. A
curated collection of openly available clinical datasets is utilized,
encompassing entities such as diseases, symptoms, medications, procedures, and
laboratory measurements. Importantly, these entities are standardized according
to the Observational Medical Outcomes Partnership (OMOP) Common Data Model,
ensuring consistency and interoperability across different healthcare systems
and datasets, and a comprehensive evaluation of model performance. Performance
of models is primarily assessed using the F1-score, and it is complemented by
various assessment modes to provide comprehensive insights into model
performance. The report also includes a brief analysis of models evaluated to
date, highlighting observed trends and limitations.
  By establishing this benchmarking framework, the leaderboard aims to promote
transparency, facilitate comparative analyses, and drive innovation in clinical
entity recognition tasks, addressing the need for robust evaluation methods in
healthcare NLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can LLMs plan paths with extra hints from solvers? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Wu, Sayan Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable capabilities in natural
language processing, mathematical problem solving, and tasks related to program
synthesis. However, their effectiveness in long-term planning and higher-order
reasoning has been noted to be limited and fragile. This paper explores an
approach for enhancing LLM performance in solving a classical robotic planning
task by integrating solver-generated feedback. We explore four different
strategies for providing feedback, including visual feedback, we utilize
fine-tuning, and we evaluate the performance of three different LLMs across a
10 standard and 100 more randomly generated planning problems. Our results
suggest that the solver-generated feedback improves the LLM's ability to solve
the moderately difficult problems, but the harder problems still remain out of
reach. The study provides detailed analysis of the effects of the different
hinting strategies and the different planning tendencies of the evaluated LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DEPT: Decoupled Embeddings for <span class="highlight-title">Pre-train</span>ing Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Iacob, Lorenzo Sani, Meghdad Kurmanji, William F. Shen, Xinchi Qiu, Dongqi Cai, Yan Gao, Nicholas D. Lane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Model pre-training benefits from a broader data mixture to enhance
performance across domains and languages. However, training on such
heterogeneous text corpora is complex, requiring extensive and cost-intensive
efforts. Since these data sources vary in lexical, syntactic, and semantic
aspects, they cause negative interference or the "curse of multilinguality". We
propose a novel pre-training framework to alleviate this curse. Our method,
DEPT, decouples the embedding layers from the transformer body while
simultaneously training the latter in multiple contexts. DEPT enables the model
to train without being bound to a shared global vocabulary. DEPT: (1) can train
robustly and effectively under significant data heterogeneity, (2) reduces the
parameter count of the token embeddings by up to 80% and the communication
costs by 675x for billion-scale models (3) enhances model generalization and
plasticity in adapting to new languages and domains, and (4) allows training
with custom optimized vocabulary per data source. We prove DEPT's potential by
performing the first vocabulary-agnostic federated multilingual pre-training of
a 1.3 billion-parameter model across high and low-resource languages, reducing
its parameter count by 409 million.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Biased Assessment of Expert Finding Systems <span class="chip">RecSys</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jens-Joris Decorte, Jeroen Van Hautte, Chris Develder, Thomas Demeester
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In large organisations, identifying experts on a given topic is crucial in
leveraging the internal knowledge spread across teams and departments.
So-called enterprise expert retrieval systems automatically discover and
structure employees' expertise based on the vast amount of heterogeneous data
available about them and the work they perform. Evaluating these systems
requires comprehensive ground truth expert annotations, which are hard to
obtain. Therefore, the annotation process typically relies on automated
recommendations of knowledge areas to validate. This case study provides an
analysis of how these recommendations can impact the evaluation of expert
finding systems. We demonstrate on a popular benchmark that system-validated
annotations lead to overestimated performance of traditional term-based
retrieval models and even invalidate comparisons with more recent neural
methods. We also augment knowledge areas with synonyms to uncover a strong bias
towards literal mentions of their constituent words. Finally, we propose
constraints to the annotation process to prevent these biased evaluations, and
show that this still allows annotation suggestions of high utility. These
findings should inform benchmark creation or selection for expert finding, to
guarantee meaningful comparison of methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 4th Workshop on Recommender Systems for Human
  Resources (RecSys in HR 2024) as part of RecSys 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SkillMatch: Evaluating <span class="highlight-title">Self-supervised</span> Learning of Skill Relatedness <span class="chip">ECML-PKDD 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jens-Joris Decorte, Jeroen Van Hautte, Thomas Demeester, Chris Develder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately modeling the relationships between skills is a crucial part of
human resources processes such as recruitment and employee development. Yet, no
benchmarks exist to evaluate such methods directly. We construct and release
SkillMatch, a benchmark for the task of skill relatedness, based on expert
knowledge mining from millions of job ads. Additionally, we propose a scalable
self-supervised learning technique to adapt a Sentence-BERT model based on
skill co-occurrence in job ads. This new method greatly surpasses traditional
models for skill relatedness as measured on SkillMatch. By releasing SkillMatch
publicly, we aim to contribute a foundation for research towards increased
accuracy and transparency of skill-based recommendation systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the International workshop on AI for Human Resources and
  Public Employment Services (AI4HR&PES) as part of ECML-PKDD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Rigour of Scientific Writing: Criteria, Analysis, and Insights <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph James, Chenghao Xiao, Yucheng Li, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rigour is crucial for scientific research as it ensures the reproducibility
and validity of results and findings. Despite its importance, little work
exists on modelling rigour computationally, and there is a lack of analysis on
whether these criteria can effectively signal or measure the rigour of
scientific papers in practice. In this paper, we introduce a bottom-up,
data-driven framework to automatically identify and define rigour criteria and
assess their relevance in scientific writing. Our framework includes rigour
keyword extraction, detailed rigour definition generation, and salient criteria
identification. Furthermore, our framework is domain-agnostic and can be
tailored to the evaluation of scientific rigour for different areas,
accommodating the distinct salient criteria across fields. We conducted
comprehensive experiments based on datasets collected from two high impact
venues for Machine Learning and NLP (i.e., ICLR and ACL) to demonstrate the
effectiveness of our framework in modelling rigour. In addition, we analyse
linguistic patterns of rigour, revealing that framing certainty is crucial for
enhancing the perception of scientific rigour, while suggestion certainty and
probability uncertainty diminish it.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted Findings at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Activation Scaling for Steering and Interpreting Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Stoehr, Kevin Du, Vésteinn Snæbjarnarson, Robert West, Ryan Cotterell, Aaron Schein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the prompt "Rome is in", can we steer a language model to flip its
prediction of an incorrect token "France" to a correct token "Italy" by only
multiplying a few relevant activation vectors with scalars? We argue that
successfully intervening on a model is a prerequisite for interpreting its
internal workings. Concretely, we establish a three-term objective: a
successful intervention should flip the correct with the wrong token and vice
versa (effectiveness), and leave other tokens unaffected (faithfulness), all
while being sparse (minimality). Using gradient-based optimization, this
objective lets us learn (and later evaluate) a specific kind of efficient and
interpretable intervention: activation scaling only modifies the signed
magnitude of activation vectors to strengthen, weaken, or reverse the steering
directions already encoded in the model. On synthetic tasks, this intervention
performs comparably with steering vectors in terms of effectiveness and
faithfulness, but is much more minimal allowing us to pinpoint interpretable
model components. We evaluate activation scaling from different angles, compare
performance on different datasets, and make activation scalars a learnable
function of the activation vectors themselves to generalize to varying-length
prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of the Association for Computational Linguistics: EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intent Classification for Bank Chatbots through LLM Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bibiána Lajčinová, Patrik Valábek, Michal Spišiak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study evaluates the application of large language models (LLMs) for
intent classification within a chatbot with predetermined responses designed
for banking industry websites. Specifically, the research examines the
effectiveness of fine-tuning SlovakBERT compared to employing multilingual
generative models, such as Llama 8b instruct and Gemma 7b instruct, in both
their pre-trained and fine-tuned versions. The findings indicate that
SlovakBERT outperforms the other models in terms of in-scope accuracy and
out-of-scope false positive rate, establishing it as the benchmark for this
application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, no figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Grammar Induction for Language Understanding and Generation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jushi Kai, Shengyuan Hou, Yusheng Huang, Zhouhan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grammar induction has made significant progress in recent years. However, it
is not clear how the application of induced grammar could enhance practical
performance in downstream tasks. In this work, we introduce an unsupervised
grammar induction method for language understanding and generation. We
construct a grammar parser to induce constituency structures and dependency
relations, which is simultaneously trained on downstream tasks without
additional syntax annotations. The induced grammar features are subsequently
incorporated into Transformer as a syntactic mask to guide self-attention. We
evaluate and apply our method to multiple machine translation tasks and natural
language understanding tasks. Our method demonstrates superior performance
compared to the original Transformer and other models enhanced with external
parsers. Experimental results indicate that our method is effective in both
from-scratch and pre-trained scenarios. Additionally, our research highlights
the contribution of explicitly modeling the grammatical structure of texts to
neural network models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rationale-Aware Answer Verification by Pairwise Self-Evaluation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akira Kawabata, Saku Sugawara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answer verification identifies correct solutions among candidates generated
by large language models (LLMs). Current approaches typically train verifier
models by labeling solutions as correct or incorrect based solely on whether
the final answer matches the gold answer. However, this approach neglects any
flawed rationale in the solution yielding the correct answer, undermining the
verifier's ability to distinguish between sound and flawed rationales. We
empirically show that in StrategyQA, only 19% of LLM-generated solutions with
correct answers have valid rationales, thus leading to an unreliable verifier.
Furthermore, we demonstrate that training a verifier on valid rationales
significantly improves its ability to distinguish valid and flawed rationale.
To make a better verifier without extra human supervision, we introduce REPS
(Rationale Enhancement through Pairwise Selection), a method for selecting
valid rationales from candidates by iteratively applying pairwise
self-evaluation using the same LLM that generates the solutions. Verifiers
trained on solutions selected by REPS outperform those trained using
conventional training methods on three reasoning benchmarks (ARC-Challenge,
DROP, and StrategyQA). Our results suggest that training reliable verifiers
requires ensuring the validity of rationales in addition to the correctness of
the final answers, which would be critical for models assisting humans in
solving complex reasoning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ As Simple as Fine-tuning: LLM Alignment via Bidirectional Negative
  Feedback Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Mao, Feng-Lin Li, Huimin Xu, Wei Zhang, Wang Chen, Anh Tuan Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct Preference Optimization (DPO) has emerged as a more computationally
efficient alternative to Reinforcement Learning from Human Feedback (RLHF) with
Proximal Policy Optimization (PPO), eliminating the need for reward models and
online sampling. Despite these benefits, DPO and its variants remain sensitive
to hyper-parameters and prone to instability, particularly on mathematical
datasets. We argue that these issues arise from the unidirectional
likelihood-derivative negative feedback inherent in the log-likelihood loss
function. To address this, we propose a novel LLM alignment loss that
establishes a stable Bidirectional Negative Feedback (BNF) during optimization.
Our proposed BNF loss eliminates the need for pairwise contrastive losses and
does not require any extra tunable hyper-parameters or pairwise preference
data, streamlining the alignment pipeline to be as simple as supervised
fine-tuning. We conduct extensive experiments across two challenging QA
benchmarks and four reasoning benchmarks. The experimental results show that
BNF achieves comparable performance to the best methods on QA benchmarks, while
its performance decrease on the four reasoning benchmarks is significantly
lower compared to the best methods, thus striking a better balance between
value alignment and reasoning ability. In addition, we further validate the
performance of BNF on non-pairwise datasets, and conduct in-depth analysis of
log-likelihood and logit shifts across different preference optimization
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MINER: Mining the Underlying Pattern of Modality-Specific Neurons in
  Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaichen Huang, Jiahao Huo, Yibo Yan, Kun Wang, Yutao Yue, Xuming Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, multimodal large language models (MLLMs) have significantly
advanced, integrating more modalities into diverse applications. However, the
lack of explainability remains a major barrier to their use in scenarios
requiring decision transparency. Current neuron-level explanation paradigms
mainly focus on knowledge localization or language- and domain-specific
analyses, leaving the exploration of multimodality largely unaddressed. To
tackle these challenges, we propose MINER, a transferable framework for mining
modality-specific neurons (MSNs) in MLLMs, which comprises four stages: (1)
modality separation, (2) importance score calculation, (3) importance score
aggregation, (4) modality-specific neuron selection. Extensive experiments
across six benchmarks and two representative MLLMs show that (I) deactivating
ONLY 2% of MSNs significantly reduces MLLMs performance (0.56 to 0.24 for
Qwen2-VL, 0.69 to 0.31 for Qwen2-Audio), (II) different modalities mainly
converge in the lower layers, (III) MSNs influence how key information from
various modalities converges to the last token, (IV) two intriguing phenomena
worth further investigation, i.e., semantic probing and semantic telomeres. The
source code is available at this URL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LPZero: Language Model Zero-cost Proxy Search from Zero 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peijie Dong, Lujun Li, Xiang Liu, Zhenheng Tang, Xuebo Liu, Qiang Wang, Xiaowen Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In spite of the outstanding performance, Neural Architecture Search (NAS) is
criticized for massive computation. Recently, Zero-shot NAS has emerged as a
promising approach by exploiting Zero-cost (ZC) proxies, which markedly reduce
computational demands. Despite this, existing ZC proxies heavily rely on expert
knowledge and incur significant trial-and-error costs. Particularly in NLP
tasks, most existing ZC proxies fail to surpass the performance of the naive
baseline. To address these challenges, we introduce a novel framework,
\textbf{LPZero}, which is the first to automatically design ZC proxies for
various tasks, achieving higher ranking consistency than human-designed
proxies. Specifically, we model the ZC proxy as a symbolic equation and
incorporate a unified proxy search space that encompasses existing ZC proxies,
which are composed of a predefined set of mathematical symbols. To
heuristically search for the best ZC proxy, LPZero incorporates genetic
programming to find the optimal symbolic composition. We propose a
\textit{Rule-based Pruning Strategy (RPS),} which preemptively eliminates
unpromising proxies, thereby mitigating the risk of proxy degradation.
Extensive experiments on FlexiBERT, GPT-2, and LLaMA-7B demonstrate LPZero's
superior ranking ability and performance on downstream tasks compared to
current approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, 10 appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAPE V2: Process Attention Score as Feature Map for Length Extrapolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanyang Zheng, Yihang Gao, Han Shi, Jing Xiong, Jiankai Sun, Jingyao Li, Minbin Huang, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The attention mechanism is a fundamental component of the Transformer model,
contributing to interactions among distinct tokens, in contrast to earlier
feed-forward neural networks. In general, the attention scores are determined
simply by the key-query products. However, this work's occasional trial
(combining DAPE and NoPE) of including additional MLPs on attention scores
without position encoding indicates that the classical key-query multiplication
may limit the performance of Transformers. In this work, we conceptualize
attention as a feature map and apply the convolution operator (for neighboring
attention scores across different heads) to mimic the processing methods in
computer vision. Specifically, the main contribution of this paper is
identifying and interpreting the Transformer length extrapolation problem as a
result of the limited expressiveness of the naive query and key dot product,
and we successfully translate the length extrapolation issue into a
well-understood feature map processing problem. The novel insight, which can be
adapted to various attention-related models, reveals that the current
Transformer architecture has the potential for further evolution. Extensive
experiments demonstrate that treating attention as a feature map and applying
convolution as a processing method significantly enhances Transformer
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech Report. arXiv admin note: text overlap with arXiv:2405.14722</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representing the Under-Represented: Cultural and Core Capability
  Benchmarks for Developing Thai Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dahyun Kim, Sukyung Lee, Yungi Kim, Attapol Rutherford, Chanjun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of large language models (LLMs) has highlighted the
need for robust evaluation frameworks that assess their core capabilities, such
as reasoning, knowledge, and commonsense, leading to the inception of certain
widely-used benchmark suites such as the H6 benchmark. However, these benchmark
suites are primarily built for the English language, and there exists a lack
thereof for under-represented languages, in terms of LLM development, such as
Thai. On the other hand, developing LLMs for Thai should also include enhancing
the cultural understanding as well as core capabilities. To address these dual
challenge in Thai LLM research, we propose two key benchmarks: Thai-H6 and Thai
Cultural and Linguistic Intelligence Benchmark (ThaiCLI). Through a thorough
evaluation of various LLMs with multi-lingual capabilities, we provide a
comprehensive analysis of the proposed benchmarks and how they contribute to
Thai LLM development. Furthermore, we will make both the datasets and
evaluation code publicly available to encourage further research and
development for Thai LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GARLIC: LLM-Guided Dynamic Progress Control with Hierarchical Weighted
  Graph for Long Document QA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Wang, Yanzheng Xiang, Lin Gui, Yulan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past, Retrieval-Augmented Generation (RAG) methods split text into
chunks to enable language models to handle long documents. Recent tree-based
RAG methods are able to retrieve detailed information while preserving global
context. However, with the advent of more powerful LLMs, such as Llama 3.1,
which offer better comprehension and support for longer inputs, we found that
even recent tree-based RAG methods perform worse than directly feeding the
entire document into Llama 3.1, although RAG methods still hold an advantage in
reducing computational costs. In this paper, we propose a new retrieval method,
called LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph
(GARLIC), which outperforms previous state-of-the-art baselines, including
Llama 3.1, while retaining the computational efficiency of RAG methods. Our
method introduces several improvements: (1) Rather than using a tree structure,
we construct a Hierarchical Weighted Directed Acyclic Graph with many-to-many
summarization, where the graph edges are derived from attention mechanisms, and
each node focuses on a single event or very few events. (2) We introduce a
novel retrieval method that leverages the attention weights of LLMs rather than
dense embedding similarity. Our method allows for searching the graph along
multiple paths and can terminate at any depth. (3) We use the LLM to control
the retrieval process, enabling it to dynamically adjust the amount and depth
of information retrieved for different queries. Experimental results show that
our method outperforms previous state-of-the-art baselines, including Llama
3.1, on two single-document and two multi-document QA datasets, while
maintaining similar computational complexity to traditional RAG methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Formality is Favored: Unraveling the Learning Preferences of Large
  Language Models on Data with Conflicting Knowledge <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahuan Li, Yiqing Cao, Shujian Huang, Jiajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Having been trained on massive pretraining data, large language models have
shown excellent performance on many knowledge-intensive tasks. However,
pretraining data tends to contain misleading and even conflicting information,
and it is intriguing to understand how LLMs handle these noisy data during
training. In this study, we systematically analyze LLMs' learning preferences
for data with conflicting knowledge. We find that pretrained LLMs establish
learning preferences similar to humans, i.e., preferences towards formal texts
and texts with fewer spelling errors, resulting in faster learning and more
favorable treatment of knowledge in data with such features when facing
conflicts. This finding is generalizable across models and languages and is
more evident in larger models. An in-depth analysis reveals that LLMs tend to
trust data with features that signify consistency with the majority of data,
and it is possible to instill new preferences and erase old ones by
manipulating the degree of consistency with the majority data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by EMNLP 2024, main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImProver: Agent-Based Automated Proof Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riyaz Ahuja, Jeremy Avigad, Prasad Tetali, Sean Welleck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been used to generate formal proofs of
mathematical theorems in proofs assistants such as Lean. However, we often want
to optimize a formal proof with respect to various criteria, depending on its
downstream use. For example, we may want a proof to adhere to a certain style,
or to be readable, concise, or modularly structured. Having suitably optimized
proofs is also important for learning tasks, especially since human-written
proofs may not optimal for that purpose. To this end, we study a new problem of
automated proof optimization: rewriting a proof so that it is correct and
optimizes for an arbitrary criterion, such as length or readability. As a first
method for automated proof optimization, we present ImProver, a
large-language-model agent that rewrites proofs to optimize arbitrary
user-defined metrics in Lean. We find that naively applying LLMs to proof
optimization falls short, and we incorporate various improvements into
ImProver, such as the use of symbolic Lean context in a novel Chain-of-States
technique, as well as error-correction and retrieval. We test ImProver on
rewriting real-world undergraduate, competition, and research-level mathematics
theorems, finding that ImProver is capable of rewriting proofs so that they are
substantially shorter, more modular, and more readable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Document-level Causal Relation Extraction with Knowledge-guided Binary
  Question Answering <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zimu Wang, Lei Xia, Wei Wang, Xinya Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As an essential task in information extraction (IE), Event-Event Causal
Relation Extraction (ECRE) aims to identify and classify the causal
relationships between event mentions in natural language texts. However,
existing research on ECRE has highlighted two critical challenges, including
the lack of document-level modeling and causal hallucinations. In this paper,
we propose a Knowledge-guided binary Question Answering (KnowQA) method with
event structures for ECRE, consisting of two stages: Event Structure
Construction and Binary Question Answering. We conduct extensive experiments
under both zero-shot and fine-tuning settings with large language models (LLMs)
on the MECI and MAVEN-ERE datasets. Experimental results demonstrate the
usefulness of event structures on document-level ECRE and the effectiveness of
KnowQA by achieving state-of-the-art on the MECI dataset. We observe not only
the effectiveness but also the high generalizability and low inconsistency of
our method, particularly when with complete event structures after fine-tuning
the models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Findings of EMNLP 2024. Camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intriguing Properties of Large Language and Vision Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Young-Jun Lee, Byungsoo Ko, Han-Gyu Kim, Yechan Hwang, Ho-Jin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, large language and vision models (LLVMs) have received significant
attention and development efforts due to their remarkable generalization
performance across a wide range of tasks requiring perception and cognitive
abilities. A key factor behind their success is their simple architecture,
which consists of a vision encoder, a projector, and a large language model
(LLM). Despite their achievements in advanced reasoning tasks, their
performance on fundamental perception-related tasks (e.g., MMVP) remains
surprisingly low. This discrepancy raises the question of how LLVMs truly
perceive images and exploit the advantages of the vision encoder. To address
this, we systematically investigate this question regarding several aspects:
permutation invariance, robustness, math reasoning, alignment preserving and
importance, by evaluating the most common LLVM's families (i.e., LLaVA) across
10 evaluation benchmarks. Our extensive experiments reveal several intriguing
properties of current LLVMs: (1) they internally process the image in a global
manner, even when the order of visual patch sequences is randomly permuted; (2)
they are sometimes able to solve math problems without fully perceiving
detailed numerical information; (3) the cross-modal alignment is overfitted to
complex reasoning tasks, thereby, causing them to lose some of the original
perceptual capabilities of their vision encoder; (4) the representation space
in the lower layers (<25%) plays a crucial role in determining performance and
enhancing visual understanding. Lastly, based on the above observations, we
suggest potential future directions for building better LLVMs and constructing
more challenging evaluation benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available in https://github.com/passing2961/IP-LLVM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TableRAG: Million-Token Table Understanding with Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Si-An Chen, Lesly Miculicich, Julian Martin Eisenschlos, Zifeng Wang, Zilong Wang, Yanfei Chen, Yasuhisa Fujii, Hsuan-Tien Lin, Chen-Yu Lee, Tomas Pfister
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in language models (LMs) have notably enhanced their
ability to reason with tabular data, primarily through program-aided mechanisms
that manipulate and analyze tables. However, these methods often require the
entire table as input, leading to scalability challenges due to the positional
bias or context length constraints. In response to these challenges, we
introduce TableRAG, a Retrieval-Augmented Generation (RAG) framework
specifically designed for LM-based table understanding. TableRAG leverages
query expansion combined with schema and cell retrieval to pinpoint crucial
information before providing it to the LMs. This enables more efficient data
encoding and precise retrieval, significantly reducing prompt lengths and
mitigating information loss. We have developed two new million-token benchmarks
from the Arcade and BIRD-SQL datasets to thoroughly evaluate TableRAG's
effectiveness at scale. Our results demonstrate that TableRAG's retrieval
design achieves the highest retrieval quality, leading to the new
state-of-the-art performance on large-scale table understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TLDR: Token-Level Detective Reward Model for Large Vision Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deqing Fu, Tong Xiao, Rui Wang, Wang Zhu, Pengchuan Zhang, Guan Pang, Robin Jia, Lawrence Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although reward models have been successful in improving multimodal large
language models, the reward models themselves remain brutal and contain minimal
information. Notably, existing reward models only mimic human annotations by
assigning only one binary feedback to any text, no matter how long the text is.
In the realm of multimodal language models, where models are required to
process both images and texts, a naive reward model may learn implicit biases
toward texts and become less grounded in images. In this paper, we propose a
$\textbf{T}$oken-$\textbf{L}$evel $\textbf{D}$etective $\textbf{R}$eward Model
($\textbf{TLDR}$) to provide fine-grained annotations to each text token. We
first introduce a perturbation-based method to generate synthetic hard
negatives and their token-level labels to train TLDR models. Then we show the
rich usefulness of TLDR models both in assisting off-the-shelf models to
self-correct their generations, and in serving as a hallucination evaluation
tool. Finally, we show that TLDR models can significantly speed up human
annotation by 3 times to acquire a broader range of high-quality vision
language data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work done at Meta</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient <span class="highlight-title">transformer</span> with reinforced position embedding for language
  models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yen-Che Hsiao, Abhishek Dutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose an efficient transformer architecture that uses
reinforced positional embedding to obtain superior performance with half the
number of encoder decoder layers. We demonstrate that concatenating positional
encoding with trainable token embeddings, normalizing columns in the token
embedding matrix, and using the normalized token embedding matrix as the value
of the attention layer improve the training and validation loss and the
training time in an encoder-decoder Transformer model for a Portuguese-English
translation task with 10 epochs or 12 hours of training across 10 trials. Our
method, with roughly a threefold parameter reduction compared to the baseline
model, yields a mean training loss of 1.21, a mean validation loss of 1.51, and
an average training time of 1352.27 seconds per epoch, surpassing the baseline
model with the same embedding dimension that employs addition of positional
encoding and token embeddings, which achieves a mean training loss of 1.96, a
validation loss of 2.18, and an average training time of 4297.79 seconds per
epoch. Additionally, we evaluated our proposed architecture and the baseline
across 14 diverse translation datasets from TensorFlow. The results indicate
that our method consistently achieves lower or comparable training and
validation losses, suggesting enhanced learning efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forgetting Curve: A Reliable Method for Evaluating Memorization
  Capability for Long-context Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Liu, Runsong Zhao, Pengcheng Huang, Chunyang Xiao, Bei Li, Jingang Wang, Tong Xiao, Jingbo Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous recent works target to extend effective context length for language
models and various methods, tasks and benchmarks exist to measure model's
effective memorization length. However, through thorough investigations, we
find limitations for currently existing evaluations on model's memorization
capability. We provide an extensive survey for limitations in this work and
propose a new method called forgetting curve to measure the memorization
capability of long-context models. We show that forgetting curve has the
advantage of being robust to the tested corpus and the experimental settings,
of not relying on prompts and can be applied to any model size.
  We apply our forgetting curve to a large variety of models involving both
transformer and RNN/SSM based architectures. Our measurement provides empirical
evidence for the effectiveness of transformer extension techniques while raises
questions for the effective length of RNN/SSM based models. We also examine the
difference between our measurement and existing benchmarks as well as popular
metrics for various models. Our code and results can be found at
https://github.com/1azybug/ForgettingCurve.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction
  Diversity on Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Zhang, Justin Wang, Francois Charton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and accurately following instructions is critical for large
language models (LLMs) to be effective across diverse tasks. In this work, we
rigorously examine the key factors that enable models to generalize to unseen
instructions, providing insights to guide the collection of data for
instruction-tuning. Through controlled experiments, inspired by the
Turing-complete Markov algorithm, we demonstrate that such generalization
$\textbf{only emerges}$ when training data is diversified enough across
semantic domains. Our findings also reveal that merely diversifying within
limited domains fails to ensure robust generalization. In contrast,
cross-domain data diversification, even under constrained data budgets,
significantly enhances a model's adaptability. We further extend our analysis
to real-world scenarios, including fine-tuning of
$\textit{$\textbf{specialist}$}$ and $\textit{$\textbf{generalist}$}$ models.
In both cases, we demonstrate that 1) better performance can be achieved by
increasing the diversity of an established dataset while keeping the data size
constant, and 2) when scaling up the data, diversifying the semantics of
instructions is more effective than simply increasing the quantity of similar
data. Our research provides important insights for dataset collation,
particularly when optimizing model performance by expanding training data for
both specialist and generalist scenarios. We show that careful consideration of
data diversification is key: training specialist models with data extending
beyond their core domain leads to significant performance improvements, while
generalist models benefit from diverse data mixtures that enhance their overall
instruction-following capabilities across a wide range of applications. Our
results highlight the critical role of strategic diversification and offer
clear guidelines for improving data quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rule-based Data Selection for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaomin Li, Mingye Gao, Zhiwei Zhang, Chang Yue, Hong Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quality of training data significantly impacts the performance of large
language models (LLMs). There are increasing studies using LLMs to rate and
select data based on several human-crafted metrics (rules). However, these
conventional rule-based approaches often depend too heavily on human
heuristics, lack effective metrics for assessing rules, and exhibit limited
adaptability to new tasks. In our study, we introduce an innovative rule-based
framework that utilizes the orthogonality of score vectors associated with
rules as a novel metric for rule evaluations. Our approach includes an
automated pipeline that first uses LLMs to generate a diverse set of rules,
encompassing various rating dimensions to evaluate data quality. Then it rates
a batch of data based on these rules and uses the determinantal point process
(DPP) from random matrix theory to select the most orthogonal score vectors,
thereby identifying a set of independent rules. These rules are subsequently
used to evaluate all data, selecting samples with the highest average scores
for downstream tasks such as LLM training. We verify the effectiveness of our
method through two experimental setups: 1) comparisons with ground truth
ratings and 2) benchmarking LLMs trained with the chosen data. Our
comprehensive experiments cover a range of scenarios, including general
pre-training and domain-specific fine-tuning in areas such as IMDB, Medical,
Math, and Code. The outcomes demonstrate that our DPP-based rule rating method
consistently outperforms other approaches, including rule-free rating, uniform
sampling, importance resampling, and QuRating, in terms of both rating
precision and model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning How Hard to Think: Input-Adaptive Allocation of LM Computation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehul Damani, Idan Shenfeld, Andi Peng, Andreea Bobu, Jacob Andreas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computationally intensive decoding procedures--including search, reranking,
and self-critique--can improve the quality of language model (LM) outputs in
problems spanning code generation, numerical reasoning, and dialog. Existing
work typically applies the same decoding procedure for every input to an LM.
But not all inputs require the same amount of computation to process. Can we
allocate decoding computation adaptively, using more resources to answer
questions whose answers will be harder to compute? We present an approach that
predicts the distribution of rewards given an input and computation budget,
then allocates additional computation to inputs for which it is predicted to be
most useful. We apply this approach in two decoding procedures: first, an
adaptive best-of-k procedure that dynamically selects the number of samples to
generate as input to a reranker; second, a routing procedure that dynamically
responds to a query using a decoding procedure that is expensive but accurate,
or one that is cheaper but less capable. Across a suite of programming,
mathematics, and dialog tasks, we show that accurate computation-allocation
procedures can be learned, and reduce computation by up to 50% at no cost to
response quality, or improve quality by up to 10% at a fixed computational
budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling and Estimation of Vocal Tract and Glottal Source Parameters
  Using ARMAX-LF Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Lia, Masato Akagia, Yongwei Lib, Masashi Unokia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling and estimation of the vocal tract and glottal source parameters of
vowels from raw speech can be typically done by using the Auto-Regressive with
eXogenous input (ARX) model and Liljencrants-Fant (LF) model with an
iteration-based estimation approach. However, the all-pole autoregressive model
in the modeling of vocal tract filters cannot provide the locations of
anti-formants (zeros), which increases the estimation errors in certain classes
of speech sounds, such as nasal, fricative, and stop consonants. In this paper,
we propose the Auto-Regressive Moving Average eXogenous with LF (ARMAX-LF)
model to extend the ARX-LF model to a wider variety of speech sounds, including
vowels and nasalized consonants. The LF model represents the glottal source
derivative as a parametrized time-domain model, and the ARMAX model represents
the vocal tract as a pole-zero filter with an additional exogenous LF
excitation as input. To estimate multiple parameters with fewer errors, we
first utilize the powerful nonlinear fitting ability of deep neural networks
(DNNs) to build a mapping from extracted glottal source derivatives or speech
waveforms to corresponding LF parameters. Then, glottal source and vocal tract
parameters can be estimated with fewer estimation errors and without any
iterations as in the analysis-by-synthesis strategy. Experimental results with
synthesized speech using the linear source-filter model, synthesized speech
using the physical model, and real speech signals showed that the proposed
ARMAX-LF model with a DNN-based estimation method can estimate the parameters
of both vowels and nasalized sounds with fewer errors and estimation time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The LLM Effect: Are Humans Truly Using LLMs, or Are They Being
  Influenced By Them Instead? <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander S. Choi, Syeda Sabrina Akter, JP Singh, Antonios Anastasopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown capabilities close to human
performance in various analytical tasks, leading researchers to use them for
time and labor-intensive analyses. However, their capability to handle highly
specialized and open-ended tasks in domains like policy studies remains in
question. This paper investigates the efficiency and accuracy of LLMs in
specialized tasks through a structured user study focusing on Human-LLM
partnership. The study, conducted in two stages-Topic Discovery and Topic
Assignment-integrates LLMs with expert annotators to observe the impact of LLM
suggestions on what is usually human-only analysis. Results indicate that
LLM-generated topic lists have significant overlap with human generated topic
lists, with minor hiccups in missing document-specific topics. However, LLM
suggestions may significantly improve task completion speed, but at the same
time introduce anchoring bias, potentially affecting the depth and nuance of
the analysis, raising a critical question about the trade-off between increased
efficiency and the risk of biased analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP Main 2024. First two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning
  in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Wang, Shan Dong, Yuhui Xu, Hanze Dong, Yalu Wang, Amrita Saha, Ee-Peng Lim, Caiming Xiong, Doyen Sahoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large language models (LLMs) have demonstrated versatile capabilities
in long-context scenarios. Although some recent benchmarks have been developed
to evaluate the long-context capabilities of LLMs, there is a lack of
benchmarks evaluating the mathematical reasoning abilities of LLMs over long
contexts, which is crucial for LLMs' application in real-world scenarios. In
this paper, we introduce MathHay, an automated benchmark designed to assess the
long-context mathematical reasoning capabilities of LLMs. Unlike previous
benchmarks like Needle in a Haystack, which focus primarily on information
retrieval within long texts, MathHay demands models with both
information-seeking and complex mathematical reasoning abilities. We conduct
extensive experiments on MathHay to assess the long-context mathematical
reasoning abilities of eight top-performing LLMs. Even the best-performing
model, Gemini-1.5-Pro-002, still struggles with mathematical reasoning over
long contexts, achieving only 51.26% accuracy at 128K tokens. This highlights
the significant room for improvement on the MathHay benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work-in-Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deeper Insights Without Updates: The Power of In-Context Learning Over
  Fine-Tuning <span class="chip">EMNLP'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyu Yin, Xuzheng He, Luoao Deng, Chak Tou Leong, Fan Wang, Yanzhao Yan, Xiaoyu Shen, Qiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning and in-context learning (ICL) are two prevalent methods in
imbuing large language models with task-specific knowledge. It is commonly
believed that fine-tuning can surpass ICL given sufficient training samples as
it allows the model to adjust its internal parameters based on the data.
However, this paper presents a counterintuitive finding: For tasks with
implicit patterns, ICL captures these patterns significantly better than
fine-tuning. We developed several datasets featuring implicit patterns, such as
sequences determining answers through parity or identifying reducible terms in
calculations. We then evaluated the models' understanding of these patterns
under both fine-tuning and ICL across models ranging from 0.5B to 7B
parameters. The results indicate that models employing ICL can quickly grasp
deep patterns and significantly improve accuracy. In contrast, fine-tuning,
despite utilizing thousands of times more training samples than ICL, achieved
only limited improvements. We also proposed circuit shift theory from a
mechanistic interpretability's view to explain why ICL wins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP'24 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Multi-Agent Evaluation of Large Language Models through
  Iterative Debates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaithanya Bandi, Hari Bandi, Abir Harrasse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores optimal architectures for evaluating the outputs of large
language models (LLMs) using LLMs themselves. We propose a novel framework that
interprets LLMs as advocates within an ensemble of interacting agents, allowing
them to defend their answers and reach conclusions through a judge and jury
system. This approach offers a more dynamic and comprehensive evaluation
process compared to traditional human-based assessments or automated metrics.
We discuss the motivation behind this framework, its key components, and
comparative advantages. We also present a probabilistic model to evaluate the
error reduction achieved by iterative advocate systems. Finally, we outline
experiments to validate the effectiveness of multi-advocate architectures and
discuss future research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Learning to Improve Retrieval for Real-world Fact Checking <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniruddh Sriram, Fangyuan Xu, Eunsol Choi, Greg Durrett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work on fact-checking addresses a realistic setting where models
incorporate evidence retrieved from the web to decide the veracity of claims. A
bottleneck in this pipeline is in retrieving relevant evidence: traditional
methods may surface documents directly related to a claim, but fact-checking
complex claims requires more inferences. For instance, a document about how a
vaccine was developed is relevant to addressing claims about what it might
contain, even if it does not address them directly. We present Contrastive
Fact-Checking Reranker (CFR), an improved retriever for this setting. By
leveraging the AVeriTeC dataset, which annotates subquestions for claims with
human written answers from evidence documents, we fine-tune Contriever with a
contrastive objective based on multiple training signals, including
distillation from GPT-4, evaluating subquestion answers, and gold labels in the
dataset. We evaluate our model on both retrieval and end-to-end veracity
judgments about claims. On the AVeriTeC dataset, we find a 6\% improvement in
veracity classification accuracy. We also show our gains can be transferred to
FEVER, ClaimDecomp, HotpotQA, and a synthetic dataset requiring retrievers to
make inferences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 FEVER Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ mDPO: Conditional Preference Optimization for Multimodal Large Language
  Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Wang, Wenxuan Zhou, James Y. Huang, Nan Xu, Sheng Zhang, Hoifung Poon, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct preference optimization (DPO) has shown to be an effective method for
large language model (LLM) alignment. Recent works have attempted to apply DPO
to multimodal scenarios but have found it challenging to achieve consistent
improvement. Through a comparative experiment, we identify the unconditional
preference problem in multimodal preference optimization, where the model
overlooks the image condition. To address this problem, we propose mDPO, a
multimodal DPO objective that prevents the over-prioritization of language-only
preferences by also optimizing image preference. Moreover, we introduce a
reward anchor that forces the reward to be positive for chosen responses,
thereby avoiding the decrease in their likelihood -- an intrinsic problem of
relative preference optimization. Experiments on two multimodal LLMs of
different sizes and three widely used benchmarks demonstrate that mDPO
effectively addresses the unconditional preference problem in multimodal
preference optimization and significantly improves model performance,
particularly in reducing hallucination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference. Project website:
  https://feiwang96.github.io/mDPO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How
  to Fix It) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17975v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17975v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Meeus, Igor Shilov, Shubham Jain, Manuel Faysse, Marek Rei, Yves-Alexandre de Montjoye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whether LLMs memorize their training data and what this means, from privacy
leakage to detecting copyright violations -- has become a rapidly growing area
of research over the last two years. In recent months, more than 10 new methods
have been proposed to perform Membership Inference Attacks (MIAs) against LLMs.
Contrary to traditional MIAs which rely on fixed -- but randomized -- records
or models, these methods are mostly evaluated on datasets collected post-hoc.
Sets of members and non-members, used to evaluate the MIA, are constructed
using informed guesses after the release of a model. This lack of randomization
raises concerns of a distribution shift between members and non-members. In the
first part, we review the literature on MIAs against LLMs. While most work
focuses on sequence-level MIAs evaluated in post-hoc setups, we show that a
range of target models, motivations and units of interest have been considered
in the literature. We then quantify distribution shifts present in the 6
datasets used in the literature, ranging from books to papers, using a bag of
word classifier. Our analysis reveals that all of them suffer from severe
distribution shifts. This challenges the validity of using such setups to
measure LLM memorization and may undermine the benchmarking of recently
proposed methods. Yet, all hope might not be lost. In the second part, we
introduce important considerations to properly evaluate MIAs against LLMs and
discuss potential ways forward: randomized test splits, injections of
randomized (unique) sequences, randomized finetuning, and post-hoc control
methods. While each option comes with its advantages and limitations, we
believe they collectively provide solid grounds to guide the development of MIA
methods and study LLM memorization. We conclude by proposing comprehensive,
easy-to-use benchmarks for sequence- and document-level MIAs against LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMLU-Pro: A More Robust and Challenging Multi-Task Language
  Understanding Benchmark (Published at NeurIPS 2024 Track <span class="highlight-title">Dataset</span>s and
  Benchmarks) <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01574v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01574v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the age of large-scale language models, benchmarks like the Massive
Multitask Language Understanding (MMLU) have been pivotal in pushing the
boundaries of what AI can achieve in language comprehension and reasoning
across diverse domains. However, as models continue to improve, their
performance on these benchmarks has begun to plateau, making it increasingly
difficult to discern differences in model capabilities. This paper introduces
MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven
MMLU benchmark by integrating more challenging, reasoning-focused questions and
expanding the choice set from four to ten options. Additionally, MMLU-Pro
eliminates the trivial and noisy questions in MMLU. Our experimental results
show that MMLU-Pro not only raises the challenge, causing a significant drop in
accuracy by 16% to 33% compared to MMLU but also demonstrates greater stability
under varying prompts. With 24 different prompt styles tested, the sensitivity
of model scores to prompt variations decreased from 4-5% in MMLU to just 2% in
MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT)
reasoning achieved better performance on MMLU-Pro compared to direct answering,
which is in stark contrast to the findings on the original MMLU, indicating
that MMLU-Pro includes more complex reasoning questions. Our assessments
confirm that MMLU-Pro is a more discriminative benchmark to better track
progress in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has been accepted and published at NeurIPS 2024 Track
  Datasets and Benchmarks (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BigCodeBench: Benchmarking Code Generation with Diverse Function Calls
  and Complex Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15877v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15877v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, David Lo, Binyuan Hui, Niklas Muennighoff, Daniel Fried, Xiaoning Du, Harm de Vries, Leandro Von Werra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task automation has been greatly empowered by the recent advances in Large
Language Models (LLMs) via Python code, where the tasks ranging from software
engineering development to general-purpose reasoning. While current benchmarks
have shown that LLMs can solve tasks using programs like human developers, the
majority of their evaluations are limited to short and self-contained
algorithmic tasks or standalone function calls. Solving challenging and
practical requires the capability of utilizing diverse function calls as tools
to efficiently implement functionalities like data analysis and web
development. In addition, using multiple tools to solve a task needs
compositional reasoning by accurately understanding complex instructions.
Fulfilling both of these characteristics can pose a great challenge for LLMs.To
assess how well LLMs can solve challenging and practical tasks via programs, we
introduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple
function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained
tasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with
an average branch coverage of 99%. In addition, we propose a
natural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that
automatically transforms the original docstrings into short instructions only
with essential information. Our extensive evaluation of 60 LLMs shows that LLMs
are not yet capable of following complex instructions to use function calls
precisely, with scores up to 60%, significantly lower than the human
performance of 97%. The results underscore the need for further advancements in
this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 14 figures, 7 tables, built with love by the BigCode
  community :)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question
  Answering (Published in Findings of EMNLP 2024) <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02233v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02233v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Wang, Xueguang Ma, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale language models (LLMs) like ChatGPT have demonstrated impressive
abilities in generating responses based on human instructions. However, their
use in the medical field can be challenging due to their lack of specific,
in-depth knowledge. In this study, we present a system called LLMs Augmented
with Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in
specialized domains. LLM-AMT integrates authoritative medical textbooks into
the LLMs' framework using plug-and-play modules. These modules include a Query
Augmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together,
they incorporate authoritative medical knowledge. Additionally, an LLM Reader
aids in contextual understanding. Our experimental results on three medical QA
tasks demonstrate that LLMAMT significantly improves response quality, with
accuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as the
base model, LLM-AMT outperforms the specialized Med-PaLM 2 model pre-trained on
a massive amount of medical corpus by 2-3%. We found that despite being 100x
smaller in size, medical textbooks as a retrieval corpus is proven to be a more
effective knowledge database than Wikipedia in the medical domain, boosting
performance by 7.8%-13.7%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has been accepted and published at EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Annotation alignment: Comparing LLM and human annotations of
  conversational safety <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06369v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06369v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajiv Movva, Pang Wei Koh, Emma Pierson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Do LLMs align with human perceptions of safety? We study this question via
annotation alignment, the extent to which LLMs and humans agree when annotating
the safety of user-chatbot conversations. We leverage the recent DICES dataset
(Aroyo et al., 2023), in which 350 conversations are each rated for safety by
112 annotators spanning 10 race-gender groups. GPT-4 achieves a Pearson
correlation of $r = 0.59$ with the average annotator rating, \textit{higher}
than the median annotator's correlation with the average ($r=0.51$). We show
that larger datasets are needed to resolve whether LLMs exhibit disparities in
how well they correlate with different demographic groups. Also, there is
substantial idiosyncratic variation in correlation within groups, suggesting
that race & gender do not fully capture differences in alignment. Finally, we
find that GPT-4 cannot predict when one demographic group finds a conversation
more unsafe than another.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Main). Main text contains 6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Document Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John X. Morris, Alexander M. Rush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense document embeddings are central to neural retrieval. The dominant
paradigm is to train and construct embeddings by running encoders directly on
individual documents. In this work, we argue that these embeddings, while
effective, are implicitly out-of-context for targeted use cases of retrieval,
and that a contextualized document embedding should take into account both the
document and neighboring documents in context - analogous to contextualized
word embeddings. We propose two complementary methods for contextualized
document embeddings: first, an alternative contrastive learning objective that
explicitly incorporates the document neighbors into the intra-batch contextual
loss; second, a new contextual architecture that explicitly encodes neighbor
document information into the encoded representation. Results show that both
methods achieve better performance than biencoders in several settings, with
differences especially pronounced out-of-domain. We achieve state-of-the-art
results on the MTEB benchmark with no hard negative mining, score distillation,
dataset-specific instructions, intra-GPU example-sharing, or extremely large
batch sizes. Our method can be applied to improve performance on any
contrastive learning dataset and any biencoder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Creative Beam Search: LLM-as-a-Judge For Improving Response Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00099v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00099v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Franceschelli, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are revolutionizing several areas, including artificial
creativity. However, the process of generation in machines profoundly diverges
from that observed in humans. In particular, machine generation is
characterized by a lack of intentionality and an underlying creative process.
We propose a method called Creative Beam Search that uses Diverse Beam Search
and LLM-as-a-Judge to perform response generation and response validation. The
results of a qualitative experiment show how our approach can provide better
output than standard sampling techniques. We also show that the response
validation step is a necessary complement to the response generation step.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented as a short paper at the 15th International Conference on
  Computational Creativity (ICCC'24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaMetrics: Calibrating Metrics For Generation Tasks Using Human
  Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genta Indra Winata, David Anugraha, Lucky Susanto, Garry Kuwanto, Derry Tanti Wijaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the quality of a performance evaluation metric is crucial for
ensuring that model outputs align with human preferences. However, it remains
unclear how well each metric captures the diverse aspects of these preferences,
as metrics often excel in one particular area but not across all dimensions. To
address this, it is essential to systematically calibrate metrics to specific
aspects of human preference, catering to the unique characteristics of each
aspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate
generation tasks across different modalities in a supervised manner.
MetaMetrics optimizes the combination of existing metrics to enhance their
alignment with human preferences. Our metric demonstrates flexibility and
effectiveness in both language and vision downstream tasks, showing significant
benefits across various multilingual and multi-domain scenarios. MetaMetrics
aligns closely with human preferences and is highly extendable and easily
integrable into any application. This makes MetaMetrics a powerful tool for
improving the evaluation of generation tasks, ensuring that metrics are more
representative of human judgment across diverse contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Usage-centric Take on Intent Understanding in E-Commerce <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wendi Zhou, Tianyi Li, Pavlos Vougiouklis, Mark Steedman, Jeff Z. Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying and understanding user intents is a pivotal task for E-Commerce.
Despite its essential role in product recommendation and business user
profiling analysis, intent understanding has not been consistently defined or
accurately benchmarked. In this paper, we focus on predicative user intents as
"how a customer uses a product", and pose intent understanding as a natural
language reasoning task, independent of product ontologies. We identify two
weaknesses of FolkScope, the SOTA E-Commerce Intent Knowledge Graph:
category-rigidity and property-ambiguity. They limit its ability to strongly
align user intents with products having the most desirable property, and to
recommend useful products across diverse categories. Following these
observations, we introduce a Product Recovery Benchmark featuring a novel
evaluation framework and an example dataset. We further validate the above
FolkScope weaknesses on this benchmark. Our code and dataset are available at
https://github.com/stayones/Usgae-Centric-Intent-Understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Acepted by EMNLP 2024 main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Model-Agnostic Multi-Group Equivariant Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09675v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09675v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Razan Baltaji, Sourya Basu, Lav R. Varshney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constructing model-agnostic group equivariant networks, such as equitune
(Basu et al., 2023b) and its generalizations (Kim et al., 2023), can be
computationally expensive for large product groups. We address this problem by
providing efficient model-agnostic equivariant designs for two related
problems: one where the network has multiple inputs each with potentially
different groups acting on them, and another where there is a single input but
the group acting on it is a large product group. For the first design, we
initially consider a linear model and characterize the entire equivariant space
that satisfies this constraint. This characterization gives rise to a novel
fusion layer between different channels that satisfies an invariance-symmetry
(IS) constraint, which we call an IS layer. We then extend this design beyond
linear models, similar to equitune, consisting of equivariant and IS layers. We
also show that the IS layer is a universal approximator of invariant-symmetric
functions. Inspired by the first design, we use the notion of the IS property
to design a second efficient model-agnostic equivariant design for large
product groups acting on a single input. For the first design, we provide
experiments on multi-image classification where each view is transformed
independently with transformations such as rotations. We find equivariant
models are robust to such transformations and perform competitively otherwise.
For the second design, we consider three applications: language
compositionality on the SCAN dataset to product groups; fairness in natural
language generation from GPT-2 to address intersectionality; and robust
zero-shot image classification with CLIP. Overall, our methods are simple and
general, competitive with equitune and its variants, while also being
computationally more efficient.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When "A Helpful Assistant" Is Not Really Helpful: Personas in System
  <span class="highlight-title">Prompt</span>s Do Not Improve Performances of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10054v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10054v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingqian Zheng, Jiaxin Pei, Lajanugen Logeswaran, Moontae Lee, David Jurgens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompting serves as the major way humans interact with Large Language Models
(LLM). Commercial AI systems commonly define the role of the LLM in system
prompts. For example, ChatGPT uses "You are a helpful assistant" as part of its
default system prompt. Despite current practices of adding personas to system
prompts, it remains unclear how different personas affect a model's performance
on objective tasks. In this study, we present a systematic evaluation of
personas in system prompts. We curate a list of 162 roles covering 6 types of
interpersonal relationships and 8 domains of expertise. Through extensive
analysis of 4 popular families of LLMs and 2,410 factual questions, we
demonstrate that adding personas in system prompts does not improve model
performance across a range of questions compared to the control setting where
no persona is added. Nevertheless, further analysis suggests that the gender,
type, and domain of the persona can all influence the resulting prediction
accuracies. We further experimented with a list of persona search strategies
and found that, while aggregating results from the best persona for each
question significantly improves prediction accuracy, automatically identifying
the best persona is challenging, with predictions often performing no better
than random selection. Overall, our findings suggest that while adding a
persona may lead to performance gains in certain settings, the effect of each
persona can be largely random. Code and data are available at
https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Better Instruction-Following Through Minimum Bayes Risk 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02902v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02902v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Wu, Patrick Fernandes, Amanda Bertsch, Seungone Kim, Sina Pakazad, Graham Neubig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General-purpose LLM judges capable of human-level evaluation provide not only
a scalable and accurate way of evaluating instruction-following LLMs but also
new avenues for supervising and improving their performance. One promising way
of leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR)
decoding, which uses a reference-based evaluator to select a high-quality
output from amongst a set of candidate outputs. In the first part of this work,
we explore using MBR decoding as a method for improving the test-time
performance of instruction-following LLMs. We find that MBR decoding with
reference-based LLM judges substantially improves over greedy decoding,
best-of-N decoding with reference-free judges and MBR decoding with lexical and
embedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent
across LLMs with up to 70B parameters, demonstrating that smaller LLM judges
can be used to supervise much larger LLMs. Then, seeking to retain the
improvements from MBR decoding while mitigating additional test-time costs, we
explore iterative self-training on MBR-decoded outputs. We find that
self-training using Direct Preference Optimisation leads to significant
performance gains, such that the self-trained models with greedy decoding
generally match and sometimes exceed the performance of their base models with
MBR decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation noising effectively prevents harmful fine-tuning on LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14577v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14577v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Domenic Rosati, Jan Wehner, Kai Williams, Łukasz Bartoszcze, David Atanasov, Robie Gonzales, Subhabrata Majumdar, Carsten Maple, Hassan Sajjad, Frank Rudzicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Releasing open-source large language models (LLMs) presents a dual-use risk
since bad actors can easily fine-tune these models for harmful purposes. Even
without the open release of weights, weight stealing and fine-tuning APIs make
closed models vulnerable to harmful fine-tuning attacks (HFAs). While safety
measures like preventing jailbreaks and improving safety guardrails are
important, such measures can easily be reversed through fine-tuning. In this
work, we propose Representation Noising (RepNoise), a defence mechanism that is
effective even when attackers have access to the weights. RepNoise works by
removing information about harmful representations such that it is difficult to
recover them during fine-tuning. Importantly, our defence is also able to
generalize across different subsets of harm that have not been seen during the
defence process as long as they are drawn from the same distribution of the
attack set. Our method does not degrade the general capability of LLMs and
retains the ability to train the model on harmless tasks. We provide empirical
evidence that the effectiveness of our defence lies in its "depth": the degree
to which information about harmful representations is removed across all layers
of the LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPs 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Social Bias Probing: Fairness Benchmarking for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09090v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09090v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marta Marchiori Manerba, Karolina Stańczak, Riccardo Guidotti, Isabelle Augenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the impact of social biases in language models has been recognized,
prior methods for bias evaluation have been limited to binary association tests
on small datasets, limiting our understanding of bias complexities. This paper
proposes a novel framework for probing language models for social biases by
assessing disparate treatment, which involves treating individuals differently
according to their affiliation with a sensitive demographic group. We curate
SoFa, a large-scale benchmark designed to address the limitations of existing
fairness collections. SoFa expands the analysis beyond the binary comparison of
stereotypical versus anti-stereotypical identities to include a diverse range
of identities and stereotypes. Comparing our methodology with existing
benchmarks, we reveal that biases within language models are more nuanced than
acknowledged, indicating a broader scope of encoded biases than previously
recognized. Benchmarking LMs on SoFa, we expose how identities expressing
different religions lead to the most pronounced disparate treatments across all
models. Finally, our findings indicate that real-life adversities faced by
various groups such as women and people with disabilities are mirrored in the
behavior of these models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-Tuning and <span class="highlight-title">Prompt</span> Optimization: Two Great Steps that Work Better
  Together <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dilara Soylu, Christopher Potts, Omar Khattab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) systems are increasingly taking the form of
sophisticated modular pipelines, e.g., Retrieval Augmented Generation (RAG),
where each module may involve a distinct Language Model (LM) and an associated
prompt template. These compound systems often lack intermediate labels or
gradient flow to optimize each module, making their end-to-end optimization
challenging. Here we seek strategies to optimize both the module-level LM
weights and the associated prompt templates of such systems to maximize a
downstream task metric. We propose for the first time combining the weight and
prompt optimization strategies to optimize a modular LM pipeline by alternating
between the two to get the same LM to teach itself. In experiments with
multi-hop QA, mathematical reasoning, and feature-based classification using
mistral-7b, llama-2-7b, and llama-3-8b, these BetterTogether strategies
optimizing the weights and prompts of a pipeline together outperform directly
optimizing weights alone and prompts alone by up to 60% and 6%, respectively,
on average across LMs and tasks. BetterTogether optimizer is released in DSPy
at http://dspy.ai
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FAC$^2$E: Better Understanding Large Language Model Capabilities by
  Dissociating Language and Cognition <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00126v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00126v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoqiang Wang, Lingfei Wu, Tengfei Ma, Bang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are primarily evaluated by overall performance
on various text understanding and generation tasks. However, such a paradigm
fails to comprehensively differentiate the fine-grained language and cognitive
skills, rendering the lack of sufficient interpretation to LLMs' capabilities.
In this paper, we present FAC$^2$E, a framework for Fine-grAined and
Cognition-grounded LLMs' Capability Evaluation. Specifically, we formulate
LLMs' evaluation in a multi-dimensional and explainable manner by dissociating
the language-related capabilities and the cognition-related ones. Besides,
through extracting the intermediate reasoning from LLMs, we further break down
the process of applying a specific capability into three sub-steps: recalling
relevant knowledge, utilizing knowledge, and solving problems. Finally,
FAC$^2$E evaluates each sub-step of each fine-grained capability, providing a
two-faceted diagnosis for LLMs. Utilizing FAC$^2$E, we identify a common
shortfall in knowledge utilization among models and propose a straightforward,
knowledge-enhanced method to mitigate this issue. Our results not only showcase
promising performance enhancements but also highlight a direction for future
LLM advancements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-Invasive Suicide Risk Prediction Through Speech Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahin Amiriparian, Maurice Gerczuk, Justina Lutz, Wolfgang Strube, Irina Papazova, Alkomiet Hasan, Alexander Kathan, Björn W. Schuller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The delayed access to specialized psychiatric assessments and care for
patients at risk of suicidal tendencies in emergency departments creates a
notable gap in timely intervention, hindering the provision of adequate mental
health support during critical situations. To address this, we present a
non-invasive, speech-based approach for automatic suicide risk assessment. For
our study, we collected a novel speech recording dataset from $20$ patients. We
extract three sets of features, including wav2vec, interpretable speech and
acoustic features, and deep learning-based spectral representations. We proceed
by conducting a binary classification to assess suicide risk in a
leave-one-subject-out fashion. Our most effective speech model achieves a
balanced accuracy of $66.2\,\%$. Moreover, we show that integrating our speech
model with a series of patients' metadata, such as the history of suicide
attempts or access to firearms, improves the overall result. The metadata
integration yields a balanced accuracy of $94.4\,\%$, marking an absolute
improvement of $28.2\,\%$, demonstrating the efficacy of our proposed
approaches for automatic suicide risk assessment in emergency medicine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Native Design Bias: Studying the Impact of English Nativeness on
  Language Model Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manon Reusens, Philipp Borchert, Jochen De Weerdt, Bart Baesens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel at providing information acquired during
pretraining on large-scale corpora and following instructions through user
prompts. This study investigates whether the quality of LLM responses varies
depending on the demographic profile of users. Considering English as the
global lingua franca, along with the diversity of its dialects among speakers
of different native languages, we explore whether non-native English speakers
receive lower-quality or even factually incorrect responses from LLMs more
frequently. Our results show that performance discrepancies occur when LLMs are
prompted by native versus non-native English speakers and persist when
comparing native speakers from Western countries with others. Additionally, we
find a strong anchoring effect when the model recognizes or is made aware of
the user's nativeness, which further degrades the response quality when
interacting with non-native speakers. Our analysis is based on a newly
collected dataset with over 12,000 unique annotations from 124 annotators,
including information on their native language and English proficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UPCS: Unbiased Persona Construction for Dialogue Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05257v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05257v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuiyun Chen, Yanbin Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Narrative systems, such as dialogue and storytelling systems, often utilize
persona profiles to enhance personalized interactions. Existing persona
profiles frequently exhibit biases, posing risks to system integrity and
fairness. To address this, we introduce the UPCS framework, which categorizes
character descriptions into eight dimensions, including bias mitigation
strategies. Experimental results demonstrate UPCS's superiority in accuracy,
diversity, bias elimination, and user satisfaction, marking a significant
advancement in persona construction for reliable narrative systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diversity Over Size: On the Effect of Sample and Topic Sizes for
  Topic-Dependent Argument Mining <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.11472v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.11472v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Schiller, Johannes Daxenberger, Andreas Waldis, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of Argument Mining, that is extracting and classifying argument
components for a specific topic from large document sources, is an inherently
difficult task for machine learning models and humans alike, as large Argument
Mining datasets are rare and recognition of argument components requires expert
knowledge. The task becomes even more difficult if it also involves stance
detection of retrieved arguments. In this work, we investigate the effect of
Argument Mining dataset composition in few- and zero-shot settings. Our
findings show that, while fine-tuning is mandatory to achieve acceptable model
performance, using carefully composed training samples and reducing the
training sample size by up to almost 90% can still yield 95% of the maximum
performance. This gain is consistent across three Argument Mining tasks on
three different datasets. We also publish a new dataset for future
benchmarking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KV-Compress: Paged KV-Cache Compression with Variable Compression Rates
  per Attention Head 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00161v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00161v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isaac Rehg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context lengths of Large Language Models (LLMs) have exploded in recent
years, with 128k-token context becoming a standard and million-token context
becoming a reality. Efficiently supporting long-context inference remains
challenging as the memory that must be allocated in key-value (KV) cache for a
generation scales with its context length, limiting the number of long-context
requests that can be served concurrently under a given memory budget. KV cache
compression can mitigate this issue by removing under-utilized KVs from each
attention head's cache and reducing its memory footprint. Higher theoretical
compression rates can be achieved when the number of removed KVs varies across
attention heads, but application of such a strategy within existing inference
frameworks adds fragmentation and cannot realize the theoretical compression
rates in physical memory. We introduce KV-Compress, a novel compression method
that evicts contiguous KV blocks within a PagedAttention framework, reducing
the memory footprint of the KV cache proportionally to this theoretical
compression rate. Our method achieves state-of-the-art performance on LongBench
for both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the
total number of compressed KVs by 4x compared with prior methods. Evaluations
on Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression
rates up to 8x with negligible impact on performance, and up to 64x while
retaining over 90% of full-cache performance for all but three of the suite's
subsets. We benchmark an integration of our method with vLLM that increases
total throughput by up to 5.18x by enabling larger decoding batches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoding Intelligence: A Framework for Certifying Knowledge
  Comprehension in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isha Chaudhary, Vedaant V. Jain, Gagandeep Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge comprehension capability is an important aspect of human
intelligence. As Large Language Models (LLMs) are being envisioned as
superhuman agents, it is crucial for them to be proficient at knowledge
comprehension. However, existing benchmarking studies do not provide
consistent, generalizable, and formal guarantees on the knowledge comprehension
capabilities of LLMs. In this work, we propose the first framework to certify
knowledge comprehension in LLMs with formal probabilistic guarantees. Our
certificates are quantitative -- they consist of high-confidence, tight bounds
on the probability that a target LLM gives the correct answer on any knowledge
comprehension prompt sampled from a distribution. We design and certify novel
specifications that precisely represent distributions of knowledge
comprehension prompts leveraging knowledge graphs. We certify SOTA LLMs for
specifications over the Wikidata5m knowledge graph. We find that the knowledge
comprehension capability improves significantly with scaling the size of the
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ First Heuristic Then Rational: Dynamic Use of Heuristics in Language
  Model Reasoning <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16078v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16078v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoichi Aoki, Keito Kudo, Tatsuki Kuribayashi, Shusaku Sone, Masaya Taniguchi, Keisuke Sakaguchi, Kentaro Inui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-step reasoning instruction, such as chain-of-thought prompting, is
widely adopted to explore better language models (LMs) performance. We report
on the systematic strategy that LMs employ in such a multi-step reasoning
process. Our controlled experiments reveal that LMs rely more heavily on
heuristics, such as lexical overlap, in the earlier stages of reasoning, where
more reasoning steps remain to reach a goal. Conversely, their reliance on
heuristics decreases as LMs progress closer to the final answer through
multiple reasoning steps. This suggests that LMs can backtrack only a limited
number of future steps and dynamically combine heuristic strategies with
rationale ones in tasks involving multi-step reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs Know More Than They Show: On the Intrinsic Representation of LLM
  Hallucinations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02707v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02707v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often produce errors, including factual
inaccuracies, biases, and reasoning failures, collectively referred to as
"hallucinations". Recent studies have demonstrated that LLMs' internal states
encode information regarding the truthfulness of their outputs, and that this
information can be utilized to detect errors. In this work, we show that the
internal representations of LLMs encode much more information about
truthfulness than previously recognized. We first discover that the
truthfulness information is concentrated in specific tokens, and leveraging
this property significantly enhances error detection performance. Yet, we show
that such error detectors fail to generalize across datasets, implying that --
contrary to prior claims -- truthfulness encoding is not universal but rather
multifaceted. Next, we show that internal representations can also be used for
predicting the types of errors the model is likely to make, facilitating the
development of tailored mitigation strategies. Lastly, we reveal a discrepancy
between LLMs' internal encoding and external behavior: they may encode the
correct answer, yet consistently generate an incorrect one. Taken together,
these insights deepen our understanding of LLM errors from the model's internal
perspective, which can guide future research on enhancing error analysis and
mitigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StructLM: Towards Building Generalist Models for Structured Knowledge
  Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16671v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16671v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Zhuang, Ge Zhang, Tianyu Zheng, Xinrun Du, Junjie Wang, Weiming Ren, Stephen W. Huang, Jie Fu, Xiang Yue, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structured data sources, such as tables, graphs, and databases, are
ubiquitous knowledge sources. Despite the demonstrated capabilities of large
language models (LLMs) on plain text, their proficiency in interpreting and
utilizing structured data remains limited. Our investigation reveals a notable
deficiency in LLMs' ability to process structured data, e.g., ChatGPT lags
behind state-of-the-art (SoTA) model by an average of 35%. To augment the
Structured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a
comprehensive instruction tuning dataset comprising 1.1 million examples.
Utilizing this dataset, we train a series of models, referred to as StructLM,
based on the Mistral and the CodeLlama model family, ranging from 7B to 34B
parameters. Our StructLM series surpasses task-specific models on 16 out of 18
evaluated datasets and establishes new SoTA performance on 8 SKG tasks.
Furthermore, StructLM demonstrates strong generalization across 6 novel
held-out SKG tasks, outperforming TableLlama by an average of 35\% and Flan-UL2
20B by an average of 10\%. Contrary to expectations, we observe that scaling
model size offers marginal benefits, with StructLM-34B showing only slight
improvements over StructLM-7B. This suggests that structured knowledge
grounding is still a challenging task and requires more innovative design to
push to a new level.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of
  Large Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14768v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14768v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) need knowledge updates to meet the ever-growing
world facts and correct the hallucinated responses, facilitating the methods of
lifelong model editing. Where the updated knowledge resides in memories is a
fundamental question for model editing. In this paper, we find that editing
either long-term memory (direct model parameters) or working memory
(non-parametric knowledge of neural network activations/representations by
retrieval) will result in an impossible triangle -- reliability,
generalization, and locality can not be realized together in the lifelong
editing settings. For long-term memory, directly editing the parameters will
cause conflicts with irrelevant pretrained knowledge or previous edits (poor
reliability and locality). For working memory, retrieval-based activations can
hardly make the model understand the edits and generalize (poor
generalization). Therefore, we propose WISE to bridge the gap between memories.
In WISE, we design a dual parametric memory scheme, which consists of the main
memory for the pretrained knowledge and a side memory for the edited knowledge.
We only edit the knowledge in the side memory and train a router to decide
which memory to go through when given a query. For continual editing, we devise
a knowledge-sharding mechanism where different sets of edits reside in distinct
subspaces of parameters, and are subsequently merged into a shared memory
without conflicts. Extensive experiments show that WISE can outperform previous
model editing methods and overcome the impossible triangle under lifelong model
editing of question answering, hallucination, and out-of-distribution settings
across trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is
available at https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-MoE: Towards Compositional Large Language Models with
  Self-Specialized Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12034v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12034v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junmo Kang, Leonid Karlinsky, Hongyin Luo, Zhen Wang, Jacob Hansen, James Glass, David Cox, Rameswar Panda, Rogerio Feris, Alan Ritter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Self-MoE, an approach that transforms a monolithic LLM into a
compositional, modular system of self-specialized experts, named MiXSE (MiXture
of Self-specialized Experts). Our approach leverages self-specialization, which
constructs expert modules using self-generated synthetic data, each equipping a
shared base LLM with distinct domain-specific capabilities, activated via
self-optimized routing. This allows for dynamic and capability-specific
handling of various target tasks, enhancing overall capabilities, without
extensive human-labeled data and added parameters. Our empirical results reveal
that specializing LLMs may exhibit potential trade-offs in performances on
non-specialized tasks. On the other hand, our Self-MoE demonstrates substantial
improvements (6.5%p on average) over the base LLM across diverse benchmarks
such as knowledge, reasoning, math, and coding. It also consistently
outperforms other methods, including instance merging and weight merging, while
offering better flexibility and interpretability by design with semantic
experts and routing. Our findings highlight the critical role of modularity,
the applicability of Self-MoE to multiple base LLMs, and the potential of
self-improvement in achieving efficient, scalable, and adaptable systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Russian Jeopardy! Data Set for Question-Answering Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.02325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.02325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elena Mikhalkova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering (QA) is one of the most common NLP tasks that relates to
named entity recognition, fact extraction, semantic search and some other
fields. In industry, it is much appreciated in chatbots and corporate
information systems. It is also a challenging task that attracted the attention
of a very general audience at the quiz show Jeopardy! In this article we
describe a Jeopardy!-like Russian QA data set collected from the official
Russian quiz database Chgk (che ge ka). The data set includes 379,284 quiz-like
questions with 29,375 from the Russian analogue of Jeopardy! - "Own Game". We
observe its linguistic features and the related QA-task. We conclude about
perspectives of a QA competition based on the data set collected from this
database.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances
  Retrieval-Augmented Generation with Zero Inference Overhead 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19745v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19745v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Tan, Yining Qian, Ang Lv, Hongzhan Lin, Songhao Wu, Yongbo Wang, Feng Wang, Jingtong Wu, Xin Lu, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) enhanced with retrieval-augmented generation
(RAG) have introduced a new paradigm for web search. However, the limited
context awareness of LLMs degrades their performance on RAG tasks. Existing
methods to enhance context awareness are often inefficient, incurring time or
memory overhead during inference, and many are tailored to specific position
embeddings. In this paper, we propose Position-Embedding-Agnostic attention
Re-weighting (PEAR), which enhances the context awareness of LLMs with zero
inference overhead. Specifically, on a proxy task focused on context copying,
we first detect heads which suppress the models' context awareness thereby
diminishing RAG performance. To weaken the impact of these heads, we re-weight
their outputs with learnable coefficients. The LLM (with frozen parameters) is
optimized by adjusting these coefficients to minimize loss on the proxy task.
As a result, the coefficients are optimized to values less than one, thereby
reducing their tendency to suppress RAG performance. During inference, the
optimized coefficients are fixed to re-weight these heads, regardless of the
specific task at hand. Our proposed PEAR offers two major advantages over
previous approaches: (1) It introduces zero additional inference overhead in
terms of memory usage or inference time, while outperforming competitive
baselines in accuracy and efficiency across various RAG tasks. (2) It is
independent of position embedding algorithms, ensuring broader applicability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WellDunn: On the Robustness and Explainability of Language Models and
  Large Language Models in Identifying Wellness Dimensions <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12058v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12058v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyedali Mohammadi, Edward Raff, Jinendra Malekar, Vedant Palit, Francis Ferraro, Manas Gaur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Models (LMs) are being proposed for mental health applications where
the heightened risk of adverse outcomes means predictive performance may not be
a sufficient litmus test of a model's utility in clinical practice. A model
that can be trusted for practice should have a correspondence between
explanation and clinical determination, yet no prior research has examined the
attention fidelity of these models and their effect on ground truth
explanations. We introduce an evaluation design that focuses on the robustness
and explainability of LMs in identifying Wellness Dimensions (WDs). We focus on
two existing mental health and well-being datasets: (a) Multi-label
Classification-based MultiWD, and (b) WellXplain for evaluating attention
mechanism veracity against expert-labeled explanations. The labels are based on
Halbert Dunn's theory of wellness, which gives grounding to our evaluation. We
reveal four surprising results about LMs/LLMs: (1) Despite their human-like
capabilities, GPT-3.5/4 lag behind RoBERTa, and MedAlpaca, a fine-tuned LLM on
WellXplain fails to deliver any remarkable improvements in performance or
explanations. (2) Re-examining LMs' predictions based on a confidence-oriented
loss function reveals a significant performance drop. (3) Across all LMs/LLMs,
the alignment between attention and explanations remains low, with LLMs scoring
a dismal 0.0. (4) Most mental health-specific LMs/LLMs overlook domain-specific
knowledge and undervalue explanations, causing these discrepancies. This study
highlights the need for further research into their consistency and
explanations in mental health and well-being.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in BlackboxNLP @ EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Can <span class="highlight-title">Transformer</span>s Count to n? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15160v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15160v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gilad Yehudai, Haim Kaplan, Asma Ghandeharioun, Mor Geva, Amir Globerson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models based on the transformer architectures can solve highly
complex tasks. But are there simple tasks that such models cannot solve? Here
we focus on very simple counting tasks, that involve counting how many times a
token in the vocabulary have appeared in a string. We show that if the
dimension of the transformer state is linear in the context length, this task
can be solved. However, the solution we propose does not scale beyond this
limit, and we provide theoretical arguments for why it is likely impossible for
a size limited transformer to implement this task. Our empirical results
demonstrate the same phase-transition in performance, as anticipated by the
theoretical argument. Our results demonstrate the importance of understanding
how transformers can solve simple tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tokenization Is More Than Compression <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18376v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18376v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Craig W. Schmidt, Varshini Reddy, Haoran Zhang, Alec Alameddine, Omri Uzan, Yuval Pinter, Chris Tanner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tokenization is a foundational step in natural language processing (NLP)
tasks, bridging raw text and language models. Existing tokenization approaches
like Byte-Pair Encoding (BPE) originate from the field of data compression, and
it has been suggested that the effectiveness of BPE stems from its ability to
condense text into a relatively small number of tokens. We test the hypothesis
that fewer tokens lead to better downstream performance by introducing
PathPiece, a new tokenizer that segments a document's text into the minimum
number of tokens for a given vocabulary. Through extensive experimentation we
find this hypothesis not to be the case, casting doubt on the understanding of
the reasons for effective tokenization. To examine which other factors play a
role, we evaluate design decisions across all three phases of tokenization:
pre-tokenization, vocabulary construction, and segmentation, offering new
insights into the design of effective tokenizers. Specifically, we illustrate
the importance of pre-tokenization and the benefits of using BPE to initialize
vocabulary construction. We train 64 language models with varying tokenization,
ranging in size from 350M to 2.4B parameters, all of which are made publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ComplexTempQA: A Large-Scale <span class="highlight-title">Dataset</span> for Complex Temporal Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04866v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04866v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphael Gruber, Abdelrahman Abdallah, Michael Färber, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ComplexTempQA, a large-scale dataset consisting of over 100
million question-answer pairs designed to tackle the challenges in temporal
question answering. ComplexTempQA significantly surpasses existing benchmarks
like HOTPOTQA, TORQUE, and TEQUILA in scale and scope. Utilizing data from
Wikipedia and Wikidata, the dataset covers questions spanning over two decades
and offers an unmatched breadth of topics. We introduce a unique taxonomy that
categorizes questions as attributes, comparisons, and counting questions, each
revolving around events, entities, and time periods. One standout feature of
ComplexTempQA is the high complexity of its questions, which demand effective
capabilities for answering such as across-time comparison, temporal
aggregation, and multi-hop reasoning involving temporal event ordering and
entity recognition. Additionally, each question is accompanied by detailed
metadata, including specific time scopes, allowing for comprehensive evaluation
and enhancement of the temporal reasoning abilities of large language models.
ComplexTempQA serves both as a testing ground for developing sophisticated AI
models and as a foundation for advancing research in question answering,
information retrieval, and language understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reconstruct Your Previous Conversations! Comprehensively Investigating
  Privacy Leakage Risks in Conversations with <span class="highlight-title">GPT</span> Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02987v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02987v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Chu, Zeyang Sha, Michael Backes, Yang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant advancements have recently been made in large language models
represented by GPT models. Users frequently have multi-round private
conversations with cloud-hosted GPT models for task optimization. Yet, this
operational paradigm introduces additional attack surfaces, particularly in
custom GPTs and hijacked chat sessions. In this paper, we introduce a
straightforward yet potent Conversation Reconstruction Attack. This attack
targets the contents of previous conversations between GPT models and benign
users, i.e., the benign users' input contents during their interaction with GPT
models. The adversary could induce GPT models to leak such contents by querying
them with designed malicious prompts. Our comprehensive examination of privacy
risks during the interactions with GPT models under this attack reveals GPT-4's
considerable resilience. We present two advanced attacks targeting improved
reconstruction of past conversations, demonstrating significant privacy leakage
across all models under these advanced techniques. Evaluating various defense
mechanisms, we find them ineffective against these attacks. Our findings
highlight the ease with which privacy can be compromised in interactions with
GPT models, urging the community to safeguard against potential abuses of these
models' capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in EMNLP 2024. 14 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Question Decomposition on Multimodal Large Language Models <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19339v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19339v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Zhang, Jianzhe Liu, Zhen Han, Shuo Chen, Bailan He, Volker Tresp, Zhiqiang Xu, Jindong Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question decomposition has emerged as an effective strategy for prompting
Large Language Models (LLMs) to answer complex questions. However, while
existing methods primarily focus on unimodal language models, the question
decomposition capability of Multimodal Large Language Models (MLLMs) has yet to
be explored. To this end, this paper explores visual question decomposition on
MLLMs. Specifically, we introduce a systematic evaluation framework including a
dataset and several evaluation criteria to assess the quality of the decomposed
sub-questions, revealing that existing MLLMs struggle to produce high-quality
sub-questions. To address this limitation, we propose a specific finetuning
dataset, DecoVQA+, for enhancing the model's question decomposition capability.
Aiming at enabling models to perform appropriate selective decomposition, we
propose an efficient finetuning pipeline. The finetuning pipeline consists of
our proposed dataset and a training objective for selective decomposition.
Finetuned MLLMs demonstrate significant improvements in the quality of
sub-questions and the policy of selective question decomposition. Additionally,
the models also achieve higher accuracy with selective decomposition on VQA
benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17023v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17023v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Vera Marjanović, Haeun Yu, Pepa Atanasova, Maria Maistro, Christina Lioma, Isabelle Augenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge-intensive language understanding tasks require Language Models
(LMs) to integrate relevant context, mitigating their inherent weaknesses, such
as incomplete or outdated knowledge. However, conflicting knowledge can be
present in the LM's parameters, termed intra-memory conflict, which can affect
a model's propensity to accept contextual knowledge. To study the effect of
intra-memory conflict on an LM's ability to accept relevant context, we utilize
two knowledge conflict measures and a novel dataset containing inherently
conflicting data, DynamicQA. This dataset includes facts with a temporal
dynamic nature where facts can change over time and disputable dynamic facts,
which can change depending on the viewpoint. DynamicQA is the first to include
real-world knowledge conflicts and provide context to study the link between
the different types of knowledge conflicts. We also evaluate several measures
on their ability to reflect the presence of intra-memory conflict: semantic
entropy and a novel coherent persuasion score. With our extensive experiments,
we verify that LMs exhibit a greater degree of intra-memory conflict with
dynamic facts compared to facts that have a single truth value. Furthermore, we
reveal that facts with intra-memory conflict are harder to update with context,
suggesting that retrieval-augmented generation will struggle with the most
commonly adapted facts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures, Accepted to Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Residual Stream Analysis with Multi-Layer SAEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Lawson, Lucy Farnik, Conor Houghton, Laurence Aitchison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse autoencoders (SAEs) are a promising approach to interpreting the
internal representations of transformer language models. However, SAEs are
usually trained separately on each transformer layer, making it difficult to
use them to study how information flows across layers. To solve this problem,
we introduce the multi-layer SAE (MLSAE): a single SAE trained on the residual
stream activation vectors from every transformer layer. Given that the residual
stream is understood to preserve information across layers, we expected MLSAE
latents to `switch on' at a token position and remain active at later layers.
Interestingly, we find that individual latents are often active at a single
layer for a given token or prompt, but this layer may differ for different
tokens or prompts. We quantify these phenomena by defining a distribution over
layers and considering its variance. We find that the variance of the
distributions of latent activations over layers is about two orders of
magnitude greater when aggregating over tokens compared with a single token.
For larger underlying models, the degree to which latents are active at
multiple layers increases, which is consistent with the fact that the residual
stream activation vectors at adjacent layers become more similar. Finally, we
relax the assumption that the residual stream basis is the same at every layer
by applying pre-trained tuned-lens transformations, but our findings remain
qualitatively similar. Our results represent a new approach to understanding
how representations change as they flow through transformers. We release our
code to train and analyze MLSAEs at https://github.com/tim-lawson/mlsae.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 26 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning
  with Knowledge-guided Retrieval Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10805v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10805v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Cehao Yang, Jiaxin Mao, Jian Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has enhanced large language models
(LLMs) by using knowledge retrieval to address knowledge gaps. However,
existing RAG approaches often fail to ensure the depth and completeness of the
information retrieved, which is essential for complex reasoning tasks. In this
work, we present Think-on-Graph 2.0 (ToG-2), a hybrid RAG framework that
iteratively retrieves information from both unstructured and structured
knowledge sources in a tightly integrated manner. Specifically, ToG-2 leverages
knowledge graphs (KGs) to connect documents via entities, facilitating deep and
knowledge-guided context retrieval. Simultaneously, it uses documents as entity
contexts to enable precise and efficient graph retrieval.
  ToG-2 alternates between graph retrieval and context retrieval to search for
in-depth clues relevant to the question, enabling LLMs to generate accurate
answers. We conduct a series of experiments to demonstrate the following
advantages of ToG-2: (1) ToG-2 tightly integrates context retrieval and graph
retrieval, enhancing context retrieval through the KG while enabling reliable
graph retrieval based on contexts; (2) it achieves deep and faithful reasoning
in LLMs through an iterative knowledge retrieval process that integrates
contexts and the KG; and (3) ToG-2 is training-free and compatible with various
LLMs as a plug-and-play solution. Extensive experiments show that ToG-2
achieves state-of-the-art (SOTA) performance on 6 out of 7 knowledge-intensive
datasets with GPT-3.5, and can elevate the performance of smaller models (e.g.,
LLAMA-2-13B) to the level of GPT-3.5's direct reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language in Vivo vs. in Silico: Size Matters but Larger Language Models
  Still Do Not Comprehend Language on a Par with Humans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14883v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14883v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vittoria Dentella, Fritz Guenther, Evelina Leivada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the limits of language is a prerequisite for Large Language
Models (LLMs) to act as theories of natural language. LLM performance in some
language tasks presents both quantitative and qualitative differences from that
of humans, however it remains to be determined whether such differences are
amenable to model size. This work investigates the critical role of model
scaling, determining whether increases in size make up for such differences
between humans and models. We test three LLMs from different families (Bard,
137 billion parameters; ChatGPT-3.5, 175 billion; ChatGPT-4, 1.5 trillion) on a
grammaticality judgment task featuring anaphora, center embedding,
comparatives, and negative polarity. N=1,200 judgments are collected and scored
for accuracy, stability, and improvements in accuracy upon repeated
presentation of a prompt. Results of the best performing LLM, ChatGPT-4, are
compared to results of n=80 humans on the same stimuli. We find that humans are
overall less accurate than ChatGPT-4 (76% vs. 80% accuracy, respectively), but
that this is due to ChatGPT-4 outperforming humans only in one task condition,
namely on grammatical sentences. Additionally, ChatGPT-4 wavers more than
humans in its answers (12.5% vs. 9.6% likelihood of an oscillating answer,
respectively). Thus, while increased model size may lead to better performance,
LLMs are still not sensitive to (un)grammaticality the same way as humans are.
It seems possible but unlikely that scaling alone can fix this issue. We
interpret these results by comparing language learning in vivo and in silico,
identifying three critical differences concerning (i) the type of evidence,
(ii) the poverty of the stimulus, and (iii) the occurrence of semantic
hallucinations due to impenetrable linguistic reference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.05930v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.05930v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jushi Kai, Tianhang Zhang, Hai Hu, Zhouhan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate great performance in text
generation. However, LLMs are still suffering from hallucinations. In this
work, we propose an inference-time method, Self-Highlighted Hesitation (SH2),
to help LLMs decode more truthfully. SH2 is based on a simple fact rooted in
information theory that for an LLM, the tokens predicted with lower
probabilities are prone to be more informative than others. Our analysis shows
that the tokens assigned with lower probabilities by an LLM are more likely to
be closely related to factual information, such as nouns, proper nouns, and
adjectives. Therefore, we propose to ''highlight'' the factual information by
selecting the tokens with the lowest probabilities and concatenating them to
the original context, thus forcing the model to repeatedly read and hesitate on
these tokens before generation. During decoding, we also adopt contrastive
decoding to emphasize the difference in the output probabilities brought by the
hesitation. Experimental results demonstrate that our SH2, requiring no
additional data or models, can effectively help LLMs elicit factual knowledge
and distinguish hallucinated contexts. Significant and consistent improvements
are achieved by SH2 for LLaMA-7b, LLaMA2-7b and Mistral-7b on multiple
hallucination tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CBF-LLM: Safe Control for LLM Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15625v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15625v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuya Miyaoka, Masaki Inoue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a control-based framework for aligning large language
models (LLMs) by leveraging a control barrier function (CBF) to ensure
user-desirable text generation. The presented framework applies the safety
filter, designed based on the CBF, to the output generation of the baseline
LLM, i.e., the sequence of the token, with the aim of intervening in the
generated text. The overall text-generation system is implemented with Llama 3
and a RoBERTa model, and the source code is available at
https://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control
ability and effectiveness in reducing the number of interventions needed for
user-specified alignment tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparison of Language Modeling and Translation as Multilingual
  <span class="highlight-title">Pretrain</span>ing Objectives <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Li, Shaoxiong Ji, Timothee Mickus, Vincent Segonne, Jörg Tiedemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models (PLMs) display impressive performances and have
captured the attention of the NLP community. Establishing best practices in
pretraining has, therefore, become a major focus of NLP research, especially
since insights gained from monolingual English models may not necessarily apply
to more complex multilingual models. One significant caveat of the current
state of the art is that different works are rarely comparable: they often
discuss different parameter counts, training data, and evaluation methodology.
  This paper proposes a comparison of multilingual pretraining objectives in a
controlled methodological environment. We ensure that training data and model
architectures are comparable, and discuss the downstream performances across 6
languages that we observe in probing and fine-tuning scenarios. We make two key
observations: (1) the architecture dictates which pretraining objective is
optimal; (2) multilingual translation is a very effective pretraining objective
under the right conditions. We make our code, data, and model weights available
at \texttt{\url{https://github.com/Helsinki-NLP/lm-vs-mt}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine
  Translation with a Human-centered Study <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00545v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00545v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beatrice Savoldi, Sara Papi, Matteo Negri, Ana Guerberof, Luisa Bentivogli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gender bias in machine translation (MT) is recognized as an issue that can
harm people and society. And yet, advancements in the field rarely involve
people, the final MT users, or inform how they might be impacted by biased
technologies. Current evaluations are often restricted to automatic methods,
which offer an opaque estimate of what the downstream impact of gender
disparities might be. We conduct an extensive human-centered study to examine
if and to what extent bias in MT brings harms with tangible costs, such as
quality of service gaps across women and men. To this aim, we collect
behavioral data from 90 participants, who post-edited MT outputs to ensure
correct gender translation. Across multiple datasets, languages, and types of
users, our study shows that feminine post-editing demands significantly more
technical and temporal effort, also corresponding to higher financial costs.
Existing bias measurements, however, fail to reflect the found disparities. Our
findings advocate for human-centered approaches that can inform the societal
impact of bias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted ad EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OffsetBias: Leveraging Debiased Data for Tuning Evaluators <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung Kim, Sanghyuk Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Employing Large Language Models (LLMs) to assess the quality of generated
responses, such as prompting instruct-tuned models or fine-tuning judge models,
has become a widely adopted evaluation method. It is also known that such
evaluators are vulnerable to biases, such as favoring longer responses. While
it is important to overcome this problem, the specifics of these biases remain
under-explored. In this work, we qualitatively identify six types of biases
inherent in various judge models. We propose EvalBiasBench as a meta-evaluation
collection of hand-crafted test cases for each bias type. Additionally, we
present de-biasing dataset construction methods and the associated preference
dataset OffsetBias. Experimental results demonstrate that fine-tuning on our
dataset significantly enhances the robustness of judge models against biases
and improves performance across most evaluation scenarios. We release our
datasets and the fine-tuned judge model to public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Large Language Models Understand Symbolic Graphics Programs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08313v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08313v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Z. Xiao, Katherine M. Collins, Joshua B. Tenenbaum, Adrian Weller, Michael J. Black, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Against the backdrop of enthusiasm for large language models (LLMs), there is
an urgent need to scientifically assess their capabilities and shortcomings.
This is nontrivial in part because it is difficult to find tasks which the
models have not encountered during training. Utilizing symbolic graphics
programs, we propose a domain well-suited to test multiple spatial-semantic
reasoning skills of LLMs. Popular in computer graphics, these programs
procedurally generate visual data. While LLMs exhibit impressive skills in
general program synthesis and analysis, symbolic graphics programs offer a new
layer of evaluation: they allow us to test an LLM's ability to answer
different-grained semantic-level questions of the images or 3D geometries
without a vision encoder. To semantically understand the symbolic programs,
LLMs would need to possess the ability to "imagine" and reason how the
corresponding graphics content would look with only the symbolic description.
We use this task to evaluate LLMs by creating a large benchmark for the
semantic visual understanding of symbolic graphics programs, built procedurally
with minimal human effort. Particular emphasis is placed on transformations of
images that leave the image level semantics invariant while introducing
significant changes to the underlying program. We evaluate commercial and
open-source LLMs on our benchmark to assess their ability to reason about
visual output of programs, finding that LLMs considered stronger at reasoning
generally perform better. Lastly, we introduce a novel method to improve this
ability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned
with pre-collected instruction data on symbolic graphics programs.
Interestingly, we find that SIT not only improves LLM's understanding on
symbolic programs, but it also improves general reasoning ability on various
other benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report v2 (46 pages, 24 figures, project page:
  https://sgp-bench.github.io/, substantial update from v1)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse
  Representation Adjustment in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02298v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02298v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guobin Shen, Dongcheng Zhao, Yiting Dong, Xiang He, Yi Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become integral to various applications,
ensuring both their safety and utility is paramount. Jailbreak attacks, which
manipulate LLMs into generating harmful content, pose significant challenges to
this balance. Existing defenses, such as prompt engineering and safety
fine-tuning, often introduce computational overhead, increase inference
latency, and lack runtime flexibility. Moreover, overly restrictive safety
measures can degrade model utility by causing refusals of benign queries. In
this paper, we introduce Jailbreak Antidote, a method that enables real-time
adjustment of LLM safety preferences by manipulating a sparse subset of the
model's internal states during inference. By shifting the model's hidden
representations along a safety direction with varying strengths, we achieve
flexible control over the safety-utility balance without additional token
overhead or inference delays. Our analysis reveals that safety-related
information in LLMs is sparsely distributed; adjusting approximately 5% of the
internal state is as effective as modifying the entire state. Extensive
experiments on nine LLMs (ranging from 2 billion to 72 billion parameters),
evaluated against ten jailbreak attack methods and compared with six defense
strategies, validate the effectiveness and efficiency of our approach. By
directly manipulating internal states during reasoning, Jailbreak Antidote
offers a lightweight, scalable solution that enhances LLM safety while
preserving utility, opening new possibilities for real-time safety mechanisms
in widely-deployed AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a Universal Method for Meaningful Signal Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00016v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00016v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louis Mahon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is known that human speech and certain animal vocalizations can convey
meaningful content because we can decipher the content that a given utterance
does convey. This paper explores an alternative approach to determining whether
a signal is meaningful, one that analyzes only the signal itself and is
independent of what the conveyed meaning might be. We devise a method that
takes a waveform as input and outputs a score indicating its degree of
`meaningfulness`. We cluster contiguous portions of the input to minimize the
total description length, and then take the length of the code of the assigned
cluster labels as meaningfulness score. We evaluate our method empirically,
against several baselines, and show that it is the only one to give a high
score to human speech in various languages and with various speakers, a
moderate score to animal vocalizations from birds and orcas, and a low score to
ambient noise from various sources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Persuasion: Towards Conversational Recommender System with
  Credible Explanations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14399v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14399v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peixin Qin, Chen Huang, Yang Deng, Wenqiang Lei, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the aid of large language models, current conversational recommender
system (CRS) has gaining strong abilities to persuade users to accept
recommended items. While these CRSs are highly persuasive, they can mislead
users by incorporating incredible information in their explanations, ultimately
damaging the long-term trust between users and the CRS. To address this, we
propose a simple yet effective method, called PC-CRS, to enhance the
credibility of CRS's explanations during persuasion. It guides the explanation
generation through our proposed credibility-aware persuasive strategies and
then gradually refines explanations via post-hoc self-reflection. Experimental
results demonstrate the efficacy of PC-CRS in promoting persuasive and credible
explanations. Further analysis reveals the reason behind current methods
producing incredible explanations and the potential of credible explanations to
improve recommendation accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2024. Our code is available at
  https://github.com/mumen798/PC-CRS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ColPali: Efficient Document Retrieval with Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01449v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01449v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, Pierre Colombo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Documents are visually rich structures that convey information through text,
as well as tables, figures, page layouts, or fonts. While modern document
retrieval systems exhibit strong performance on query-to-text matching, they
struggle to exploit visual cues efficiently, hindering their performance on
practical document retrieval applications such as Retrieval Augmented
Generation. To benchmark current systems on visually rich document retrieval,
we introduce the Visual Document Retrieval Benchmark ViDoRe, composed of
various page-level retrieving tasks spanning multiple domains, languages, and
settings. The inherent shortcomings of modern systems motivate the introduction
of a new retrieval model architecture, ColPali, which leverages the document
understanding capabilities of recent Vision Language Models to produce
high-quality contextualized embeddings solely from images of document pages.
Combined with a late interaction matching mechanism, ColPali largely
outperforms modern document retrieval pipelines while being drastically faster
and end-to-end trainable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lighthouse: A User-Friendly Library for Reproducible Video Moment
  Retrieval and Highlight Detection <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02901v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02901v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taichi Nishimura, Shota Nakada, Hokuto Munakata, Tatsuya Komatsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Lighthouse, a user-friendly library for reproducible video moment
retrieval and highlight detection (MR-HD). Although researchers proposed
various MR-HD approaches, the research community holds two main issues. The
first is a lack of comprehensive and reproducible experiments across various
methods, datasets, and video-text features. This is because no unified training
and evaluation codebase covers multiple settings. The second is user-unfriendly
design. Because previous works use different libraries, researchers set up
individual environments. In addition, most works release only the training
codes, requiring users to implement the whole inference process of MR-HD.
Lighthouse addresses these issues by implementing a unified reproducible
codebase that includes six models, three features, and five datasets. In
addition, it provides an inference API and web demo to make these methods
easily accessible for researchers and developers. Our experiments demonstrate
that Lighthouse generally reproduces the reported scores in the reference
papers. The code is available at https://github.com/line/lighthouse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at EMNLP2024 - system demonstration track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Typing to Listen at the Cocktail Party: Text-Guided Target Speaker
  Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07284v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07284v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Hao, Jibin Wu, Jianwei Yu, Chenglin Xu, Kay Chen Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans can easily isolate a single speaker from a complex acoustic
environment, a capability referred to as the "Cocktail Party Effect." However,
replicating this ability has been a significant challenge in the field of
target speaker extraction (TSE). Traditional TSE approaches predominantly rely
on voiceprints, which raise privacy concerns and face issues related to the
quality and availability of enrollment samples, as well as intra-speaker
variability. To address these issues, this work introduces a novel text-guided
TSE paradigm named LLM-TSE. In this paradigm, a state-of-the-art large language
model, LLaMA 2, processes typed text input from users to extract semantic cues.
We demonstrate that textual descriptions alone can effectively serve as cues
for extraction, thus addressing privacy concerns and reducing dependency on
voiceprints. Furthermore, our approach offers flexibility by allowing the user
to specify the extraction or suppression of a speaker and enhances robustness
against intra-speaker variability by incorporating context-dependent textual
information. Experimental results show competitive performance with text-based
cues alone and demonstrate the effectiveness of using text as a task selector.
Additionally, they achieve a new state-of-the-art when combining text-based
cues with pre-registered cues. This work represents the first integration of
LLMs with TSE, potentially establishing a new benchmark in solving the cocktail
party problem and expanding the scope of TSE applications by providing a
versatile, privacy-conscious solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review, https://github.com/haoxiangsnr/llm-tse</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Contrastive Decoding in Retrieval-Augmented Generation for
  Handling Noisy Contexts <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01084v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01084v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youna Kim, Hyuhng Joon Kim, Cheonbok Park, Choonghyun Park, Hyunsoo Cho, Junyeob Kim, Kang Min Yoo, Sang-goo Lee, Taeuk Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When using large language models (LLMs) in knowledge-intensive tasks, such as
open-domain question answering, external context can bridge the gap between
external knowledge and the LLMs' parametric knowledge. Recent research has been
developed to amplify contextual knowledge over the parametric knowledge of LLMs
with contrastive decoding approaches. While these approaches could yield
truthful responses when relevant context is provided, they are prone to
vulnerabilities when faced with noisy contexts. We extend the scope of previous
studies to encompass noisy contexts and propose adaptive contrastive decoding
(ACD) to leverage contextual influence effectively. ACD demonstrates
improvements in open-domain question answering tasks compared to baselines,
especially in robustness by remaining undistracted by noisy contexts in
retrieval-augmented generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IndicVoices-R: Unlocking a Massive Multilingual Multi-speaker Speech
  Corpus for Scaling Indian TTS <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05356v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05356v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashwin Sankar, Srija Anand, Praveen Srinivasa Varadhan, Sherry Thomas, Mehak Singal, Shridhar Kumar, Deovrat Mehendale, Aditi Krishana, Giri Raju, Mitesh Khapra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in text-to-speech (TTS) synthesis show that large-scale
models trained with extensive web data produce highly natural-sounding output.
However, such data is scarce for Indian languages due to the lack of
high-quality, manually subtitled data on platforms like LibriVox or YouTube. To
address this gap, we enhance existing large-scale ASR datasets containing
natural conversations collected in low-quality environments to generate
high-quality TTS training data. Our pipeline leverages the cross-lingual
generalization of denoising and speech enhancement models trained on English
and applied to Indian languages. This results in IndicVoices-R (IV-R), the
largest multilingual Indian TTS dataset derived from an ASR dataset, with 1,704
hours of high-quality speech from 10,496 speakers across 22 Indian languages.
IV-R matches the quality of gold-standard TTS datasets like LJSpeech, LibriTTS,
and IndicTTS. We also introduce the IV-R Benchmark, the first to assess
zero-shot, few-shot, and many-shot speaker generalization capabilities of TTS
models on Indian voices, ensuring diversity in age, gender, and style. We
demonstrate that fine-tuning an English pre-trained model on a combined dataset
of high-quality IndicTTS and our IV-R dataset results in better zero-shot
speaker generalization compared to fine-tuning on the IndicTTS dataset alone.
Further, our evaluation reveals limited zero-shot generalization for Indian
voices in TTS models trained on prior datasets, which we improve by fine-tuning
the model on our data containing diverse set of speakers across language
families. We open-source all data and code, releasing the first TTS model for
all 22 official Indian languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024 Datasets and Benchmarks track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMR Scaling Law: Predicting Critical Mixture Ratios for Continual
  <span class="highlight-title">Pre-train</span>ing of Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17467v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17467v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Gu, Zacc Yang, Chuanghao Ding, Rui Zhao, Fei Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel in diverse tasks but often underperform in
specialized fields due to limited domain-specific or proprietary corpus.
Continual pre-training (CPT) enhances LLM capabilities by imbuing new
domain-specific or proprietary knowledge while replaying general corpus to
prevent catastrophic forgetting. The data mixture ratio of general corpus and
domain-specific corpus, however, has been chosen heuristically, leading to
sub-optimal training efficiency in practice. In this context, we attempt to
re-visit the scaling behavior of LLMs under the hood of CPT, and discover a
power-law relationship between loss, mixture ratio, and training tokens scale.
We formalize the trade-off between general and domain-specific capabilities,
leading to a well-defined Critical Mixture Ratio (CMR) of general and domain
data. By striking the balance, CMR maintains the model's general ability and
achieves the desired domain transfer, ensuring the highest utilization of
available resources. Considering the balance between efficiency and
effectiveness, CMR can be regarded as the optimal mixture ratio. Through
extensive experiments, we ascertain the predictability of CMR, propose CMR
scaling law and have substantiated its generalization. These findings offer
practical guidelines for optimizing LLM training in specialized domains,
ensuring both general and domain-specific performance while efficiently
managing training resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAPE: Data-Adaptive Positional Encoding for Length Extrapolation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14722v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14722v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanyang Zheng, Yihang Gao, Han Shi, Minbin Huang, Jingyao Li, Jing Xiong, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Positional encoding plays a crucial role in transformers, significantly
impacting model performance and length generalization. Prior research has
introduced absolute positional encoding (APE) and relative positional encoding
(RPE) to distinguish token positions in given sequences. However, both APE and
RPE remain fixed after model training regardless of input data, limiting their
adaptability and flexibility. Hence, we expect that the desired positional
encoding should be data-adaptive and can be dynamically adjusted with the given
attention. In this paper, we propose a Data-Adaptive Positional Encoding (DAPE)
method, which dynamically and semantically adjusts based on input context and
learned fixed priors. Experimental validation on real-world datasets (Arxiv,
Books3, and CHE) demonstrates that DAPE enhances model performances in terms of
trained length and length generalization, where the improvements are
statistically significant. The model visualization suggests that our model can
keep both local and anti-local information. Finally, we successfully train the
model on sequence length 128 and achieve better performance at evaluation
sequence length 8192, compared with other static positional encoding methods,
revealing the benefit of the adaptive positional encoding method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Progressive-Hint <span class="highlight-title">Prompt</span>ing Improves Reasoning in Large Language Models <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.09797v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.09797v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of Large Language Models (LLMs) in reasoning tasks depends
heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency
being critical methods that enhance this ability. However, these methods do not
fully exploit the answers generated by the LLM to guide subsequent responses.
This paper proposes a new prompting method, named Progressive-Hint Prompting
(PHP), that enables automatic multiple interactions between users and LLMs by
using previously generated answers as hints to progressively guide toward the
correct answers. PHP is orthogonal to CoT and self-consistency, making it easy
to combine with state-of-the-art techniques to further improve performance. We
conducted extensive and comprehensive experiments on seven benchmarks. The
results show that PHP significantly improves accuracy while remaining highly
efficient. For instance, with text-davinci-003, we observed a 4.2% improvement
on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction
in sample paths with self-consistency. With GPT-4 and PHP, we achieve
state-of-the-art performances on SVAMP (89.1% -> 91.9%), GSM8K (92% -> 95.5%),
AQuA (76.4% -> 79.9%) and MATH (50.3% -> 53.9%).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML AI4MATH 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ sDPO: Don't Use Your Data All at Once 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dahyun Kim, Yungi Kim, Wonho Song, Hyeonwoo Kim, Yunsu Kim, Sanghoon Kim, Chanjun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As development of large language models (LLM) progresses, aligning them with
human preferences has become increasingly important. We propose stepwise DPO
(sDPO), an extension of the recently popularized direct preference optimization
(DPO) for alignment tuning. This approach involves dividing the available
preference datasets and utilizing them in a stepwise manner, rather than
employing it all at once. We demonstrate that this method facilitates the use
of more precisely aligned reference models within the DPO training framework.
Furthermore, sDPO trains the final model to be more performant, even
outperforming other popular LLMs with more parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "I Like Sunnie More Than I Expected!": Exploring User Expectation and
  Perception of an Anthropomorphic LLM-based Conversational Agent for
  Well-Being Support 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13803v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13803v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyi Wu, Julie Y. A. Cachia, Feixue Han, Bingsheng Yao, Tianyi Xie, Xuan Zhao, Dakuo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The human-computer interaction (HCI) research community has a longstanding
interest in exploring the mismatch between users' actual experiences and
expectation toward new technologies, for instance, large language models
(LLMs). In this study, we compared users' (N = 38) initial expectations against
their post-interaction perceptions of two LLM-powered mental well-being
intervention activity recommendation systems. Both systems have a built-in LLM
to recommend a personalized well-being intervention activity, but one system
(Sunnie) has an anthropomorphic conversational interaction design via elements
such as appearance, persona, and natural conversation. Results showed that user
engagement was high with both systems, and both systems exceeded users'
expectations along the utility dimension, highlighting AI's potential to offer
useful intervention activity recommendations. In addition, Sunnie further
outperformed the non-anthropomorphic baseline system in relational warmth.
These findings suggest that anthropomorphic conversational interaction design
may be particularly effective in fostering warmth in mental health support
contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and
  Iterative Sub-SQL Refinement for Text-to-SQL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07930v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07930v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Xie, Gaochen Wu, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent In-Context Learning based methods have achieved remarkable success in
Text-to-SQL task. However, there is still a large gap between the performance
of these models and human performance on datasets with complex database schema
and difficult questions, such as BIRD. Besides, existing work has neglected to
supervise intermediate steps when solving questions iteratively with question
decomposition methods, and the schema linking methods used in these works are
very rudimentary. To address these issues, we propose MAG-SQL, a multi-agent
generative approach with soft schema linking and iterative Sub-SQL refinement.
In our framework, an entity-based method with tables' summary is used to select
the columns in database, and a novel targets-conditions decomposition method is
introduced to decompose those complex questions. Additionally, we build a
iterative generating module which includes a Sub-SQL Generator and Sub-SQL
Refiner, introducing external oversight for each step of generation. Through a
series of ablation studies, the effectiveness of each agent in our framework
has been demonstrated. When evaluated on the BIRD benchmark with GPT-4, MAG-SQL
achieves an execution accuracy of 61.08%, compared to the baseline accuracy of
46.35% for vanilla GPT-4 and the baseline accuracy of 57.56% for MAC-SQL.
Besides, our approach makes similar progress on Spider.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06927v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06927v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher M. Ackerman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Activation engineering is becoming increasingly popular as a means of online
control of large language models (LLMs). In this work, I extend the idea of
active steering with vectors that represent a behavioral direction of interest
to tuning those vectors directly into the model, obviating the need for online
control. First, I identify activation vectors related to honesty in an
open-source LLM (Llama- 2-13b-chat). Next, I demonstrate that model output can
be made more or less honest by adding positive or negative multiples of these
vectors to residual stream activations during generation. Then, I show that a
similar effect can be achieved by fine-tuning the vectors directly into the
model, by use of a dual loss function based on the cosine similarity of
residual stream activations to the vectors combined with a standard token-based
loss ("representation tuning"). Finally, I compare the generations in response
to honesty-probing prompts from the resulting models to those from models
fine-tuned with a token-based loss alone, and to those from the untuned model
subjected to online steering. Overall, fine-tuning the vectors into the models
using the cosine similarity plus token loss showed a stronger effect than
online steering, and generalized better than using the standard loss,
suggesting the potential utility of this approach as a safety measure. Code and
data are available at https://github.com/cma1114/representation_tuning; tuned
models are available at https://huggingface.co/collections/cackerman/
representation-tuning-66da1e5ab41cd1b824687d9f.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability
  of Large Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17169v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17169v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nisarg Patel, Mohith Kulkarni, Mihir Parmar, Aashna Budhiraja, Mutsumi Nakamura, Neeraj Varshney, Chitta Baral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) continue to exhibit remarkable performance in
natural language understanding tasks, there is a crucial need to measure their
ability for human-like multi-step logical reasoning. Existing logical reasoning
evaluation benchmarks often focus primarily on simplistic single-step or
multi-step reasoning with a limited set of inference rules. Furthermore, the
lack of datasets for evaluating non-monotonic reasoning represents a crucial
gap since it aligns more closely with human-like reasoning. To address these
limitations, we propose Multi-LogiEval, a comprehensive evaluation dataset
encompassing multi-step logical reasoning with various inference rules and
depths. Multi-LogiEval covers three logic types--propositional, first-order,
and non-monotonic--consisting of more than 30 inference rules and more than 60
of their combinations with various depths. Leveraging this dataset, we conduct
evaluations on a range of LLMs including GPT-4, ChatGPT, Gemini-Pro, Yi, Orca,
and Mistral, employing a zero-shot chain-of-thought. Experimental results show
that there is a significant drop in the performance of LLMs as the reasoning
steps/depth increases (average accuracy of ~68% at depth-1 to ~43% at depth-5).
We further conduct a thorough investigation of reasoning chains generated by
LLMs which reveals several important findings. We believe that Multi-LogiEval
facilitates future research for evaluating and enhancing the logical reasoning
ability of LLMs. Data is available at
https://github.com/Mihir3009/Multi-LogiEval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Who's Harry Potter: Towards Targeted Unlearning from a Causal
  Intervention Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujian Liu, Yang Zhang, Tommi Jaakkola, Shiyu Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates Who's Harry Potter (WHP), a pioneering yet
insufficiently understood method for LLM unlearning. We explore it in two
steps. First, we introduce a new task of LLM targeted unlearning, where given
an unlearning target (e.g., a person) and some unlearning documents, we aim to
unlearn only the information about the target, rather than everything in the
unlearning documents. We further argue that a successful unlearning should
satisfy criteria such as not outputting gibberish, not fabricating facts about
the unlearning target, and not releasing factual information under jailbreak
attacks. Second, we construct a causal intervention framework for targeted
unlearning, where the knowledge of the unlearning target is modeled as a
confounder between LLM input and output, and the unlearning process as a
deconfounding process. This framework justifies and extends WHP, deriving a
simple unlearning algorithm that includes WHP as a special case. Experiments on
existing and new datasets show that our approach, without explicitly optimizing
for the aforementioned criteria, achieves competitive performance in all of
them. Our code is available at
https://github.com/UCSB-NLP-Chang/causal_unlearn.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PILLOW: Enhancing Efficient Instruction Fine-tuning via <span class="highlight-title">Prompt</span> Matching <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05621v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05621v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenting Qi, Xiaoyu Tan, Shaojie Shi, Chao Qu, Yinghui Xu, Yuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction fine-tuning has conventionally been employed to adapt Large
Language Models (LLMs) to a variety of tasks. Nonetheless, this technique often
necessitates substantial computational resources, making it impractical for
deployment by individuals or small-scale entities. Recently, Low-Rank
Adaptation (LoRA) has become a promising alternative, offering high
capabilities on par with full tuning with reduced resource overhead. However,
attaining satisfactory performance through the fine-tuning of LoRA is a
non-trivial challenge. In this paper, we propose PILLOW, which aims to improve
LoRA's performance by a discrimination-based prompting method, leveraging LLMs'
In-Context Learning ability. PILLOW incorporates a matching network that
selects prompts from a user-defined prompt pool, concatenates the selected
prompts with the user instruction as input, and performs inference using the
LoRA-fine-tuned LLMs. Trained with Reinforcement Learning, PILLOW exhibits
commensurate performance on various evaluation metrics compared with typical
instruction fine-tuning methods, utilizing only consumer-grade GPU resources
and exhibiting a large reduction in computational costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2023 (Industry Track), Oral Presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaAligner: Towards Generalizable Multi-Objective Alignment of Language
  Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17141v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17141v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kailai Yang, Zhiwei Liu, Qianqian Xie, Jimin Huang, Tianlin Zhang, Sophia Ananiadou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) focus on aligning to
heterogeneous human expectations and values via multi-objective preference
alignment. However, existing methods are dependent on the policy model
parameters, which require high-cost repetition of their alignment algorithms
for each new policy model, and they cannot expand to unseen objectives due to
their static alignment objectives. In this work, we propose Meta-Objective
Aligner (MetaAligner), the first policy-agnostic and generalizable method for
multi-objective preference alignment. MetaAligner models multi-objective
alignment into three stages: (1) dynamic objectives reformulation algorithm
reorganizes traditional alignment datasets to supervise the model on performing
flexible alignment across different objectives; (2) conditional weak-to-strong
correction paradigm aligns the weak outputs of fixed policy models to approach
strong outputs with higher preferences in the corresponding alignment
objectives, enabling plug-and-play inferences on any policy models, which
significantly reduces training costs and facilitates alignment on close-source
policy models; (3) generalizable inference method flexibly adjusts target
objectives by updating their text descriptions in the prompts, facilitating
generalizable alignment to unseen objectives. Experimental results show that
MetaAligner achieves significant and balanced improvements in multi-objective
alignments on 10 state-of-the-art policy models, and saves up to 93.63% of GPU
training hours compared to previous alignment methods. The model also
effectively aligns unseen objectives, marking the first step towards
generalizable multi-objective preference alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 main track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12327v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12327v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Kaushal, Tejas Vaidhya, Arnab Kumar Mondal, Tejas Pandey, Aaryan Bhagat, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid advancements in GPU computational power has outpaced memory capacity
and bandwidth growth, creating bottlenecks in Large Language Model (LLM)
inference. Post-training quantization is the leading method for addressing
memory-related bottlenecks in LLM inference, but it suffers from significant
performance degradation below 4-bit precision. This paper addresses these
challenges by investigating the pretraining of low-bitwidth models specifically
Ternary Language Models (TriLMs) as an alternative to traditional
floating-point models (FloatLMs) and their post-training quantized versions
(QuantLMs). We present Spectra LLM suite, the first open suite of LLMs spanning
multiple bit-widths, including FloatLMs, QuantLMs, and TriLMs, ranging from 99M
to 3.9B parameters trained on 300B tokens. Our comprehensive evaluation
demonstrates that TriLMs offer superior scaling behavior in terms of model size
(in bits). Surprisingly, at scales exceeding one billion parameters, TriLMs
consistently outperform their QuantLM and FloatLM counterparts for a given bit
size across various benchmarks. Notably, the 3.9B parameter TriLM matches the
performance of the FloatLM 3.9B across all benchmarks, despite having fewer
bits than FloatLM 830M. Overall, this research provides valuable insights into
the feasibility and scalability of low-bitwidth language models, paving the way
for the development of more efficient LLMs.
  To enhance understanding of low-bitwidth models, we are releasing 500+
intermediate checkpoints of the Spectra suite at
\href{https://github.com/NolanoOrg/SpectraSuite}{https://github.com/NolanoOrg/SpectraSuite}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 21 figures, and 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating and Safeguarding the Adversarial Robustness of
  Retrieval-Based In-Context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15984v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15984v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Yu, Jie He, Pasquale Minervini, Jeff Z. Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of large language models, such as LLaMA and OpenAI GPT-3,
In-Context Learning (ICL) gained significant attention due to its effectiveness
and efficiency. However, ICL is very sensitive to the choice, order, and
verbaliser used to encode the demonstrations in the prompt. Retrieval-Augmented
ICL methods try to address this problem by leveraging retrievers to extract
semantically related examples as demonstrations. While this approach yields
more accurate results, its robustness against various types of adversarial
attacks, including perturbations on test samples, demonstrations, and retrieved
data, remains under-explored. Our study reveals that retrieval-augmented models
can enhance robustness against test sample attacks, outperforming vanilla ICL
with a 4.87% reduction in Attack Success Rate (ASR); however, they exhibit
overconfidence in the demonstrations, leading to a 2% increase in ASR for
demonstration attacks. Adversarial training can help improve the robustness of
ICL methods to adversarial attacks; however, such a training scheme can be too
costly in the context of LLMs. As an alternative, we introduce an effective
training-free adversarial defence method, DARD, which enriches the example pool
with those attacked samples. We show that DARD yields improvements in
performance and robustness, achieving a 15% reduction in ASR over the
baselines. Code and data are released to encourage further research:
https://github.com/simonucl/adv-retreival-icl
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLM 2024, 30 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FacLens: Transferable Probe for Foreseeing Non-Factuality in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanling Wang, Haoyang Li, Hao Zou, Jing Zhang, Xinlei He, Qi Li, Ke Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advancements in large language models (LLMs), non-factual responses
remain prevalent. Unlike extensive studies on post-hoc detection of such
responses, this work studies non-factuality prediction (NFP), aiming to predict
whether an LLM will generate a non-factual response to a question before the
generation process. Previous efforts on NFP have demonstrated LLMs' awareness
of their internal knowledge, but they still face challenges in efficiency and
transferability. In this work, we propose a lightweight NFP model named
Factuality Lens (FacLens), which effectively probes hidden representations of
questions for the NFP task. Besides, we discover that hidden question
representations sourced from different LLMs exhibit similar NFP patterns, which
enables the transferability of FacLens across LLMs to reduce development costs.
Extensive experiments highlight FacLens's superiority in both effectiveness and
efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frame-Voyager: Learning to Query Frames for Video Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03226v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03226v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicheng Yu, Chengkai Jin, Huanyu Wang, Zhenghao Chen, Sheng Jin, Zhongrong Zuo, Xiaolei Xu, Zhenbang Sun, Bingni Zhang, Jiawei Wu, Hao Zhang, Qianru Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Large Language Models (Video-LLMs) have made remarkable progress in
video understanding tasks. However, they are constrained by the maximum length
of input tokens, making it impractical to input entire videos. Existing frame
selection approaches, such as uniform frame sampling and text-frame retrieval,
fail to account for the information density variations in the videos or the
complex instructions in the tasks, leading to sub-optimal performance. In this
paper, we propose Frame-Voyager that learns to query informative frame
combinations, based on the given textual queries in the task. To train
Frame-Voyager, we introduce a new data collection and labeling pipeline, by
ranking frame combinations using a pre-trained Video-LLM. Given a video of M
frames, we traverse its T-frame combinations, feed them into a Video-LLM, and
rank them based on Video-LLM's prediction losses. Using this ranking as
supervision, we train Frame-Voyager to query the frame combinations with lower
losses. In experiments, we evaluate Frame-Voyager on four Video Question
Answering benchmarks by plugging it into two different Video-LLMs. The
experimental results demonstrate that Frame-Voyager achieves impressive results
in all settings, highlighting its potential as a plug-and-play solution for
Video-LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Auto-Arena: Automating LLM Evaluations with Agent Peer Battles and
  Committee Discussions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20267v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20267v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruochen Zhao, Wenxuan Zhang, Yew Ken Chia, Weiwen Xu, Deli Zhao, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As LLMs continuously evolve, there is an urgent need for a reliable
evaluation method that delivers trustworthy results promptly. Currently, static
benchmarks suffer from inflexibility and unreliability, leading users to prefer
human voting platforms like Chatbot Arena. However, human evaluations require
significant manual effort. To address this, we propose the Auto-Arena, an
innovative framework that automates the entire evaluation process using
LLM-powered agents. Firstly, an LLM examiner generates questions. Then, two LLM
candidates engage in a multi-round peer battle based on individual questions,
aiming at revealing their true performance differences. Finally, a committee of
LLM judges collaboratively discusses and decides the winner, reducing bias and
enhancing fairness. During the peer battles, we observe intriguing scenarios
where the LLM candidates display competitive behaviors and even learn from the
opponents. In our extensive experiments involving 15 recent LLMs, Auto-Arena
shows a 92.14% correlation with human preferences, surpassing all previous
expert-annotated benchmarks without any manual efforts. As a result, Auto-Arena
offers a promising alternative to current human evaluation platforms for
evaluating LLMs automatically.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evalverse: Unified and Accessible Library for Large Language Model
  Evaluation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihoo Kim, Wonho Song, Dahyun Kim, Yunsu Kim, Yungi Kim, Chanjun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Evalverse, a novel library that streamlines the
evaluation of Large Language Models (LLMs) by unifying disparate evaluation
tools into a single, user-friendly framework. Evalverse enables individuals
with limited knowledge of artificial intelligence to easily request LLM
evaluations and receive detailed reports, facilitated by an integration with
communication platforms like Slack. Thus, Evalverse serves as a powerful tool
for the comprehensive assessment of LLMs, offering both researchers and
practitioners a centralized and easily accessible evaluation framework.
Finally, we also provide a demo video for Evalverse, showcasing its
capabilities and implementation in a two-minute format.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Demo Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Corrective Retrieval Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15884v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15884v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, Zhen-Hua Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) inevitably exhibit hallucinations since the
accuracy of generated texts cannot be secured solely by the parametric
knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a
practicable complement to LLMs, it relies heavily on the relevance of retrieved
documents, raising concerns about how the model behaves if retrieval goes
wrong. To this end, we propose the Corrective Retrieval Augmented Generation
(CRAG) to improve the robustness of generation. Specifically, a lightweight
retrieval evaluator is designed to assess the overall quality of retrieved
documents for a query, returning a confidence degree based on which different
knowledge retrieval actions can be triggered. Since retrieval from static and
limited corpora can only return sub-optimal documents, large-scale web searches
are utilized as an extension for augmenting the retrieval results. Besides, a
decompose-then-recompose algorithm is designed for retrieved documents to
selectively focus on key information and filter out irrelevant information in
them. CRAG is plug-and-play and can be seamlessly coupled with various
RAG-based approaches. Experiments on four datasets covering short- and
long-form generation tasks show that CRAG can significantly improve the
performance of RAG-based approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update results, add more analysis, and fix typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Magpie: Alignment Data Synthesis from Scratch by <span class="highlight-title">Prompt</span>ing Aligned LLMs
  with Nothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, Bill Yuchen Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality instruction data is critical for aligning large language models
(LLMs). Although some models, such as Llama-3-Instruct, have open weights,
their alignment data remain private, which hinders the democratization of AI.
High human labor costs and a limited, predefined scope for prompting prevent
existing open-source data creation methods from scaling effectively,
potentially limiting the diversity and quality of public alignment datasets. Is
it possible to synthesize high-quality instruction data at scale by extracting
it directly from an aligned LLM? We present a self-synthesis method for
generating large-scale alignment data named Magpie. Our key observation is that
aligned LLMs like Llama-3-Instruct can generate a user query when we input only
the left-side templates up to the position reserved for user messages, thanks
to their auto-regressive nature. We use this method to prompt Llama-3-Instruct
and generate 4 million instructions along with their corresponding responses.
We perform a comprehensive analysis of the extracted data and select 300K
high-quality instances. To compare Magpie data with other public instruction
datasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate the
performance of the fine-tuned models. Our results indicate that in some tasks,
models fine-tuned with Magpie perform comparably to the official
Llama-3-8B-Instruct, despite the latter being enhanced with 10 million data
points through supervised fine-tuning (SFT) and subsequent feedback learning.
We also show that using Magpie solely for SFT can surpass the performance of
previous public datasets utilized for both SFT and preference optimization,
such as direct preference optimization with UltraFeedback. This advantage is
evident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Link: https://magpie-align.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpinQuant: LLM quantization with learned rotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16406v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16406v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, Tijmen Blankevoort
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization (PTQ) techniques applied to weights, activations,
and the KV cache greatly reduce memory usage, latency, and power consumption of
Large Language Models (LLMs), but may lead to large quantization errors when
outliers are present. Rotating activation or weight matrices helps remove
outliers and benefits quantization. In this work, we identify a collection of
applicable rotation parameterizations that lead to identical outputs in
full-precision Transformer architectures while enhancing quantization accuracy.
In addition, we find that some random rotations lead to much better
quantization than others, with an up to 13 points difference in downstream
zero-shot reasoning performance. As a result, we propose SpinQuant, a novel
approach that incorporates learned rotation matrices for optimal quantized
network accuracy. With 4-bit quantization of weight, activation, and KV-cache,
SpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full
precision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by
19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also
outperforms concurrent work QuaRot, which applies random rotations to remove
outliers. In particular, for LLaMA-3 8B models that are hard to quantize,
SpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12832v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12832v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tenghao Huang, Donghee Lee, John Sweeney, Jiatong Shi, Emily Steliotes, Matthew Lange, Jonathan May, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flavor development in the food industry is increasingly challenged by the
need for rapid innovation and precise flavor profile creation. Traditional
flavor research methods typically rely on iterative, subjective testing, which
lacks the efficiency and scalability required for modern demands. This paper
presents three contributions to address the challenges. Firstly, we define a
new problem domain for scientific agents in flavor science, conceptualized as
the generation of hypotheses for flavor profile sourcing and understanding. To
facilitate research in this area, we introduce the FoodPuzzle, a challenging
benchmark consisting of 978 food items and 1,766 flavor molecules profiles. We
propose a novel Scientific Agent approach, integrating in-context learning and
retrieval augmented techniques to generate grounded hypotheses in the domain of
food science. Experimental results indicate that our model significantly
surpasses traditional methods in flavor profile prediction tasks, demonstrating
its potential to transform flavor development practices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Generation Gap: Exploring Age Bias in the Value Systems of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08760v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08760v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyang Liu, Trish Maturi, Bowen Yi, Siqi Shen, Rada Mihalcea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the alignment of values in Large Language Models (LLMs) with
specific age groups, leveraging data from the World Value Survey across
thirteen categories. Through a diverse set of prompts tailored to ensure
response robustness, we find a general inclination of LLM values towards
younger demographics, especially when compared to the US population. Although a
general inclination can be observed, we also found that this inclination toward
younger groups can be different across different value categories.
Additionally, we explore the impact of incorporating age identity information
in prompts and observe challenges in mitigating value discrepancies with
different age cohorts. Our findings highlight the age bias in LLMs and provide
insights for future work. Materials for our analysis are available at \url{
https://github.com/MichiganNLP/Age-Bias-In-LLMs}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Advisor: Dynamic Data Curation for Safety Alignment of Large
  Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Wang, Ninareh Mehrabi, Palash Goyal, Rahul Gupta, Kai-Wei Chang, Aram Galstyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data is a crucial element in large language model (LLM) alignment. Recent
studies have explored using LLMs for efficient data collection. However,
LLM-generated data often suffers from quality issues, with underrepresented or
absent aspects and low-quality datapoints. To address these problems, we
propose Data Advisor, an enhanced LLM-based method for generating data that
takes into account the characteristics of the desired dataset. Starting from a
set of pre-defined principles in hand, Data Advisor monitors the status of the
generated data, identifies weaknesses in the current dataset, and advises the
next iteration of data generation accordingly. Data Advisor can be easily
integrated into existing data generation methods to enhance data quality and
coverage. Experiments on safety alignment of three representative LLMs (i.e.,
Mistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in
enhancing model safety against various fine-grained safety issues without
sacrificing model utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference. Project website:
  https://feiwang96.github.io/DataAdvisor/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Regression Conformal Prediction under Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matt Y. Cheung, Tucker J. Netherton, Laurence E. Court, Ashok Veeraraghavan, Guha Balakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty quantification is crucial to account for the imperfect
predictions of machine learning algorithms for high-impact applications.
Conformal prediction (CP) is a powerful framework for uncertainty
quantification that generates calibrated prediction intervals with valid
coverage. In this work, we study how CP intervals are affected by bias - the
systematic deviation of a prediction from ground truth values - a phenomenon
prevalent in many real-world applications. We investigate the influence of bias
on interval lengths of two different types of adjustments -- symmetric
adjustments, the conventional method where both sides of the interval are
adjusted equally, and asymmetric adjustments, a more flexible method where the
interval can be adjusted unequally in positive or negative directions. We
present theoretical and empirical analyses characterizing how symmetric and
asymmetric adjustments impact the "tightness" of CP intervals for regression
tasks. Specifically for absolute residual and quantile-based non-conformity
scores, we prove: 1) the upper bound of symmetrically adjusted interval lengths
increases by $2|b|$ where $b$ is a globally applied scalar value representing
bias, 2) asymmetrically adjusted interval lengths are not affected by bias, and
3) conditions when asymmetrically adjusted interval lengths are guaranteed to
be smaller than symmetric ones. Our analyses suggest that even if predictions
exhibit significant drift from ground truth values, asymmetrically adjusted
intervals are still able to maintain the same tightness and validity of
intervals as if the drift had never happened, while symmetric ones
significantly inflate the lengths. We demonstrate our theoretical results with
two real-world prediction tasks: sparse-view computed tomography (CT)
reconstruction and time-series weather forecasting. Our work paves the way for
more bias-robust machine learning systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures, code available at:
  https://github.com/matthewyccheung/conformal-metric</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TextHawk2: A Large Vision-Language Model Excels in Bilingual OCR and
  Grounding with 16x Fewer Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ya-Qi Yu, Minghui Liao, Jiwen Zhang, Jihao Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reading dense text and locating objects within images are fundamental
abilities for Large Vision-Language Models (LVLMs) tasked with advanced jobs.
Previous LVLMs, including superior proprietary models like GPT-4o, have
struggled to excel in both tasks simultaneously. Moreover, previous LVLMs with
fine-grained perception cost thousands of tokens per image, making them
resource-intensive. We present TextHawk2, a bilingual LVLM featuring efficient
fine-grained perception and demonstrating cutting-edge performance across
general-purpose, OCR, and grounding tasks with 16 times fewer image tokens.
Critical improvements include: (1) Token Compression: Building on the efficient
architecture of its predecessor, TextHawk2 significantly reduces the number of
tokens per image by 16 times, facilitating training and deployment of the
TextHawk series with minimal resources. (2) Visual Encoder Reinforcement: We
enhance the visual encoder through LVLM co-training, unlocking its potential
for previously unseen tasks like Chinese OCR and grounding. (3) Data Diversity:
We maintain a comparable scale of 100 million samples while diversifying the
sources of pre-training data. We assess TextHawk2 across multiple benchmarks,
where it consistently delivers superior performance and outperforms
closed-source models of similar scale, such as achieving 78.4% accuracy on
OCRBench, 81.4% accuracy on ChartQA, 89.6% ANLS on DocVQA, and 88.1%
accuracy@0.5 on RefCOCOg-test.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLEE: A Unified Framework and Benchmark for Language-based Economic
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eilam Shapira, Omer Madmon, Itamar Reinman, Samuel Joseph Amouyal, Roi Reichart, Moshe Tennenholtz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) show significant potential in economic and
strategic interactions, where communication via natural language is often
prevalent. This raises key questions: Do LLMs behave rationally? Can they mimic
human behavior? Do they tend to reach an efficient and fair outcome? What is
the role of natural language in the strategic interaction? How do
characteristics of the economic environment influence these dynamics? These
questions become crucial concerning the economic and societal implications of
integrating LLM-based agents into real-world data-driven systems, such as
online retail platforms and recommender systems. While the ML community has
been exploring the potential of LLMs in such multi-agent setups, varying
assumptions, design choices and evaluation criteria across studies make it
difficult to draw robust and meaningful conclusions. To address this, we
introduce a benchmark for standardizing research on two-player, sequential,
language-based games. Inspired by the economic literature, we define three base
families of games with consistent parameterization, degrees of freedom and
economic measures to evaluate agents' performance (self-gain), as well as the
game outcome (efficiency and fairness). We develop an open-source framework for
interaction simulation and analysis, and utilize it to collect a dataset of LLM
vs. LLM interactions across numerous game configurations and an additional
dataset of human vs. LLM interactions. Through extensive experimentation, we
demonstrate how our framework and dataset can be used to: (i) compare the
behavior of LLM-based agents to human players in various economic contexts;
(ii) evaluate agents in both individual and collective performance measures;
and (iii) quantify the effect of the economic characteristics of the
environments on the behavior of agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Micro-Narratives <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mourad Heddaya, Qingcheng Zeng, Chenhao Tan, Rob Voigt, Alexander Zentefis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to classify causal micro-narratives from text.
These narratives are sentence-level explanations of the cause(s) and/or
effect(s) of a target subject. The approach requires only a subject-specific
ontology of causes and effects, and we demonstrate it with an application to
inflation narratives. Using a human-annotated dataset spanning historical and
contemporary US news articles for training, we evaluate several large language
models (LLMs) on this multi-label classification task. The best-performing
model--a fine-tuned Llama 3.1 8B--achieves F1 scores of 0.87 on narrative
detection and 0.71 on narrative classification. Comprehensive error analysis
reveals challenges arising from linguistic ambiguity and highlights how model
errors often mirror human annotator disagreements. This research establishes a
framework for extracting causal micro-narratives from real-world data, with
wide-ranging applications to social science research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Workshop on Narrative Understanding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Xiao, Shujian Zhang, Wenxuan Zhou, Marzyeh Ghassemi, Sanqiang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To induce desired behaviors in large language models (LLMs) for
interaction-driven tasks, the instruction-tuning stage typically trains LLMs on
instruction-response pairs using the next-token prediction (NTP) loss. Previous
work aiming to improve instruction-tuning performance often emphasizes the need
for higher-quality supervised fine-tuning (SFT) datasets, which typically
involves expensive data filtering with proprietary LLMs or labor-intensive data
generation by human annotators. However, these approaches do not fully leverage
the datasets' intrinsic properties, resulting in high computational and labor
costs, thereby limiting scalability and performance gains. In this paper, we
propose SFTMix, a novel recipe that elevates instruction-tuning performance
beyond the conventional NTP paradigm, without the need for well-curated
datasets. Observing that LLMs exhibit uneven confidence across the semantic
representation space, we argue that examples with different confidence levels
should play distinct roles during the instruction-tuning process. Based on this
insight, SFTMix leverages training dynamics to identify examples with varying
confidence levels, then applies a Mixup-based regularization to mitigate
overfitting on confident examples while propagating supervision signals to
improve learning on relatively unconfident ones. This approach enables SFTMix
to significantly outperform NTP across a wide range of instruction-following
and healthcare domain-specific SFT tasks, demonstrating its adaptability to
diverse LLM families and scalability to datasets of any size. Comprehensive
ablation studies further verify the robustness of SFTMix's design choices,
underscoring its versatility in consistently enhancing performance across
different LLMs and datasets in broader natural language processing
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Navigating the Digital World as Humans Do: Universal Visual Grounding
  for GUI Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) are transforming the capabilities of
graphical user interface (GUI) agents, facilitating their transition from
controlled simulations to complex, real-world applications across various
platforms. However, the effectiveness of these agents hinges on the robustness
of their grounding capability. Current GUI agents predominantly utilize
text-based representations such as HTML or accessibility trees, which, despite
their utility, often introduce noise, incompleteness, and increased
computational overhead. In this paper, we advocate a human-like embodiment for
GUI agents that perceive the environment entirely visually and directly take
pixel-level operations on the GUI. The key is visual grounding models that can
accurately map diverse referring expressions of GUI elements to their
coordinates on the GUI across different platforms. We show that a simple
recipe, which includes web-based synthetic data and slight adaptation of the
LLaVA architecture, is surprisingly effective for training such visual
grounding models. We collect the largest dataset for GUI visual grounding so
far, containing 10M GUI elements and their referring expressions over 1.3M
screenshots, and use it to train UGround, a strong universal visual grounding
model for GUI agents. Empirical results on six benchmarks spanning three
categories (grounding, offline agent, and online agent) show that 1) UGround
substantially outperforms existing visual grounding models for GUI agents, by
up to 20% absolute, and 2) agents with UGround outperform state-of-the-art
agents, despite the fact that existing agents use additional text-based input
while ours only uses visual perception. These results provide strong support
for the feasibility and promises of GUI agents that navigate the digital world
as humans do.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CasiMedicos-Arg: A Medical Question Answering <span class="highlight-title">Dataset</span> Annotated with
  Explanatory Argumentative Structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        katerina Sviridova, Anar Yeginbergen, Ainara Estarrona, Elena Cabrio, Serena Villata, Rodrigo Agerri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explaining Artificial Intelligence (AI) decisions is a major challenge
nowadays in AI, in particular when applied to sensitive scenarios like medicine
and law. However, the need to explain the rationale behind decisions is a main
issue also for human-based deliberation as it is important to justify
\textit{why} a certain decision has been taken. Resident medical doctors for
instance are required not only to provide a (possibly correct) diagnosis, but
also to explain how they reached a certain conclusion. Developing new tools to
aid residents to train their explanation skills is therefore a central
objective of AI in education. In this paper, we follow this direction, and we
present, to the best of our knowledge, the first multilingual dataset for
Medical Question Answering where correct and incorrect diagnoses for a clinical
case are enriched with a natural language explanation written by doctors. These
explanations have been manually annotated with argument components (i.e.,
premise, claim) and argument relations (i.e., attack, support), resulting in
the Multilingual CasiMedicos-Arg dataset which consists of 558 clinical cases
in four languages (English, Spanish, French, Italian) with explanations, where
we annotated 5021 claims, 2313 premises, 2431 support relations, and 1106
attack relations. We conclude by showing how competitive baselines perform over
this challenging dataset for the argument mining task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimO Loss: Anchor-Free Contrastive Loss for Fine-Grained Supervised
  Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha Bouhsine, Imad El Aaroussi, Atik Faysal, Wang Huaxia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel anchor-free contrastive learning (AFCL) method
leveraging our proposed Similarity-Orthogonality (SimO) loss. Our approach
minimizes a semi-metric discriminative loss function that simultaneously
optimizes two key objectives: reducing the distance and orthogonality between
embeddings of similar inputs while maximizing these metrics for dissimilar
inputs, facilitating more fine-grained contrastive learning. The AFCL method,
powered by SimO loss, creates a fiber bundle topological structure in the
embedding space, forming class-specific, internally cohesive yet orthogonal
neighborhoods. We validate the efficacy of our method on the CIFAR-10 dataset,
providing visualizations that demonstrate the impact of SimO loss on the
embedding space. Our results illustrate the formation of distinct, orthogonal
class neighborhoods, showcasing the method's ability to create well-structured
embeddings that balance class separation with intra-class variability. This
work opens new avenues for understanding and leveraging the geometric
properties of learned representations in various machine learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have sparked interest in
their formal reasoning capabilities, particularly in mathematics. The GSM8K
benchmark is widely used to assess the mathematical reasoning of models on
grade-school-level questions. While the performance of LLMs on GSM8K has
significantly improved in recent years, it remains unclear whether their
mathematical reasoning capabilities have genuinely advanced, raising questions
about the reliability of the reported metrics. To address these concerns, we
conduct a large-scale study on several SOTA open and closed models. To overcome
the limitations of existing evaluations, we introduce GSM-Symbolic, an improved
benchmark created from symbolic templates that allow for the generation of a
diverse set of questions. GSM-Symbolic enables more controllable evaluations,
providing key insights and more reliable metrics for measuring the reasoning
capabilities of models.Our findings reveal that LLMs exhibit noticeable
variance when responding to different instantiations of the same question.
Specifically, the performance of all models declines when only the numerical
values in the question are altered in the GSM-Symbolic benchmark. Furthermore,
we investigate the fragility of mathematical reasoning in these models and show
that their performance significantly deteriorates as the number of clauses in a
question increases. We hypothesize that this decline is because current LLMs
cannot perform genuine logical reasoning; they replicate reasoning steps from
their training data. Adding a single clause that seems relevant to the question
causes significant performance drops (up to 65%) across all state-of-the-art
models, even though the clause doesn't contribute to the reasoning chain needed
for the final answer. Overall, our work offers a more nuanced understanding of
LLMs' capabilities and limitations in mathematical reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preserving Multi-Modal Capabilities of <span class="highlight-title">Pre-train</span>ed VLMs for Improving
  Vision-Linguistic Compositionality <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngtaek Oh, Jae Won Cho, Dong-Jin Kim, In So Kweon, Junmo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new method to enhance compositional understanding
in pre-trained vision and language models (VLMs) without sacrificing
performance in zero-shot multi-modal tasks. Traditional fine-tuning approaches
often improve compositional reasoning at the cost of degrading multi-modal
capabilities, primarily due to the use of global hard negative (HN) loss, which
contrasts global representations of images and texts. This global HN loss
pushes HN texts that are highly similar to the original ones, damaging the
model's multi-modal representations. To overcome this limitation, we propose
Fine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard
negative loss and selective calibrated regularization. These innovations
provide fine-grained negative supervision while preserving the model's
representational integrity. Our extensive evaluations across diverse benchmarks
for both compositionality and multi-modal tasks show that FSC-CLIP not only
achieves compositionality on par with state-of-the-art models but also retains
strong multi-modal capabilities. Code is available at:
https://github.com/ytaek-oh/fsc-clip.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Long, Main). Project page:
  https://ytaek-oh.github.io/fsc-clip</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond FVD: Enhanced Evaluation Metrics for Video Generation Quality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Ya,  Luo, Gian Favero, Zhi Hao Luo, Alexia Jolicoeur-Martineau, Christopher Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Fr\'echet Video Distance (FVD) is a widely adopted metric for evaluating
video generation distribution quality. However, its effectiveness relies on
critical assumptions. Our analysis reveals three significant limitations: (1)
the non-Gaussianity of the Inflated 3D Convnet (I3D) feature space; (2) the
insensitivity of I3D features to temporal distortions; (3) the impractical
sample sizes required for reliable estimation. These findings undermine FVD's
reliability and show that FVD falls short as a standalone metric for video
generation evaluation. After extensive analysis of a wide range of metrics and
backbone architectures, we propose JEDi, the JEPA Embedding Distance, based on
features derived from a Joint Embedding Predictive Architecture, measured using
Maximum Mean Discrepancy with polynomial kernel. Our experiments on multiple
open-source datasets show clear evidence that it is a superior alternative to
the widely used FVD metric, requiring only 16% of the samples to reach its
steady value, while increasing alignment with human evaluation by 34%, on
average.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LADEV: A Language-Driven Testing and Evaluation Platform for
  Vision-Language-Action Models in Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijie Wang, Zhehua Zhou, Jiayang Song, Yuheng Huang, Zhan Shu, Lei Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building on the advancements of Large Language Models (LLMs) and Vision
Language Models (VLMs), recent research has introduced Vision-Language-Action
(VLA) models as an integrated solution for robotic manipulation tasks. These
models take camera images and natural language task instructions as input and
directly generate control actions for robots to perform specified tasks,
greatly improving both decision-making capabilities and interaction with human
users. However, the data-driven nature of VLA models, combined with their lack
of interpretability, makes the assurance of their effectiveness and robustness
a challenging task. This highlights the need for a reliable testing and
evaluation platform. For this purpose, in this work, we propose LADEV, a
comprehensive and efficient platform specifically designed for evaluating VLA
models. We first present a language-driven approach that automatically
generates simulation environments from natural language inputs, mitigating the
need for manual adjustments and significantly improving testing efficiency.
Then, to further assess the influence of language input on the VLA models, we
implement a paraphrase mechanism that produces diverse natural language task
instructions for testing. Finally, to expedite the evaluation process, we
introduce a batch-style method for conducting large-scale testing of VLA
models. Using LADEV, we conducted experiments on several state-of-the-art VLA
models, demonstrating its effectiveness as a tool for evaluating these models.
Our results showed that LADEV not only enhances testing efficiency but also
establishes a solid baseline for evaluating VLA models, paving the way for the
development of more intelligent and advanced robotic systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Correlation: Interpretable Evaluation of Machine Translation
  Metrics <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Perrella, Lorenzo Proietti, Pere-Lluís Huguet Cabot, Edoardo Barba, Roberto Navigli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Translation (MT) evaluation metrics assess translation quality
automatically. Recently, researchers have employed MT metrics for various new
use cases, such as data filtering and translation re-ranking. However, most MT
metrics return assessments as scalar scores that are difficult to interpret,
posing a challenge to making informed design choices. Moreover, MT metrics'
capabilities have historically been evaluated using correlation with human
judgment, which, despite its efficacy, falls short of providing intuitive
insights into metric performance, especially in terms of new metric use cases.
To address these issues, we introduce an interpretable evaluation framework for
MT metrics. Within this framework, we evaluate metrics in two scenarios that
serve as proxies for the data filtering and translation re-ranking use cases.
Furthermore, by measuring the performance of MT metrics using Precision,
Recall, and F-score, we offer clearer insights into their capabilities than
correlation with human judgments. Finally, we raise concerns regarding the
reliability of manually curated data following the Direct Assessments+Scalar
Quality Metrics (DA+SQM) guidelines, reporting a notably low agreement with
Multidimensional Quality Metrics (MQM) annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Main Conference. 26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MARs: Multi-view Attention Regularizations for Patch-based Feature
  Recognition of Space Terrain <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothy Chase Jr, Karthik Dantu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The visual detection and tracking of surface terrain is required for
spacecraft to safely land on or navigate within close proximity to celestial
objects. Current approaches rely on template matching with pre-gathered
patch-based features, which are expensive to obtain and a limiting factor in
perceptual capability. While recent literature has focused on in-situ detection
methods to enhance navigation and operational autonomy, robust description is
still needed. In this work, we explore metric learning as the lightweight
feature description mechanism and find that current solutions fail to address
inter-class similarity and multi-view observational geometry. We attribute this
to the view-unaware attention mechanism and introduce Multi-view Attention
Regularizations (MARs) to constrain the channel and spatial attention across
multiple feature views, regularizing the what and where of attention focus. We
thoroughly analyze many modern metric learning losses with and without MARs and
demonstrate improved terrain-feature recognition performance by upwards of 85%.
We additionally introduce the Luna-1 dataset, consisting of Moon crater
landmarks and reference navigation frames from NASA mission data to support
future research in this difficult task. Luna-1 and source code are publicly
available at https://droneslab.github.io/mars/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024. Project page available at
  https://droneslab.github.io/mars/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Presto! Distilling Steps and Layers for Accelerating Music Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary Novack, Ge Zhu, Jonah Casebeer, Julian McAuley, Taylor Berg-Kirkpatrick, Nicholas J. Bryan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advances in diffusion-based text-to-music (TTM) methods, efficient,
high-quality generation remains a challenge. We introduce Presto!, an approach
to inference acceleration for score-based diffusion transformers via reducing
both sampling steps and cost per step. To reduce steps, we develop a new
score-based distribution matching distillation (DMD) method for the EDM-family
of diffusion models, the first GAN-based distillation method for TTM. To reduce
the cost per step, we develop a simple, but powerful improvement to a recent
layer distillation method that improves learning via better preserving hidden
state variance. Finally, we combine our step and layer distillation methods
together for a dual-faceted approach. We evaluate our step and layer
distillation methods independently and show each yield best-in-class
performance. Our combined distillation method can generate high-quality outputs
with improved diversity, accelerating our base model by 10-18x (230/435ms
latency for 32 second mono/stereo 44.1kHz, 15x faster than comparable SOTA) --
the fastest high-quality TTM to our knowledge. Sound examples can be found at
https://presto-music.github.io/web/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLM2Vec: Training Vision-Language Models for Massive Multimodal
  Embedding Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding models have been crucial in enabling various downstream tasks such
as semantic similarity, information retrieval, and clustering. Recently, there
has been a surge of interest in developing universal text embedding models that
can generalize across tasks (e.g., MTEB). However, progress in learning
universal multimodal embedding models has been relatively slow despite their
importance. In this work, we aim to explore the potential for building
universal embeddings capable of handling a wide range of downstream tasks. Our
contributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark),
which covers 4 meta-tasks (i.e. classification, visual question answering,
multimodal retrieval, and visual grounding) and 36 datasets, including 20
training and 16 evaluation datasets, and (2) VLM2Vec (Vision-Language Model ->
Vector), a contrastive training framework that converts any state-of-the-art
vision-language model into an embedding model via training on MMEB. Unlike
previous models such as CLIP and BLIP, VLM2Vec can process any combination of
images and text to generate a fixed-dimensional vector based on task
instructions. We build a series of VLM2Vec models on Phi-3.5-V and evaluate
them on MMEB's evaluation split. Our results show that \model achieves an
absolute average improvement of 10% to 20% over existing multimodal embedding
models on both in-distribution and out-of-distribution datasets in MMEB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CTC-GMM: CTC guided modality matching for fast and accurate streaming
  speech translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Zhao, Jinyu Li, Ruchao Fan, Matt Post
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models for streaming speech translation (ST) can achieve high accuracy and
low latency if they're developed with vast amounts of paired audio in the
source language and written text in the target language. Yet, these text labels
for the target language are often pseudo labels due to the prohibitive cost of
manual ST data labeling. In this paper, we introduce a methodology named
Connectionist Temporal Classification guided modality matching (CTC-GMM) that
enhances the streaming ST model by leveraging extensive machine translation
(MT) text data. This technique employs CTC to compress the speech sequence into
a compact embedding sequence that matches the corresponding text sequence,
allowing us to utilize matched {source-target} language text pairs from the MT
corpora to refine the streaming ST model further. Our evaluations with FLEURS
and CoVoST2 show that the CTC-GMM approach can increase translation accuracy
relatively by 13.9% and 6.4% respectively, while also boosting decoding speed
by 59.7% on GPU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Spoken Language Technology Workshop (SLT 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable and Accurate Graph Reasoning with LLM-based Multi-Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Hu, Runlin Lei, Xinyi Huang, Zhewei Wei, Yongchao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has explored the use of Large Language Models (LLMs) for
tackling complex graph reasoning tasks. However, due to the intricacies of
graph structures and the inherent limitations of LLMs in handling long text,
current approaches often fail to deliver satisfactory accuracy, even on
small-scale graphs and simple tasks. To address these challenges, we introduce
GraphAgent-Reasoner, a fine-tuning-free framework that utilizes a multi-agent
collaboration strategy for explicit and precise graph reasoning. Inspired by
distributed graph computation theory, our framework decomposes graph problems
into smaller, node-centric tasks that are distributed among multiple agents.
The agents collaborate to solve the overall problem, significantly reducing the
amount of information and complexity handled by a single LLM, thus enhancing
the accuracy of graph reasoning. By simply increasing the number of agents,
GraphAgent-Reasoner can efficiently scale to accommodate larger graphs with
over 1,000 nodes. Evaluated on the GraphInstruct dataset, our framework
demonstrates near-perfect accuracy on polynomial-time graph reasoning tasks,
significantly outperforming the best available models, both closed-source and
fine-tuned open-source variants. Our framework also demonstrates the capability
to handle real-world graph reasoning applications such as webpage importance
analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Last Iterate Convergence in Monotone Mean Field Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noboru Isobe, Kenshi Abe, Kaito Ariu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mean Field Game (MFG) is a framework utilized to model and approximate the
behavior of a large number of agents, and the computation of equilibria in MFG
has been a subject of interest. Despite the proposal of methods to approximate
the equilibria, algorithms where the sequence of updated policy converges to
equilibrium, specifically those exhibiting last-iterate convergence, have been
limited. We propose the use of a simple, proximal-point-type algorithm to
compute equilibria for MFGs. Subsequently, we provide the first last-iterate
convergence guarantee under the Lasry--Lions-type monotonicity condition. We
further employ the Mirror Descent algorithm for the regularized MFG to
efficiently approximate the update rules of the proximal point method for MFGs.
We demonstrate that the algorithm can approximate with an accuracy of
$\varepsilon$ after $\mathcal{O}({\log(1/\varepsilon)})$ iterations. This
research offers a tractable approach for large-scale and large-population
games.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review, 25 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-Feedback Efficient Reinforcement Learning for Online Diffusion
  Model Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayano Hiranaka, Shang-Fu Chen, Chieh-Hsin Lai, Dongjun Kim, Naoki Murata, Takashi Shibuya, Wei-Hsiang Liao, Shao-Hua Sun, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controllable generation through Stable Diffusion (SD) fine-tuning aims to
improve fidelity, safety, and alignment with human guidance. Existing
reinforcement learning from human feedback methods usually rely on predefined
heuristic reward functions or pretrained reward models built on large-scale
datasets, limiting their applicability to scenarios where collecting such data
is costly or difficult. To effectively and efficiently utilize human feedback,
we develop a framework, HERO, which leverages online human feedback collected
on the fly during model learning. Specifically, HERO features two key
mechanisms: (1) Feedback-Aligned Representation Learning, an online training
method that captures human feedback and provides informative learning signals
for fine-tuning, and (2) Feedback-Guided Image Generation, which involves
generating images from SD's refined initialization samples, enabling faster
convergence towards the evaluator's intent. We demonstrate that HERO is 4x more
efficient in online feedback for body part anomaly correction compared to the
best existing method. Additionally, experiments show that HERO can effectively
handle tasks like reasoning, counting, personalization, and reducing NSFW
content with only 0.5K online feedback.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlphaRouter: Quantum Circuit Routing with Reinforcement Learning and
  Tree Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Tang, Yiheng Duan, Yaroslav Kharkov, Rasool Fakoor, Eric Kessler, Yunong Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum computers have the potential to outperform classical computers in
important tasks such as optimization and number factoring. They are
characterized by limited connectivity, which necessitates the routing of their
computational bits, known as qubits, to specific locations during program
execution to carry out quantum operations. Traditionally, the NP-hard
optimization problem of minimizing the routing overhead has been addressed
through sub-optimal rule-based routing techniques with inherent human biases
embedded within the cost function design. This paper introduces a solution that
integrates Monte Carlo Tree Search (MCTS) with Reinforcement Learning (RL). Our
RL-based router, called AlphaRouter, outperforms the current state-of-the-art
routing methods and generates quantum programs with up to $20\%$ less routing
overhead, thus significantly enhancing the overall efficiency and feasibility
of quantum computing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 11 figures, International Conference on Quantum Computing
  and Engineering - QCE24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form
  Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan Reddy Mekala, Frederik Pahde, Simon Baur, Sneha Chandrashekar, Madeline Diep, Markus Wenzel, Eric L. Wisotzky, Galip Ümit Yolcu, Sebastian Lapuschkin, Jackie Ma, Peter Eisert, Mikael Lindvall, Adam Porter, Wojciech Samek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of dermatological diagnoses, where the analysis of dermatoscopic
and microscopic skin lesion images is pivotal for the accurate and early
detection of various medical conditions, the costs associated with creating
diverse and high-quality annotated datasets have hampered the accuracy and
generalizability of machine learning models. We propose an innovative
unsupervised augmentation solution that harnesses Generative Adversarial
Network (GAN) based models and associated techniques over their latent space to
generate controlled semiautomatically-discovered semantic variations in
dermatoscopic images. We created synthetic images to incorporate the semantic
variations and augmented the training data with these images. With this
approach, we were able to increase the performance of machine learning models
and set a new benchmark amongst non-ensemble based models in skin lesion
classification on the HAM10000 dataset; and used the observed analytics and
generated models for detailed studies on model explainability, affirming the
effectiveness of our solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has been submitted to the Workshop on Synthetic Data
  for Computer Vision (SyntheticData4CV 2024 is a side event on 18th European
  Conference on Computer Vision 2024). This preprint has not undergone peer
  review or any post-submission improvements or corrections</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-Enhanced Ethical Hacking: A Linux-Focused Experiment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitham S. Al-Sinani, Chris J. Mitchell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This technical report investigates the integration of generative AI (GenAI),
specifically ChatGPT, into the practice of ethical hacking through a
comprehensive experimental study and conceptual analysis. Conducted in a
controlled virtual environment, the study evaluates GenAI's effectiveness
across the key stages of penetration testing on Linux-based target machines
operating within a virtual local area network (LAN), including reconnaissance,
scanning and enumeration, gaining access, maintaining access, and covering
tracks. The findings confirm that GenAI can significantly enhance and
streamline the ethical hacking process while underscoring the importance of
balanced human-AI collaboration rather than the complete replacement of human
input. The report also critically examines potential risks such as misuse, data
biases, hallucination, and over-reliance on AI. This research contributes to
the ongoing discussion on the ethical use of AI in cybersecurity and highlights
the need for continued innovation to strengthen security defences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SparsePO: Controlling Preference Alignment of LLMs via Sparse Token
  Masks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fenia Christopoulou, Ronald Cardenas, Gerasimos Lampouras, Haitham Bou-Ammar, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference Optimization (PO) has proven an effective step for aligning
language models to human-desired behaviors. Current variants, following the
offline Direct Preference Optimization objective, have focused on a strict
setting where all tokens are contributing signals of KL divergence and rewards
to the loss function. However, human preference is not affected by each word in
a sequence equally but is often dependent on specific words or phrases, e.g.
existence of toxic terms leads to non-preferred responses. Based on this
observation, we argue that not all tokens should be weighted equally during PO
and propose a flexible objective termed SparsePO, that aims to automatically
learn to weight the KL divergence and reward corresponding to each token during
PO training. We propose two different variants of weight-masks that can either
be derived from the reference model itself or learned on the fly. Notably, our
method induces sparsity in the learned masks, allowing the model to learn how
to best weight reward and KL divergence contributions at the token level,
learning an optimal level of mask sparsity. Extensive experiments on multiple
domains, including sentiment control, dialogue, text summarization and
text-to-code generation, illustrate that our approach assigns meaningful
weights to tokens according to the target task, generates more responses with
the desired preference and improves reasoning tasks by up to 2 percentage
points compared to other token- and response-level PO methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 papges, 9 figures, 5 tables. Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Structure of Game Provenance and its Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shawn Bowers, Yilin Xia, Bertram Ludäscher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Provenance in databases has been thoroughly studied for positive and for
recursive queries, then for first-order (FO) queries, i.e., having negation but
no recursion. Query evaluation can be understood as a two-player game where the
opponents argue whether or not a tuple is in the query answer. This
game-theoretic approach yields a natural provenance model for FO queries,
unifying how and why-not provenance. Here, we study the fine-grain structure of
game provenance. A game $G=(V,E)$ consists of positions $V$ and moves $E$ and
can be solved by computing the well-founded model of a single, unstratifiable
rule: \[ \text{win}(X) \leftarrow \text{move}(X, Y), \neg \, \text{win}(Y). \]
In the solved game $G^{\lambda}$, the value of a position $x\,{\in}\,V$ is
either won, lost, or drawn. This value is explained by the provenance
$\mathscr{P}$(x), i.e., certain (annotated) edges reachable from $x$. We
identify seven edge types that give rise to new kinds of provenance, i.e.,
potential, actual, and primary, and demonstrate that "not all moves are created
equal". We describe the new provenance types, show how they can be computed
while solving games, and discuss applications, e.g., for abstract argumentation
frameworks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ScienceAgentBench: Toward Rigorous Assessment of Language Agents for
  Data-Driven Scientific Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N. Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancements of language language models (LLMs) have piqued growing
interest in developing LLM-based language agents to automate scientific
discovery end-to-end, which has sparked both excitement and skepticism about
the true capabilities of such agents. In this work, we argue that for an agent
to fully automate scientific discovery, it must be able to complete all
essential tasks in the workflow. Thus, we call for rigorous assessment of
agents on individual tasks in a scientific workflow before making bold claims
on end-to-end automation. To this end, we present ScienceAgentBench, a new
benchmark for evaluating language agents for data-driven scientific discovery.
To ensure the scientific authenticity and real-world relevance of our
benchmark, we extract 102 tasks from 44 peer-reviewed publications in four
disciplines and engage nine subject matter experts to validate them. We unify
the target output for every task to a self-contained Python program file and
employ an array of evaluation metrics to examine the generated programs,
execution results, and costs. Each task goes through multiple rounds of manual
validation by annotators and subject matter experts to ensure its annotation
quality and scientific plausibility. We also propose two effective strategies
to mitigate data contamination concerns. Using our benchmark, we evaluate five
open-weight and proprietary LLMs, each with three frameworks: direct prompting,
OpenHands, and self-debug. Given three attempts for each task, the
best-performing agent can only solve 32.4% of the tasks independently and 34.3%
with expert-provided knowledge. These results underscore the limited capacities
of current language agents in generating code for data-driven discovery, let
alone end-to-end automation for scientific research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compression via <span class="highlight-title">Pre-train</span>ed <span class="highlight-title">Transformer</span>s: A Study on Byte-Level
  Multimodal Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Heurtel-Depeiges, Anian Ruoss, Joel Veness, Tim Genewein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models have recently been shown to be strong data compressors.
However, when accounting for their excessive parameter count, their compression
ratios are actually inferior to standard compression algorithms. Moreover,
naively reducing the number of parameters may not necessarily help as it leads
to worse predictions and thus weaker compression. In this paper, we conduct a
large-scale empirical study to investigate whether there is a sweet spot where
competitive compression ratios with pre-trained vanilla transformers are
possible. To this end, we train families of models on 165GB of raw byte
sequences of either text, image, or audio data (and all possible combinations
of the three) and then compress 1GB of out-of-distribution (OOD) data from each
modality. We find that relatively small models (i.e., millions of parameters)
can outperform standard general-purpose compression algorithms (gzip, LZMA2)
and even domain-specific compressors (PNG, JPEG 2000, FLAC) - even when
factoring in parameter count. We achieve, e.g., the lowest compression ratio of
0.49 on OOD audio data (vs. 0.54 for FLAC). To study the impact of model- and
dataset scale, we conduct extensive ablations and hyperparameter sweeps, and we
investigate the effect of unimodal versus multimodal training. We find that
even small models can be trained to perform well on multiple modalities, but,
in contrast to previously reported results with large-scale foundation models,
transfer to unseen modalities is generally weak.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent
  Sparse Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijie Yang, Zhihao Zhang, Zhuofu Chen, Zikun Li, Zhihao Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have driven significant advancements across
diverse NLP tasks, with long-context models gaining prominence for handling
extended inputs. However, the expanding key-value (KV) cache size required by
Transformer architectures intensifies the memory constraints, particularly
during the decoding phase, creating a significant bottleneck. Existing sparse
attention mechanisms designed to address this bottleneck have two limitations:
(1) they often fail to reliably identify the most relevant tokens for
attention, and (2) they overlook the spatial coherence of token selection
across consecutive Transformer layers, which can lead to performance
degradation and substantial overhead in token selection. This paper introduces
TidalDecode, a simple yet effective algorithm and system for fast and accurate
LLM decoding through position persistent sparse attention. TidalDecode
leverages the spatial coherence of tokens selected by existing sparse attention
methods and introduces a few token selection layers that perform full attention
to identify the tokens with the highest attention scores, while all other
layers perform sparse attention with the pre-selected tokens. This design
enables TidalDecode to substantially reduce the overhead of token selection for
sparse attention without sacrificing the quality of the generated results.
Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely
matches the generative performance of full attention methods while reducing the
LLM decoding latency by up to 2.1x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transition of $α$-mixing in Random Iterations with Applications in
  Queuing Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Attila Lovas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonlinear time series models incorporating exogenous regressors provide the
foundation for numerous significant models across econometrics, queuing theory,
machine learning, and various other disciplines. Despite their importance, the
framework for the statistical analysis of such models is still incomplete. In
contrast, multiple versions of the law of large numbers and the (functional)
central limit theorem have been established for weakly dependent variables. We
prove the transition of mixing properties of the exogenous regressor to the
response through a coupling argument, leveraging these established results.
Furthermore, we study Markov chains in random environments under a suitable
form of drift and minorization condition when the environment process is
non-stationary, merely having favorable mixing properties. Following a novel
statistical estimation theory approach and using the Cram\'er-Rao lower bound,
we also establish the functional central limit theorem. Additionally, we apply
our framework to single-server queuing models. Overall, these results open the
door to the statistical analysis of a large class of random iterative models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FreSh: Frequency Shifting for Accelerated Neural Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Kania, Marko Mihajlovic, Sergey Prokudin, Jacek Tabor, Przemysław Spurek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit Neural Representations (INRs) have recently gained attention as a
powerful approach for continuously representing signals such as images, videos,
and 3D shapes using multilayer perceptrons (MLPs). However, MLPs are known to
exhibit a low-frequency bias, limiting their ability to capture high-frequency
details accurately. This limitation is typically addressed by incorporating
high-frequency input embeddings or specialized activation layers. In this work,
we demonstrate that these embeddings and activations are often configured with
hyperparameters that perform well on average but are suboptimal for specific
input signals under consideration, necessitating a costly grid search to
identify optimal settings. Our key observation is that the initial frequency
spectrum of an untrained model's output correlates strongly with the model's
eventual performance on a given target signal. Leveraging this insight, we
propose frequency shifting (or FreSh), a method that selects embedding
hyperparameters to align the frequency spectrum of the model's initial output
with that of the target signal. We show that this simple initialization
technique improves performance across various neural representation methods and
tasks, achieving results comparable to extensive hyperparameter sweeps but with
only marginal computational overhead compared to training a single model with
default hyperparameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Named Clinical Entity Recognition Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wadood M Abdul, Marco AF Pimentel, Muhammad Umar Salman, Tathagata Raha, Clément Christophe, Praveen K Kanithi, Nasir Hayat, Ronnie Rajan, Shadab Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This technical report introduces a Named Clinical Entity Recognition
Benchmark for evaluating language models in healthcare, addressing the crucial
natural language processing (NLP) task of extracting structured information
from clinical narratives to support applications like automated coding,
clinical trial cohort identification, and clinical decision support.
  The leaderboard provides a standardized platform for assessing diverse
language models, including encoder and decoder architectures, on their ability
to identify and classify clinical entities across multiple medical domains. A
curated collection of openly available clinical datasets is utilized,
encompassing entities such as diseases, symptoms, medications, procedures, and
laboratory measurements. Importantly, these entities are standardized according
to the Observational Medical Outcomes Partnership (OMOP) Common Data Model,
ensuring consistency and interoperability across different healthcare systems
and datasets, and a comprehensive evaluation of model performance. Performance
of models is primarily assessed using the F1-score, and it is complemented by
various assessment modes to provide comprehensive insights into model
performance. The report also includes a brief analysis of models evaluated to
date, highlighting observed trends and limitations.
  By establishing this benchmarking framework, the leaderboard aims to promote
transparency, facilitate comparative analyses, and drive innovation in clinical
entity recognition tasks, addressing the need for robust evaluation methods in
healthcare NLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can LLMs plan paths with extra hints from solvers? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Wu, Sayan Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable capabilities in natural
language processing, mathematical problem solving, and tasks related to program
synthesis. However, their effectiveness in long-term planning and higher-order
reasoning has been noted to be limited and fragile. This paper explores an
approach for enhancing LLM performance in solving a classical robotic planning
task by integrating solver-generated feedback. We explore four different
strategies for providing feedback, including visual feedback, we utilize
fine-tuning, and we evaluate the performance of three different LLMs across a
10 standard and 100 more randomly generated planning problems. Our results
suggest that the solver-generated feedback improves the LLM's ability to solve
the moderately difficult problems, but the harder problems still remain out of
reach. The study provides detailed analysis of the effects of the different
hinting strategies and the different planning tendencies of the evaluated LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PhotoReg: Photometrically Registering 3D Gaussian Splatting Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwen Yuan, Tianyi Zhang, Matthew Johnson-Roberson, Weiming Zhi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building accurate representations of the environment is critical for
intelligent robots to make decisions during deployment. Advances in
photorealistic environment models have enabled robots to develop
hyper-realistic reconstructions, which can be used to generate images that are
intuitive for human inspection. In particular, the recently introduced
\ac{3DGS}, which describes the scene with up to millions of primitive
ellipsoids, can be rendered in real time. \ac{3DGS} has rapidly gained
prominence. However, a critical unsolved problem persists: how can we fuse
multiple \ac{3DGS} into a single coherent model? Solving this problem will
enable robot teams to jointly build \ac{3DGS} models of their surroundings. A
key insight of this work is to leverage the {duality} between photorealistic
reconstructions, which render realistic 2D images from 3D structure, and
\emph{3D foundation models}, which predict 3D structure from image pairs. To
this end, we develop PhotoReg, a framework to register multiple photorealistic
\ac{3DGS} models with 3D foundation models. As \ac{3DGS} models are generally
built from monocular camera images, they have \emph{arbitrary scale}. To
resolve this, PhotoReg actively enforces scale consistency among the different
\ac{3DGS} models by considering depth estimates within these models. Then, the
alignment is iteratively refined with fine-grained photometric losses to
produce high-quality fused \ac{3DGS} models. We rigorously evaluate PhotoReg on
both standard benchmark datasets and our custom-collected datasets, including
with two quadruped robots. The code is released at
\url{ziweny11.github.io/photoreg}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stage-Wise and Prior-Aware Neural Speech Phase Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Liu, Yang Ai, Hui-Peng Du, Ye-Xin Lu, Rui-Chen Zheng, Zhen-Hua Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel Stage-wise and Prior-aware Neural Speech Phase
Prediction (SP-NSPP) model, which predicts the phase spectrum from input
amplitude spectrum by two-stage neural networks. In the initial
prior-construction stage, we preliminarily predict a rough prior phase spectrum
from the amplitude spectrum. The subsequent refinement stage transforms the
amplitude spectrum into a refined high-quality phase spectrum conditioned on
the prior phase. Networks in both stages use ConvNeXt v2 blocks as the backbone
and adopt adversarial training by innovatively introducing a phase spectrum
discriminator (PSD). To further improve the continuity of the refined phase, we
also incorporate a time-frequency integrated difference (TFID) loss in the
refinement stage. Experimental results confirm that, compared to neural
network-based no-prior phase prediction methods, the proposed SP-NSPP achieves
higher phase prediction accuracy, thanks to introducing the coarse phase priors
and diverse training criteria. Compared to iterative phase estimation
algorithms, our proposed SP-NSPP does not require multiple rounds of staged
iterations, resulting in higher generation efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SLT2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 6DGS: Enhanced Direction-Aware Gaussian Splatting for Volumetric
  Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongpai Gao, Benjamin Planche, Meng Zheng, Anwesa Choudhuri, Terrence Chen, Ziyan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novel view synthesis has advanced significantly with the development of
neural radiance fields (NeRF) and 3D Gaussian splatting (3DGS). However,
achieving high quality without compromising real-time rendering remains
challenging, particularly for physically-based ray tracing with view-dependent
effects. Recently, N-dimensional Gaussians (N-DG) introduced a 6D
spatial-angular representation to better incorporate view-dependent effects,
but the Gaussian representation and control scheme are sub-optimal. In this
paper, we revisit 6D Gaussians and introduce 6D Gaussian Splatting (6DGS),
which enhances color and opacity representations and leverages the additional
directional information in the 6D space for optimized Gaussian control. Our
approach is fully compatible with the 3DGS framework and significantly improves
real-time radiance field rendering by better modeling view-dependent effects
and fine details. Experiments demonstrate that 6DGS significantly outperforms
3DGS and N-DG, achieving up to a 15.73 dB improvement in PSNR with a reduction
of 66.5% Gaussian points compared to 3DGS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Demo Video: https://www.youtube.com/watch?v=77wN-K6Q9aM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaboration! Towards Robust Neural Methods for Routing Problems <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianan Zhou, Yaoxin Wu, Zhiguang Cao, Wen Song, Jie Zhang, Zhiqi Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite enjoying desirable efficiency and reduced reliance on domain
expertise, existing neural methods for vehicle routing problems (VRPs) suffer
from severe robustness issues -- their performance significantly deteriorates
on clean instances with crafted perturbations. To enhance robustness, we
propose an ensemble-based Collaborative Neural Framework (CNF) w.r.t. the
defense of neural VRP methods, which is crucial yet underexplored in the
literature. Given a neural VRP method, we adversarially train multiple models
in a collaborative manner to synergistically promote robustness against
attacks, while boosting standard generalization on clean instances. A neural
router is designed to adeptly distribute training instances among models,
enhancing overall load balancing and collaborative efficacy. Extensive
experiments verify the effectiveness and versatility of CNF in defending
against various attacks across different neural VRP methods. Notably, our
approach also achieves impressive out-of-distribution generalization on
benchmark instances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Activation Scaling for Steering and Interpreting Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Stoehr, Kevin Du, Vésteinn Snæbjarnarson, Robert West, Ryan Cotterell, Aaron Schein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the prompt "Rome is in", can we steer a language model to flip its
prediction of an incorrect token "France" to a correct token "Italy" by only
multiplying a few relevant activation vectors with scalars? We argue that
successfully intervening on a model is a prerequisite for interpreting its
internal workings. Concretely, we establish a three-term objective: a
successful intervention should flip the correct with the wrong token and vice
versa (effectiveness), and leave other tokens unaffected (faithfulness), all
while being sparse (minimality). Using gradient-based optimization, this
objective lets us learn (and later evaluate) a specific kind of efficient and
interpretable intervention: activation scaling only modifies the signed
magnitude of activation vectors to strengthen, weaken, or reverse the steering
directions already encoded in the model. On synthetic tasks, this intervention
performs comparably with steering vectors in terms of effectiveness and
faithfulness, but is much more minimal allowing us to pinpoint interpretable
model components. We evaluate activation scaling from different angles, compare
performance on different datasets, and make activation scalars a learnable
function of the activation vectors themselves to generalize to varying-length
prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of the Association for Computational Linguistics: EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leverage Knowledge Graph and Large Language Model for Law Article
  Recommendation: A Case Study of Chinese Criminal Law 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongming Chen, Miner Chen, Ye Zhu, Juan Pei, Siyu Chen, Yu Zhou, Yi Wang, Yifan Zhou, Hao Li, Songan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Court efficiency is vital for social stability. However, in most countries
around the world, the grassroots courts face case backlogs, with decisions
relying heavily on judicial personnel's cognitive labor, lacking intelligent
tools to improve efficiency. To address this issue, we propose an efficient law
article recommendation approach utilizing a Knowledge Graph (KG) and a Large
Language Model (LLM). Firstly, we propose a Case-Enhanced Law Article Knowledge
Graph (CLAKG) as a database to store current law statutes, historical case
information, and correspondence between law articles and historical cases.
Additionally, we introduce an automated CLAKG construction method based on LLM.
On this basis, we propose a closed-loop law article recommendation method.
Finally, through a series of experiments using judgment documents from the
website "China Judgements Online", we have improved the accuracy of law article
recommendation in cases from 0.549 to 0.694, demonstrating that our proposed
method significantly outperforms baseline approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Ship Recognition and Georeferencing for the Improvement of
  Maritime Situational Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Borja Carrillo Perez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an era where maritime infrastructures are crucial, advanced situational
awareness solutions are increasingly important. The use of optical camera
systems can allow real-time usage of maritime footage. This thesis presents an
investigation into leveraging deep learning and computer vision to advance
real-time ship recognition and georeferencing for the improvement of maritime
situational awareness. A novel dataset, ShipSG, is introduced, containing 3,505
images and 11,625 ship masks with corresponding class and geographic position.
After an exploration of state-of-the-art, a custom real-time segmentation
architecture, ScatYOLOv8+CBAM, is designed for the NVIDIA Jetson AGX Xavier
embedded system. This architecture adds the 2D scattering transform and
attention mechanisms to YOLOv8, achieving an mAP of 75.46% and an 25.3 ms per
frame, outperforming state-of-the-art methods by over 5%. To improve small and
distant ship recognition in high-resolution images on embedded systems, an
enhanced slicing mechanism is introduced, improving mAP by 8% to 11%.
Additionally, a georeferencing method is proposed, achieving positioning errors
of 18 m for ships up to 400 m away and 44 m for ships between 400 m and 1200 m.
The findings are also applied in real-world scenarios, such as the detection of
abnormal ship behaviour, camera integrity assessment and 3D reconstruction. The
approach of this thesis outperforms existing methods and provides a framework
for integrating recognized and georeferenced ships into real-time systems,
enhancing operational effectiveness and decision-making for maritime
stakeholders. This thesis contributes to the maritime computer vision field by
establishing a benchmark for ship segmentation and georeferencing research,
demonstrating the viability of deep-learning-based recognition and
georeferencing methods for real-time maritime monitoring.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting and Approximating Redundant Computational Blocks in Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irene Cannistraci, Emanuele Rodolà, Bastian Rieck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks often learn similar internal representations, both
across different models and within their own layers. While inter-network
similarities have enabled techniques such as model stitching and merging,
intra-network similarities present new opportunities for designing more
efficient architectures. In this paper, we investigate the emergence of these
internal similarities across different layers in diverse neural architectures,
showing that similarity patterns emerge independently of the datataset used. We
introduce a simple metric, Block Redundancy, to detect redundant blocks,
providing a foundation for future architectural optimization methods. Building
on this, we propose Redundant Blocks Approximation (RBA), a general framework
that identifies and approximates one or more redundant computational blocks
using simpler transformations. We show that the transformation $\mathcal{T}$
between two representations can be efficiently computed in closed-form, and it
is enough to replace the redundant blocks from the network. RBA reduces model
parameters and time complexity while maintaining good performance. We validate
our method on classification tasks in the vision domain using a variety of
pretrained foundational models and datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 10 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Interactive Agent in Large FPS Game Map with Rule-enhanced
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Zhang, Huan Hu, Yuan Zhou, Qiyang Cao, Ruochen Liu, Wenya Wei, Elvis S. Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of competitive gaming, 3D first-person shooter (FPS) games have
gained immense popularity, prompting the development of game AI systems to
enhance gameplay. However, deploying game AI in practical scenarios still poses
challenges, particularly in large-scale and complex FPS games. In this paper,
we focus on the practical deployment of game AI in the online multiplayer
competitive 3D FPS game called Arena Breakout, developed by Tencent Games. We
propose a novel gaming AI system named Private Military Company Agent (PMCA),
which is interactable within a large game map and engages in combat with
players while utilizing tactical advantages provided by the surrounding
terrain.
  To address the challenges of navigation and combat in modern 3D FPS games, we
introduce a method that combines navigation mesh (Navmesh) and shooting-rule
with deep reinforcement learning (NSRL). The integration of Navmesh enhances
the agent's global navigation capabilities while shooting behavior is
controlled using rule-based methods to ensure controllability. NSRL employs a
DRL model to predict when to enable the navigation mesh, resulting in a diverse
range of behaviors for the game AI. Customized rewards for human-like behaviors
are also employed to align PMCA's behavior with that of human players.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Role of Governments in Increasing Interconnected Post-Deployment
  Monitoring of AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Merlin Stein, Jamie Bernardi, Connor Dunlop
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language-based AI systems are diffusing into society, bringing positive and
negative impacts. Mitigating negative impacts depends on accurate impact
assessments, drawn from an empirical evidence base that makes causal
connections between AI usage and impacts. Interconnected post-deployment
monitoring combines information about model integration and use, application
use, and incidents and impacts. For example, inference time monitoring of
chain-of-thought reasoning can be combined with long-term monitoring of
sectoral AI diffusion, impacts and incidents. Drawing on information sharing
mechanisms in other industries, we highlight example data sources and specific
data points that governments could collect to inform AI risk management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defense-as-a-Service: Black-box Shielding against Backdoored Graph
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Yang, Kai Zhou, Yuni Lai, Gaolei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the trend of large graph learning models, business owners tend to employ
a model provided by a third party to deliver business services to users.
However, these models might be backdoored, and malicious users can submit
trigger-embedded inputs to manipulate the model predictions. Current graph
backdoor defenses have several limitations: 1) depending on model-related
details, 2) requiring additional model fine-tuning, and 3) relying upon extra
explainability tools, all of which are infeasible under stringent privacy
policies. To address those limitations, we propose GraphProt, which allows
resource-constrained business owners to rely on third parties to avoid backdoor
attacks on GNN-based graph classifiers. Our GraphProt is model-agnostic and
only relies on the input graph. The key insight is to leverage subgraph
information for prediction, thereby mitigating backdoor effects induced by
triggers. GraphProt comprises two components: clustering-based trigger
elimination and robust subgraph ensemble. Specifically, we first propose
feature-topology clustering that aims to remove most of the anomalous subgraphs
(triggers). Moreover, we design subgraph sampling strategies based on
feature-topology clustering to build a robust classifier via majority vote.
Experimental results across three backdoor attacks and six benchmark datasets
demonstrate that GraphProt significantly reduces the backdoor attack success
rate while preserving the model accuracy on regular graph classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Patch is Enough: Naturalistic Adversarial Patch against Vision-Language
  <span class="highlight-title">Pre-train</span>ing Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dehong Kong, Siyuan Liang, Xiaopeng Zhu, Yuansheng Zhong, Wenqi Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual language pre-training (VLP) models have demonstrated significant
success across various domains, yet they remain vulnerable to adversarial
attacks. Addressing these adversarial vulnerabilities is crucial for enhancing
security in multimodal learning. Traditionally, adversarial methods targeting
VLP models involve simultaneously perturbing images and text. However, this
approach faces notable challenges: first, adversarial perturbations often fail
to translate effectively into real-world scenarios; second, direct
modifications to the text are conspicuously visible. To overcome these
limitations, we propose a novel strategy that exclusively employs image patches
for attacks, thus preserving the integrity of the original text. Our method
leverages prior knowledge from diffusion models to enhance the authenticity and
naturalness of the perturbations. Moreover, to optimize patch placement and
improve the efficacy of our attacks, we utilize the cross-attention mechanism,
which encapsulates intermodal interactions by generating attention maps to
guide strategic patch placements. Comprehensive experiments conducted in a
white-box setting for image-to-text scenarios reveal that our proposed method
significantly outperforms existing techniques, achieving a 100% attack success
rate. Additionally, it demonstrates commendable performance in transfer tasks
involving text-to-image configurations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by Visual Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Grammar Induction for Language Understanding and Generation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jushi Kai, Shengyuan Hou, Yusheng Huang, Zhouhan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grammar induction has made significant progress in recent years. However, it
is not clear how the application of induced grammar could enhance practical
performance in downstream tasks. In this work, we introduce an unsupervised
grammar induction method for language understanding and generation. We
construct a grammar parser to induce constituency structures and dependency
relations, which is simultaneously trained on downstream tasks without
additional syntax annotations. The induced grammar features are subsequently
incorporated into Transformer as a syntactic mask to guide self-attention. We
evaluate and apply our method to multiple machine translation tasks and natural
language understanding tasks. Our method demonstrates superior performance
compared to the original Transformer and other models enhanced with external
parsers. Experimental results indicate that our method is effective in both
from-scratch and pre-trained scenarios. Additionally, our research highlights
the contribution of explicitly modeling the grammatical structure of texts to
neural network models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mastering Chinese Chess AI (Xiangqi) Without Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Chen, Juntong Lin, Zhichao Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We have developed a high-performance Chinese Chess AI that operates without
reliance on search algorithms. This AI has demonstrated the capability to
compete at a level commensurate with the top 0.1\% of human players. By
eliminating the search process typically associated with such systems, this AI
achieves a Queries Per Second (QPS) rate that exceeds those of systems based on
the Monte Carlo Tree Search (MCTS) algorithm by over a thousandfold and
surpasses those based on the AlphaBeta pruning algorithm by more than a
hundredfold. The AI training system consists of two parts: supervised learning
and reinforcement learning. Supervised learning provides an initial human-like
Chinese chess AI, while reinforcement learning, based on supervised learning,
elevates the strength of the entire AI to a new level. Based on this training
system, we carried out enough ablation experiments and discovered that 1. The
same parameter amount of Transformer architecture has a higher performance than
CNN on Chinese chess; 2. Possible moves of both sides as features can greatly
improve the training process; 3. Selective opponent pool, compared to pure
self-play training, results in a faster improvement curve and a higher strength
limit. 4. Value Estimation with Cutoff(VECT) improves the original PPO
algorithm training process and we will give the explanation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Skill Discovery for Robotic Manipulation through Automatic
  Task Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Jansonnie, Bingbing Wu, Julien Perez, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning skills that interact with objects is of major importance for robotic
manipulation. These skills can indeed serve as an efficient prior for solving
various manipulation tasks. We propose a novel Skill Learning approach that
discovers composable behaviors by solving a large and diverse number of
autonomously generated tasks. Our method learns skills allowing the robot to
consistently and robustly interact with objects in its environment. The
discovered behaviors are embedded in primitives which can be composed with
Hierarchical Reinforcement Learning to solve unseen manipulation tasks. In
particular, we leverage Asymmetric Self-Play to discover behaviors and
Multiplicative Compositional Policies to embed them. We compare our method to
Skill Learning baselines and find that our skills are more interactive.
Furthermore, the learned skills can be used to solve a set of unseen
manipulation tasks, in simulation as well as on a real robotic platform.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2024 IEEE-RAS International Conference on Humanoid
  Robots</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TimeCNN: Refining Cross-Variable Interaction on Time Point for Time
  Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao Hu, Dongkai Wang, Yong Dai, Shiyi Qi, Liangjian Wen, Jun Wang, Zhi Chen, Xun Zhou, Zenglin Xu, Jiang Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series forecasting is extensively applied across diverse domains.
Transformer-based models demonstrate significant potential in modeling
cross-time and cross-variable interaction. However, we notice that the
cross-variable correlation of multivariate time series demonstrates
multifaceted (positive and negative correlations) and dynamic progression over
time, which is not well captured by existing Transformer-based models. To
address this issue, we propose a TimeCNN model to refine cross-variable
interactions to enhance time series forecasting. Its key innovation is
timepoint-independent, where each time point has an independent convolution
kernel, allowing each time point to have its independent model to capture
relationships among variables. This approach effectively handles both positive
and negative correlations and adapts to the evolving nature of variable
relationships over time. Extensive experiments conducted on 12 real-world
datasets demonstrate that TimeCNN consistently outperforms state-of-the-art
models. Notably, our model achieves significant reductions in computational
requirements (approximately 60.46%) and parameter count (about 57.50%), while
delivering inference speeds 3 to 4 times faster than the benchmark iTransformer
model
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PostEdit: Posterior Sampling for Efficient Zero-Shot Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Tian, Yixuan Li, Yichao Yan, Shanyan Guan, Yanhao Ge, Xiaokang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of image editing, three core challenges persist:
controllability, background preservation, and efficiency. Inversion-based
methods rely on time-consuming optimization to preserve the features of the
initial images, which results in low efficiency due to the requirement for
extensive network inference. Conversely, inversion-free methods lack
theoretical support for background similarity, as they circumvent the issue of
maintaining initial features to achieve efficiency. As a consequence, none of
these methods can achieve both high efficiency and background consistency. To
tackle the challenges and the aforementioned disadvantages, we introduce
PostEdit, a method that incorporates a posterior scheme to govern the diffusion
sampling process. Specifically, a corresponding measurement term related to
both the initial features and Langevin dynamics is introduced to optimize the
estimated image generated by the given target prompt. Extensive experimental
results indicate that the proposed PostEdit achieves state-of-the-art editing
performance while accurately preserving unedited regions. Furthermore, the
method is both inversion- and training-free, necessitating approximately 1.5
seconds and 18 GB of GPU memory to generate high-quality results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Fusion Strategies for Mapping Biophysical Landscape Features <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucia Gordon, Nico Lang, Catherine Ressijac, Andrew Davies
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal aerial data are used to monitor natural systems, and machine
learning can significantly accelerate the classification of landscape features
within such imagery to benefit ecology and conservation. It remains
under-explored, however, how these multiple modalities ought to be fused in a
deep learning model. As a step towards filling this gap, we study three
strategies (Early fusion, Late fusion, and Mixture of Experts) for fusing
thermal, RGB, and LiDAR imagery using a dataset of spatially-aligned
orthomosaics in these three modalities. In particular, we aim to map three
ecologically-relevant biophysical landscape features in African savanna
ecosystems: rhino middens, termite mounds, and water. The three fusion
strategies differ in whether the modalities are fused early or late, and if
late, whether the model learns fixed weights per modality for each class or
generates weights for each class adaptively, based on the input. Overall, the
three methods have similar macro-averaged performance with Late fusion
achieving an AUC of 0.698, but their per-class performance varies strongly,
with Early fusion achieving the best recall for middens and water and Mixture
of Experts achieving the best recall for mounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, ECCV 2024 Workshop in CV for Ecology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resource-Efficient Multiview Perception: Integrating Semantic Masking
  with Masked Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kosta Dakic, Kanchana Thilakarathna, Rodrigo N. Calheiros, Teng Joon Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiview systems have become a key technology in modern computer vision,
offering advanced capabilities in scene understanding and analysis. However,
these systems face critical challenges in bandwidth limitations and
computational constraints, particularly for resource-limited camera nodes like
drones. This paper presents a novel approach for communication-efficient
distributed multiview detection and tracking using masked autoencoders (MAEs).
We introduce a semantic-guided masking strategy that leverages pre-trained
segmentation models and a tunable power function to prioritize informative
image regions. This approach, combined with an MAE, reduces communication
overhead while preserving essential visual information. We evaluate our method
on both virtual and real-world multiview datasets, demonstrating comparable
performance in terms of detection and tracking performance metrics compared to
state-of-the-art techniques, even at high masking ratios. Our selective masking
algorithm outperforms random masking, maintaining higher accuracy and precision
as the masking ratio increases. Furthermore, our approach achieves a
significant reduction in transmission data volume compared to baseline methods,
thereby balancing multiview tracking performance with communication efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Review</span> of Artificial Intelligence based Biological-Tree Construction:
  Priorities, Methods, Applications and Trends 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelin Zang, Yongjie Xu, Chenrui Duan, Jinlin Wu, Stan Z. Li, Zhen Lei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biological tree analysis serves as a pivotal tool in uncovering the
evolutionary and differentiation relationships among organisms, genes, and
cells. Its applications span diverse fields including phylogenetics,
developmental biology, ecology, and medicine. Traditional tree inference
methods, while foundational in early studies, face increasing limitations in
processing the large-scale, complex datasets generated by modern
high-throughput technologies. Recent advances in deep learning offer promising
solutions, providing enhanced data processing and pattern recognition
capabilities. However, challenges remain, particularly in accurately
representing the inherently discrete and non-Euclidean nature of biological
trees. In this review, we first outline the key biological priors fundamental
to phylogenetic and differentiation tree analyses, facilitating a deeper
interdisciplinary understanding between deep learning researchers and
biologists. We then systematically examine the commonly used data formats and
databases, serving as a comprehensive resource for model testing and
development. We provide a critical analysis of traditional tree generation
methods, exploring their underlying biological assumptions, technical
characteristics, and limitations. Current developments in deep learning-based
tree generation are reviewed, highlighting both recent advancements and
existing challenges. Furthermore, we discuss the diverse applications of
biological trees across various biological domains. Finally, we propose
potential future directions and trends in leveraging deep learning for
biological tree research, aiming to guide further exploration and innovation in
this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>83 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Interpretable Hierarchical Dynamical Systems Models from Time
  Series Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Brenner, Elias Weber, Georgia Koppe, Daniel Durstewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In science, we are often interested in obtaining a generative model of the
underlying system dynamics from observed time series. While powerful methods
for dynamical systems reconstruction (DSR) exist when data come from a single
domain, how to best integrate data from multiple dynamical regimes and leverage
it for generalization is still an open question. This becomes particularly
important when individual time series are short, and group-level information
may help to fill in for gaps in single-domain data. At the same time, averaging
is not an option in DSR, as it will wipe out crucial dynamical properties
(e.g., limit cycles in one domain vs. chaos in another). Hence, a framework is
needed that enables to efficiently harvest group-level (multi-domain)
information while retaining all single-domain dynamical characteristics. Here
we provide such a hierarchical approach and showcase it on popular DSR
benchmarks, as well as on neuroscientific and medical time series. In addition
to faithful reconstruction of all individual dynamical regimes, our
unsupervised methodology discovers common low-dimensional feature spaces in
which datasets with similar dynamics cluster. The features spanning these
spaces were further dynamically highly interpretable, surprisingly in often
linear relation to control parameters that govern the dynamics of the
underlying system. Finally, we illustrate transfer learning and generalization
to new parameter regimes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transforming Color: A Novel Image Colorization Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamza Shafiq, Bumshik Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel method for image colorization that utilizes a
color transformer and generative adversarial networks (GANs) to address the
challenge of generating visually appealing colorized images. Conventional
approaches often struggle with capturing long-range dependencies and producing
realistic colorizations. The proposed method integrates a transformer
architecture to capture global information and a GAN framework to improve
visual quality. In this study, a color encoder that utilizes a random normal
distribution to generate color features is applied. These features are then
integrated with grayscale image features to enhance the overall representation
of the images. Our method demonstrates superior performance compared with
existing approaches by utilizing the capacity of the transformer, which can
capture long-range dependencies and generate a realistic colorization of the
GAN. Experimental results show that the proposed network significantly
outperforms other state-of-the-art colorization techniques, highlighting its
potential for image colorization. This research opens new possibilities for
precise and visually compelling image colorization in domains such as digital
restoration and historical image analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representing the Under-Represented: Cultural and Core Capability
  Benchmarks for Developing Thai Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dahyun Kim, Sukyung Lee, Yungi Kim, Attapol Rutherford, Chanjun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of large language models (LLMs) has highlighted the
need for robust evaluation frameworks that assess their core capabilities, such
as reasoning, knowledge, and commonsense, leading to the inception of certain
widely-used benchmark suites such as the H6 benchmark. However, these benchmark
suites are primarily built for the English language, and there exists a lack
thereof for under-represented languages, in terms of LLM development, such as
Thai. On the other hand, developing LLMs for Thai should also include enhancing
the cultural understanding as well as core capabilities. To address these dual
challenge in Thai LLM research, we propose two key benchmarks: Thai-H6 and Thai
Cultural and Linguistic Intelligence Benchmark (ThaiCLI). Through a thorough
evaluation of various LLMs with multi-lingual capabilities, we provide a
comprehensive analysis of the proposed benchmarks and how they contribute to
Thai LLM development. Furthermore, we will make both the datasets and
evaluation code publicly available to encourage further research and
development for Thai LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis of Hybrid Compositions in Animation Film with Weakly Supervised
  Learning <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mónica Apellaniz Portos, Roberto Labadie-Tamayo, Claudius Stemmler, Erwin Feyersinger, Andreas Babic, Franziska Bruckner, Vrääth Öhner, Matthias Zeppelzauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an approach for the analysis of hybrid visual compositions in
animation in the domain of ephemeral film. We combine ideas from
semi-supervised and weakly supervised learning to train a model that can
segment hybrid compositions without requiring pre-labeled segmentation masks.
We evaluate our approach on a set of ephemeral films from 13 film archives.
Results demonstrate that the proposed learning strategy yields a performance
close to a fully supervised baseline. On a qualitative level the performed
analysis provides interesting insights on hybrid compositions in animation
film.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Vision for Art (VISART VII) Workshop at the European Conference of
  Computer Vision (ECCV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Training of Sinusoidal Neural Fields via Scaling Initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taesun Yeom, Sangyoon Lee, Jaeho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural fields are an emerging paradigm that represent data as continuous
functions parameterized by neural networks. Despite many advantages, neural
fields often have a high training cost, which prevents a broader adoption. In
this paper, we focus on a popular family of neural fields, called sinusoidal
neural fields (SNFs), and study how it should be initialized to maximize the
training speed. We find that the standard initialization scheme for SNFs --
designed based on the signal propagation principle -- is suboptimal. In
particular, we show that by simply multiplying each weight (except for the last
layer) by a constant, we can accelerate SNF training by 10$\times$. This
method, coined $\textit{weight scaling}$, consistently provides a significant
speedup over various data domains, allowing the SNFs to train faster than more
recently proposed architectures. To understand why the weight scaling works
well, we conduct extensive theoretical and empirical analyses which reveal that
the weight scaling not only resolves the spectral bias quite effectively but
also enjoys a well-conditioned optimization trajectory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Molecular topological deep learning for polymer property prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Shen, Yipeng Zhang, Fei Han, Kelin Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and efficient prediction of polymer properties is of key importance
for polymer design. Traditional experimental tools and density function theory
(DFT)-based simulations for polymer property evaluation, are both expensive and
time-consuming. Recently, a gigantic amount of graph-based molecular models
have emerged and demonstrated huge potential in molecular data analysis. Even
with the great progresses, these models tend to ignore the high-order and
mutliscale information within the data. In this paper, we develop molecular
topological deep learning (Mol-TDL) for polymer property analysis. Our Mol-TDL
incorporates both high-order interactions and multiscale properties into
topological deep learning architecture. The key idea is to represent polymer
molecules as a series of simplicial complices at different scales and build up
simplical neural networks accordingly. The aggregated information from
different scales provides a more accurate prediction of polymer molecular
properties.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Driving with Regulation: Interpretable Decision-Making for Autonomous
  Vehicles with Retrieval-Augmented Reasoning via LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhui Cai, Yifan Liu, Zewei Zhou, Haoxuan Ma, Seth Z. Zhao, Zhiwen Wu, Jiaqi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents an interpretable decision-making framework for autonomous
vehicles that integrates traffic regulations, norms, and safety guidelines
comprehensively and enables seamless adaptation to different regions. While
traditional rule-based methods struggle to incorporate the full scope of
traffic rules, we develop a Traffic Regulation Retrieval (TRR) Agent based on
Retrieval-Augmented Generation (RAG) to automatically retrieve relevant traffic
rules and guidelines from extensive regulation documents and relevant records
based on the ego vehicle's situation. Given the semantic complexity of the
retrieved rules, we also design a reasoning module powered by a Large Language
Model (LLM) to interpret these rules, differentiate between mandatory rules and
safety guidelines, and assess actions on legal compliance and safety.
Additionally, the reasoning is designed to be interpretable, enhancing both
transparency and reliability. The framework demonstrates robust performance on
both hypothesized and real-world cases across diverse scenarios, along with the
ability to adapt to different regions with ease.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Item Cluster-aware <span class="highlight-title">Prompt</span> Learning for Session-based Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wooseong Yang, Chen Wang, Zihe Song, Weizhi Zhang, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Session-based recommendation (SBR) aims to capture dynamic user preferences
by analyzing item sequences within individual sessions. However, most existing
approaches focus mainly on intra-session item relationships, neglecting the
connections between items across different sessions (inter-session
relationships), which limits their ability to fully capture complex item
interactions. While some methods incorporate inter-session information, they
often suffer from high computational costs, leading to longer training times
and reduced efficiency. To address these challenges, we propose the CLIP-SBR
(Cluster-aware Item Prompt learning for Session-Based Recommendation)
framework. CLIP-SBR is composed of two modules: 1) an item relationship mining
module that builds a global graph to effectively model both intra- and
inter-session relationships, and 2) an item cluster-aware prompt learning
module that uses soft prompts to integrate these relationships into SBR models
efficiently. We evaluate CLIP-SBR across eight SBR models and three benchmark
datasets, consistently demonstrating improved recommendation performance and
establishing CLIP-SBR as a robust solution for session-based recommendation
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImProver: Agent-Based Automated Proof Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riyaz Ahuja, Jeremy Avigad, Prasad Tetali, Sean Welleck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been used to generate formal proofs of
mathematical theorems in proofs assistants such as Lean. However, we often want
to optimize a formal proof with respect to various criteria, depending on its
downstream use. For example, we may want a proof to adhere to a certain style,
or to be readable, concise, or modularly structured. Having suitably optimized
proofs is also important for learning tasks, especially since human-written
proofs may not optimal for that purpose. To this end, we study a new problem of
automated proof optimization: rewriting a proof so that it is correct and
optimizes for an arbitrary criterion, such as length or readability. As a first
method for automated proof optimization, we present ImProver, a
large-language-model agent that rewrites proofs to optimize arbitrary
user-defined metrics in Lean. We find that naively applying LLMs to proof
optimization falls short, and we incorporate various improvements into
ImProver, such as the use of symbolic Lean context in a novel Chain-of-States
technique, as well as error-correction and retrieval. We test ImProver on
rewriting real-world undergraduate, competition, and research-level mathematics
theorems, finding that ImProver is capable of rewriting proofs so that they are
substantially shorter, more modular, and more readable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Generalization Ability of Spatiotemporal Model in Urban
  Scenario 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjun Wang, Jiyuan Chen, Tong Pan, Zheng Dong, Lingyu Zhang, Renhe Jiang, Xuan Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatiotemporal neural networks have shown great promise in urban scenarios by
effectively capturing temporal and spatial correlations. However, urban
environments are constantly evolving, and current model evaluations are often
limited to traffic scenarios and use data mainly collected only a few weeks
after training period to evaluate model performance. The generalization ability
of these models remains largely unexplored. To address this, we propose a
Spatiotemporal Out-of-Distribution (ST-OOD) benchmark, which comprises six
urban scenario: bike-sharing, 311 services, pedestrian counts, traffic speed,
traffic flow, ride-hailing demand, and bike-sharing, each with in-distribution
(same year) and out-of-distribution (next years) settings. We extensively
evaluate state-of-the-art spatiotemporal models and find that their performance
degrades significantly in out-of-distribution settings, with most models
performing even worse than a simple Multi-Layer Perceptron (MLP). Our findings
suggest that current leading methods tend to over-rely on parameters to overfit
training data, which may lead to good performance on in-distribution data but
often results in poor generalization. We also investigated whether dropout
could mitigate the negative effects of overfitting. Our results showed that a
slight dropout rate could significantly improve generalization performance on
most datasets, with minimal impact on in-distribution performance. However,
balancing in-distribution and out-of-distribution performance remains a
challenging problem. We hope that the proposed benchmark will encourage further
research on this critical issue.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TableRAG: Million-Token Table Understanding with Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Si-An Chen, Lesly Miculicich, Julian Martin Eisenschlos, Zifeng Wang, Zilong Wang, Yanfei Chen, Yasuhisa Fujii, Hsuan-Tien Lin, Chen-Yu Lee, Tomas Pfister
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in language models (LMs) have notably enhanced their
ability to reason with tabular data, primarily through program-aided mechanisms
that manipulate and analyze tables. However, these methods often require the
entire table as input, leading to scalability challenges due to the positional
bias or context length constraints. In response to these challenges, we
introduce TableRAG, a Retrieval-Augmented Generation (RAG) framework
specifically designed for LM-based table understanding. TableRAG leverages
query expansion combined with schema and cell retrieval to pinpoint crucial
information before providing it to the LMs. This enables more efficient data
encoding and precise retrieval, significantly reducing prompt lengths and
mitigating information loss. We have developed two new million-token benchmarks
from the Arcade and BIRD-SQL datasets to thoroughly evaluate TableRAG's
effectiveness at scale. Our results demonstrate that TableRAG's retrieval
design achieves the highest retrieval quality, leading to the new
state-of-the-art performance on large-scale table understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProtoNAM: Prototypical Neural Additive Models for Interpretable Deep
  Tabular Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzhi Xiong, Sanchit Sinha, Aidong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalized additive models (GAMs) have long been a powerful white-box tool
for the intelligible analysis of tabular data, revealing the influence of each
feature on the model predictions. Despite the success of neural networks (NNs)
in various domains, their application as NN-based GAMs in tabular data analysis
remains suboptimal compared to tree-based ones, and the opacity of encoders in
NN-GAMs also prevents users from understanding how networks learn the
functions. In this work, we propose a new deep tabular learning method, termed
Prototypical Neural Additive Model (ProtoNAM), which introduces prototypes into
neural networks in the framework of GAMs. With the introduced prototype-based
feature activation, ProtoNAM can flexibly model the irregular mapping from
tabular features to the outputs while maintaining the explainability of the
final prediction. We also propose a gradient-boosting inspired hierarchical
shape function modeling method, facilitating the discovery of complex feature
patterns and bringing transparency into the learning process of each network
layer. Our empirical evaluations demonstrate that ProtoNAM outperforms all
existing NN-based GAMs, while providing additional insights into the shape
function learned for each feature. The source code of ProtoNAM is available at
\url{https://github.com/Teddy-XiongGZ/ProtoNAM}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction
  Diversity on Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Zhang, Justin Wang, Francois Charton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and accurately following instructions is critical for large
language models (LLMs) to be effective across diverse tasks. In this work, we
rigorously examine the key factors that enable models to generalize to unseen
instructions, providing insights to guide the collection of data for
instruction-tuning. Through controlled experiments, inspired by the
Turing-complete Markov algorithm, we demonstrate that such generalization
$\textbf{only emerges}$ when training data is diversified enough across
semantic domains. Our findings also reveal that merely diversifying within
limited domains fails to ensure robust generalization. In contrast,
cross-domain data diversification, even under constrained data budgets,
significantly enhances a model's adaptability. We further extend our analysis
to real-world scenarios, including fine-tuning of
$\textit{$\textbf{specialist}$}$ and $\textit{$\textbf{generalist}$}$ models.
In both cases, we demonstrate that 1) better performance can be achieved by
increasing the diversity of an established dataset while keeping the data size
constant, and 2) when scaling up the data, diversifying the semantics of
instructions is more effective than simply increasing the quantity of similar
data. Our research provides important insights for dataset collation,
particularly when optimizing model performance by expanding training data for
both specialist and generalist scenarios. We show that careful consideration of
data diversification is key: training specialist models with data extending
beyond their core domain leads to significant performance improvements, while
generalist models benefit from diverse data mixtures that enhance their overall
instruction-following capabilities across a wide range of applications. Our
results highlight the critical role of strategic diversification and offer
clear guidelines for improving data quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rule-based Data Selection for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaomin Li, Mingye Gao, Zhiwei Zhang, Chang Yue, Hong Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quality of training data significantly impacts the performance of large
language models (LLMs). There are increasing studies using LLMs to rate and
select data based on several human-crafted metrics (rules). However, these
conventional rule-based approaches often depend too heavily on human
heuristics, lack effective metrics for assessing rules, and exhibit limited
adaptability to new tasks. In our study, we introduce an innovative rule-based
framework that utilizes the orthogonality of score vectors associated with
rules as a novel metric for rule evaluations. Our approach includes an
automated pipeline that first uses LLMs to generate a diverse set of rules,
encompassing various rating dimensions to evaluate data quality. Then it rates
a batch of data based on these rules and uses the determinantal point process
(DPP) from random matrix theory to select the most orthogonal score vectors,
thereby identifying a set of independent rules. These rules are subsequently
used to evaluate all data, selecting samples with the highest average scores
for downstream tasks such as LLM training. We verify the effectiveness of our
method through two experimental setups: 1) comparisons with ground truth
ratings and 2) benchmarking LLMs trained with the chosen data. Our
comprehensive experiments cover a range of scenarios, including general
pre-training and domain-specific fine-tuning in areas such as IMDB, Medical,
Math, and Code. The outcomes demonstrate that our DPP-based rule rating method
consistently outperforms other approaches, including rule-free rating, uniform
sampling, importance resampling, and QuRating, in terms of both rating
precision and model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tight Stability, Convergence, and Robustness Bounds for Predictive
  Coding Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankur Mali, Tommaso Salvatori, Alexander Ororbia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Energy-based learning algorithms, such as predictive coding (PC), have
garnered significant attention in the machine learning community due to their
theoretical properties, such as local operations and biologically plausible
mechanisms for error correction. In this work, we rigorously analyze the
stability, robustness, and convergence of PC through the lens of dynamical
systems theory. We show that, first, PC is Lyapunov stable under mild
assumptions on its loss and residual energy functions, which implies intrinsic
robustness to small random perturbations due to its well-defined
energy-minimizing dynamics. Second, we formally establish that the PC updates
approximate quasi-Newton methods by incorporating higher-order curvature
information, which makes them more stable and able to converge with fewer
iterations compared to models trained via backpropagation (BP). Furthermore,
using this dynamical framework, we provide new theoretical bounds on the
similarity between PC and other algorithms, i.e., BP and target propagation
(TP), by precisely characterizing the role of higher-order derivatives. These
bounds, derived through detailed analysis of the Hessian structures, show that
PC is significantly closer to quasi-Newton updates than TP, providing a deeper
understanding of the stability and efficiency of PC compared to conventional
learning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 9 theorems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning How Hard to Think: Input-Adaptive Allocation of LM Computation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehul Damani, Idan Shenfeld, Andi Peng, Andreea Bobu, Jacob Andreas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computationally intensive decoding procedures--including search, reranking,
and self-critique--can improve the quality of language model (LM) outputs in
problems spanning code generation, numerical reasoning, and dialog. Existing
work typically applies the same decoding procedure for every input to an LM.
But not all inputs require the same amount of computation to process. Can we
allocate decoding computation adaptively, using more resources to answer
questions whose answers will be harder to compute? We present an approach that
predicts the distribution of rewards given an input and computation budget,
then allocates additional computation to inputs for which it is predicted to be
most useful. We apply this approach in two decoding procedures: first, an
adaptive best-of-k procedure that dynamically selects the number of samples to
generate as input to a reranker; second, a routing procedure that dynamically
responds to a query using a decoding procedure that is expensive but accurate,
or one that is cheaper but less capable. Across a suite of programming,
mathematics, and dialog tasks, we show that accurate computation-allocation
procedures can be learned, and reduce computation by up to 50% at no cost to
response quality, or improve quality by up to 10% at a fixed computational
budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Measuring Goal-Directedness in AI Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Xu, Juan-Pablo Rivera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep learning have brought attention to the possibility of
creating advanced, general AI systems that outperform humans across many tasks.
However, if these systems pursue unintended goals, there could be catastrophic
consequences. A key prerequisite for AI systems pursuing unintended goals is
whether they will behave in a coherent and goal-directed manner in the first
place, optimizing for some unknown goal; there exists significant research
trying to evaluate systems for said behaviors. However, the most rigorous
definitions of goal-directedness we currently have are difficult to compute in
real-world settings. Drawing upon this previous literature, we explore policy
goal-directedness within reinforcement learning (RL) environments. In our
findings, we propose a different family of definitions of the goal-directedness
of a policy that analyze whether it is well-modeled as near-optimal for many
(sparse) reward functions. We operationalize this preliminary definition of
goal-directedness and test it in toy Markov decision process (MDP)
environments. Furthermore, we explore how goal-directedness could be measured
in frontier large-language models (LLMs). Our contribution is a definition of
goal-directedness that is simpler and more easily computable in order to
approach the question of whether AI systems could pursue dangerous goals. We
recommend further exploration of measuring coherence and goal-directedness,
based on our findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ mDPO: Conditional Preference Optimization for Multimodal Large Language
  Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Wang, Wenxuan Zhou, James Y. Huang, Nan Xu, Sheng Zhang, Hoifung Poon, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct preference optimization (DPO) has shown to be an effective method for
large language model (LLM) alignment. Recent works have attempted to apply DPO
to multimodal scenarios but have found it challenging to achieve consistent
improvement. Through a comparative experiment, we identify the unconditional
preference problem in multimodal preference optimization, where the model
overlooks the image condition. To address this problem, we propose mDPO, a
multimodal DPO objective that prevents the over-prioritization of language-only
preferences by also optimizing image preference. Moreover, we introduce a
reward anchor that forces the reward to be positive for chosen responses,
thereby avoiding the decrease in their likelihood -- an intrinsic problem of
relative preference optimization. Experiments on two multimodal LLMs of
different sizes and three widely used benchmarks demonstrate that mDPO
effectively addresses the unconditional preference problem in multimodal
preference optimization and significantly improves model performance,
particularly in reducing hallucination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference. Project website:
  https://feiwang96.github.io/mDPO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Successor Features with Distributed Hebbian Temporal Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13391v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13391v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evgenii Dzhivelikian, Petr Kuderov, Aleksandr I. Panov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to address the challenge of online
temporal memory learning for decision-making under uncertainty in
non-stationary, partially observable environments. The proposed algorithm,
Distributed Hebbian Temporal Memory (DHTM), is based on factor graph formalism
and a multicomponent neuron model. DHTM aims to capture sequential data
relationships and make cumulative predictions about future observations,
forming Successor Features (SF). Inspired by neurophysiological models of the
neocortex, the algorithm utilizes distributed representations, sparse
transition matrices, and local Hebbian-like learning rules to overcome the
instability and slow learning process of traditional temporal memory algorithms
like RNN and HMM. Experimental results demonstrate that DHTM outperforms LSTM
and a biologically inspired HMM-like algorithm, CSCG, in the case of
non-stationary datasets. Our findings suggest that DHTM is a promising approach
for addressing the challenges of online sequence learning and planning in
dynamic environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BigCodeBench: Benchmarking Code Generation with Diverse Function Calls
  and Complex Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15877v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15877v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, David Lo, Binyuan Hui, Niklas Muennighoff, Daniel Fried, Xiaoning Du, Harm de Vries, Leandro Von Werra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task automation has been greatly empowered by the recent advances in Large
Language Models (LLMs) via Python code, where the tasks ranging from software
engineering development to general-purpose reasoning. While current benchmarks
have shown that LLMs can solve tasks using programs like human developers, the
majority of their evaluations are limited to short and self-contained
algorithmic tasks or standalone function calls. Solving challenging and
practical requires the capability of utilizing diverse function calls as tools
to efficiently implement functionalities like data analysis and web
development. In addition, using multiple tools to solve a task needs
compositional reasoning by accurately understanding complex instructions.
Fulfilling both of these characteristics can pose a great challenge for LLMs.To
assess how well LLMs can solve challenging and practical tasks via programs, we
introduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple
function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained
tasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with
an average branch coverage of 99%. In addition, we propose a
natural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that
automatically transforms the original docstrings into short instructions only
with essential information. Our extensive evaluation of 60 LLMs shows that LLMs
are not yet capable of following complex instructions to use function calls
precisely, with scores up to 60%, significantly lower than the human
performance of 97%. The results underscore the need for further advancements in
this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 14 figures, 7 tables, built with love by the BigCode
  community :)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question
  Answering (Published in Findings of EMNLP 2024) <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02233v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02233v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Wang, Xueguang Ma, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale language models (LLMs) like ChatGPT have demonstrated impressive
abilities in generating responses based on human instructions. However, their
use in the medical field can be challenging due to their lack of specific,
in-depth knowledge. In this study, we present a system called LLMs Augmented
with Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in
specialized domains. LLM-AMT integrates authoritative medical textbooks into
the LLMs' framework using plug-and-play modules. These modules include a Query
Augmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together,
they incorporate authoritative medical knowledge. Additionally, an LLM Reader
aids in contextual understanding. Our experimental results on three medical QA
tasks demonstrate that LLMAMT significantly improves response quality, with
accuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as the
base model, LLM-AMT outperforms the specialized Med-PaLM 2 model pre-trained on
a massive amount of medical corpus by 2-3%. We found that despite being 100x
smaller in size, medical textbooks as a retrieval corpus is proven to be a more
effective knowledge database than Wikipedia in the medical domain, boosting
performance by 7.8%-13.7%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has been accepted and published at EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAnDOIT: Causal Discovery with Observational and Interventional Data
  from Time-Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Castri, Sariah Mghames, Marc Hanheide, Nicola Bellotto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of cause-and-effect is of the utmost importance in many branches of
science, but also for many practical applications of intelligent systems. In
particular, identifying causal relationships in situations that include hidden
factors is a major challenge for methods that rely solely on observational data
for building causal models. This paper proposes CAnDOIT, a causal discovery
method to reconstruct causal models using both observational and interventional
time-series data. The use of interventional data in the causal analysis is
crucial for real-world applications, such as robotics, where the scenario is
highly complex and observational data alone are often insufficient to uncover
the correct causal structure. Validation of the method is performed initially
on randomly generated synthetic models and subsequently on a well-known
benchmark for causal structure learning in a robotic manipulation environment.
The experiments demonstrate that the approach can effectively handle data from
interventions and exploit them to enhance the accuracy of the causal analysis.
A Python implementation of CAnDOIT has also been developed and is publicly
available on GitHub: https://github.com/lcastri/causalflow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Advanced Intelligent Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Document Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John X. Morris, Alexander M. Rush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense document embeddings are central to neural retrieval. The dominant
paradigm is to train and construct embeddings by running encoders directly on
individual documents. In this work, we argue that these embeddings, while
effective, are implicitly out-of-context for targeted use cases of retrieval,
and that a contextualized document embedding should take into account both the
document and neighboring documents in context - analogous to contextualized
word embeddings. We propose two complementary methods for contextualized
document embeddings: first, an alternative contrastive learning objective that
explicitly incorporates the document neighbors into the intra-batch contextual
loss; second, a new contextual architecture that explicitly encodes neighbor
document information into the encoded representation. Results show that both
methods achieve better performance than biencoders in several settings, with
differences especially pronounced out-of-domain. We achieve state-of-the-art
results on the MTEB benchmark with no hard negative mining, score distillation,
dataset-specific instructions, intra-GPU example-sharing, or extremely large
batch sizes. Our method can be applied to improve performance on any
contrastive learning dataset and any biencoder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Creative Beam Search: LLM-as-a-Judge For Improving Response Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00099v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00099v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Franceschelli, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are revolutionizing several areas, including artificial
creativity. However, the process of generation in machines profoundly diverges
from that observed in humans. In particular, machine generation is
characterized by a lack of intentionality and an underlying creative process.
We propose a method called Creative Beam Search that uses Diverse Beam Search
and LLM-as-a-Judge to perform response generation and response validation. The
results of a qualitative experiment show how our approach can provide better
output than standard sampling techniques. We also show that the response
validation step is a necessary complement to the response generation step.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented as a short paper at the 15th International Conference on
  Computational Creativity (ICCC'24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Foundation Models as Data Compression: On Information, Model
  Weights and Copyright Law 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13493v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13493v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Franceschelli, Claudia Cevenini, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The training process of foundation models as for other classes of deep
learning systems is based on minimizing the reconstruction error over a
training set. For this reason, they are susceptible to the memorization and
subsequent reproduction of training samples. In this paper, we introduce a
training-as-compressing perspective, wherein the model's weights embody a
compressed representation of the training data. From a copyright standpoint,
this point of view implies that the weights could be considered a reproduction
or a derivative work of a potentially protected set of works. We investigate
the technical and legal challenges that emerge from this framing of the
copyright of outputs generated by foundation models, including their
implications for practitioners and researchers. We demonstrate that adopting an
information-centric approach to the problem presents a promising pathway for
tackling these emerging complex legal issues.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Spotlight presentation at GenLaw'24, see
  https://www.genlaw.org/2024-icml-papers#training-foundation-models-as-data-compression-on-information-model-weights-and-copyright-law</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaMetrics: Calibrating Metrics For Generation Tasks Using Human
  Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genta Indra Winata, David Anugraha, Lucky Susanto, Garry Kuwanto, Derry Tanti Wijaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the quality of a performance evaluation metric is crucial for
ensuring that model outputs align with human preferences. However, it remains
unclear how well each metric captures the diverse aspects of these preferences,
as metrics often excel in one particular area but not across all dimensions. To
address this, it is essential to systematically calibrate metrics to specific
aspects of human preference, catering to the unique characteristics of each
aspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate
generation tasks across different modalities in a supervised manner.
MetaMetrics optimizes the combination of existing metrics to enhance their
alignment with human preferences. Our metric demonstrates flexibility and
effectiveness in both language and vision downstream tasks, showing significant
benefits across various multilingual and multi-domain scenarios. MetaMetrics
aligns closely with human preferences and is highly extendable and easily
integrable into any application. This makes MetaMetrics a powerful tool for
improving the evaluation of generation tasks, ensuring that metrics are more
representative of human judgment across diverse contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Usage-centric Take on Intent Understanding in E-Commerce <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wendi Zhou, Tianyi Li, Pavlos Vougiouklis, Mark Steedman, Jeff Z. Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying and understanding user intents is a pivotal task for E-Commerce.
Despite its essential role in product recommendation and business user
profiling analysis, intent understanding has not been consistently defined or
accurately benchmarked. In this paper, we focus on predicative user intents as
"how a customer uses a product", and pose intent understanding as a natural
language reasoning task, independent of product ontologies. We identify two
weaknesses of FolkScope, the SOTA E-Commerce Intent Knowledge Graph:
category-rigidity and property-ambiguity. They limit its ability to strongly
align user intents with products having the most desirable property, and to
recommend useful products across diverse categories. Following these
observations, we introduce a Product Recovery Benchmark featuring a novel
evaluation framework and an example dataset. We further validate the above
FolkScope weaknesses on this benchmark. Our code and dataset are available at
https://github.com/stayones/Usgae-Centric-Intent-Understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Acepted by EMNLP 2024 main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02151v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02151v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that even the most recent safety-aligned LLMs are not robust to
simple adaptive jailbreaking attacks. First, we demonstrate how to successfully
leverage access to logprobs for jailbreaking: we initially design an
adversarial prompt template (sometimes adapted to the target LLM), and then we
apply random search on a suffix to maximize a target logprob (e.g., of the
token "Sure"), potentially with multiple restarts. In this way, we achieve 100%
attack success rate -- according to GPT-4 as a judge -- on Vicuna-13B,
Mistral-7B, Phi-3-Mini, Nemotron-4-340B, Llama-2-Chat-7B/13B/70B,
Llama-3-Instruct-8B, Gemma-7B, GPT-3.5, GPT-4o, and R2D2 from HarmBench that
was adversarially trained against the GCG attack. We also show how to jailbreak
all Claude models -- that do not expose logprobs -- via either a transfer or
prefilling attack with a 100% success rate. In addition, we show how to use
random search on a restricted set of tokens for finding trojan strings in
poisoned models -- a task that shares many similarities with jailbreaking --
which is the algorithm that brought us the first place in the SaTML'24 Trojan
Detection Competition. The common theme behind these attacks is that adaptivity
is crucial: different models are vulnerable to different prompting templates
(e.g., R2D2 is very sensitive to in-context learning prompts), some models have
unique vulnerabilities based on their APIs (e.g., prefilling for Claude), and
in some settings, it is crucial to restrict the token search space based on
prior knowledge (e.g., for trojan detection). For reproducibility purposes, we
provide the code, logs, and jailbreak artifacts in the JailbreakBench format at
https://github.com/tml-epfl/llm-adaptive-attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updates in the v3: GPT-4o and Claude 3.5 Sonnet results, improved
  writing. Updates in the v2: more models (Llama3, Phi-3, Nemotron-4-340B),
  jailbreak artifacts for all attacks are available, evaluation with different
  judges (Llama-3-70B and Llama Guard 2), more experiments (convergence plots
  over iterations, ablation on the suffix length for random search), examples
  of jailbroken generation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Model-Agnostic Multi-Group Equivariant Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09675v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09675v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Razan Baltaji, Sourya Basu, Lav R. Varshney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constructing model-agnostic group equivariant networks, such as equitune
(Basu et al., 2023b) and its generalizations (Kim et al., 2023), can be
computationally expensive for large product groups. We address this problem by
providing efficient model-agnostic equivariant designs for two related
problems: one where the network has multiple inputs each with potentially
different groups acting on them, and another where there is a single input but
the group acting on it is a large product group. For the first design, we
initially consider a linear model and characterize the entire equivariant space
that satisfies this constraint. This characterization gives rise to a novel
fusion layer between different channels that satisfies an invariance-symmetry
(IS) constraint, which we call an IS layer. We then extend this design beyond
linear models, similar to equitune, consisting of equivariant and IS layers. We
also show that the IS layer is a universal approximator of invariant-symmetric
functions. Inspired by the first design, we use the notion of the IS property
to design a second efficient model-agnostic equivariant design for large
product groups acting on a single input. For the first design, we provide
experiments on multi-image classification where each view is transformed
independently with transformations such as rotations. We find equivariant
models are robust to such transformations and perform competitively otherwise.
For the second design, we consider three applications: language
compositionality on the SCAN dataset to product groups; fairness in natural
language generation from GPT-2 to address intersectionality; and robust
zero-shot image classification with CLIP. Overall, our methods are simple and
general, competitive with equitune and its variants, while also being
computationally more efficient.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When "A Helpful Assistant" Is Not Really Helpful: Personas in System
  <span class="highlight-title">Prompt</span>s Do Not Improve Performances of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10054v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10054v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingqian Zheng, Jiaxin Pei, Lajanugen Logeswaran, Moontae Lee, David Jurgens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompting serves as the major way humans interact with Large Language Models
(LLM). Commercial AI systems commonly define the role of the LLM in system
prompts. For example, ChatGPT uses "You are a helpful assistant" as part of its
default system prompt. Despite current practices of adding personas to system
prompts, it remains unclear how different personas affect a model's performance
on objective tasks. In this study, we present a systematic evaluation of
personas in system prompts. We curate a list of 162 roles covering 6 types of
interpersonal relationships and 8 domains of expertise. Through extensive
analysis of 4 popular families of LLMs and 2,410 factual questions, we
demonstrate that adding personas in system prompts does not improve model
performance across a range of questions compared to the control setting where
no persona is added. Nevertheless, further analysis suggests that the gender,
type, and domain of the persona can all influence the resulting prediction
accuracies. We further experimented with a list of persona search strategies
and found that, while aggregating results from the best persona for each
question significantly improves prediction accuracy, automatically identifying
the best persona is challenging, with predictions often performing no better
than random selection. Overall, our findings suggest that while adding a
persona may lead to performance gains in certain settings, the effect of each
persona can be largely random. Code and data are available at
https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Steer Markovian Agents under Model Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10207v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10207v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Huang, Vinzenz Thoma, Zebang Shen, Heinrich H. Nax, Niao He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing incentives for an adapting population is a ubiquitous problem in a
wide array of economic applications and beyond. In this work, we study how to
design additional rewards to steer multi-agent systems towards desired policies
\emph{without} prior knowledge of the agents' underlying learning dynamics.
Motivated by the limitation of existing works, we consider a new and general
category of learning dynamics called \emph{Markovian agents}. We introduce a
model-based non-episodic Reinforcement Learning (RL) formulation for our
steering problem. Importantly, we focus on learning a \emph{history-dependent}
steering strategy to handle the inherent model uncertainty about the agents'
learning dynamics. We introduce a novel objective function to encode the
desiderata of achieving a good steering outcome with reasonable cost.
Theoretically, we identify conditions for the existence of steering strategies
to guide agents to the desired policies. Complementing our theoretical
contributions, we provide empirical algorithms to approximately solve our
objective, which effectively tackles the challenge in learning
history-dependent strategies. We demonstrate the efficacy of our algorithms
through empirical evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Better Instruction-Following Through Minimum Bayes Risk 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02902v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02902v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Wu, Patrick Fernandes, Amanda Bertsch, Seungone Kim, Sina Pakazad, Graham Neubig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General-purpose LLM judges capable of human-level evaluation provide not only
a scalable and accurate way of evaluating instruction-following LLMs but also
new avenues for supervising and improving their performance. One promising way
of leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR)
decoding, which uses a reference-based evaluator to select a high-quality
output from amongst a set of candidate outputs. In the first part of this work,
we explore using MBR decoding as a method for improving the test-time
performance of instruction-following LLMs. We find that MBR decoding with
reference-based LLM judges substantially improves over greedy decoding,
best-of-N decoding with reference-free judges and MBR decoding with lexical and
embedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent
across LLMs with up to 70B parameters, demonstrating that smaller LLM judges
can be used to supervise much larger LLMs. Then, seeking to retain the
improvements from MBR decoding while mitigating additional test-time costs, we
explore iterative self-training on MBR-decoded outputs. We find that
self-training using Direct Preference Optimisation leads to significant
performance gains, such that the self-trained models with greedy decoding
generally match and sometimes exceed the performance of their base models with
MBR decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Fusion: Capturing Dependencies in Contrastive Learning via
  <span class="highlight-title">Transformer</span> Projection Heads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18681v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18681v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Learning (CL) has emerged as a powerful method for training
feature extraction models using unlabeled data. Recent studies suggest that
incorporating a linear projection head post-backbone significantly enhances
model performance. In this work, we investigate the use of a transformer model
as a projection head within the CL framework, aiming to exploit the
transformer's capacity for capturing long-range dependencies across embeddings
to further improve performance. Our key contributions are fourfold: First, we
introduce a novel application of transformers in the projection head role for
contrastive learning, marking the first endeavor of its kind. Second, our
experiments reveal a compelling "Deep Fusion" phenomenon where the attention
mechanism progressively captures the correct relational dependencies among
samples from the same class in deeper layers. Third, we provide a theoretical
framework that explains and supports this "Deep Fusion" behavior. Finally, we
demonstrate through experimental results that our model achieves superior
performance compared to the existing approach of using a feed-forward layer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preventing Collapse in Contrastive Learning with Orthonormal Prototypes
  (CLOP) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18699v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18699v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Manh Nguyen, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has emerged as a powerful method in deep learning,
excelling at learning effective representations through contrasting samples
from different distributions. However, neural collapse, where embeddings
converge into a lower-dimensional space, poses a significant challenge,
especially in semi-supervised and self-supervised setups. In this paper, we
first theoretically analyze the effect of large learning rates on contrastive
losses that solely rely on the cosine similarity metric, and derive a
theoretical bound to mitigate this collapse. {Building on these insights, we
propose CLOP, a novel semi-supervised loss function designed to prevent neural
collapse by promoting the formation of orthogonal linear subspaces among class
embeddings.} Unlike prior approaches that enforce a simplex ETF structure, CLOP
focuses on subspace separation, leading to more distinguishable embeddings.
Through extensive experiments on real and synthetic datasets, we demonstrate
that CLOP enhances performance, providing greater stability across different
learning rates and batch sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-Tuning and <span class="highlight-title">Prompt</span> Optimization: Two Great Steps that Work Better
  Together <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dilara Soylu, Christopher Potts, Omar Khattab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) systems are increasingly taking the form of
sophisticated modular pipelines, e.g., Retrieval Augmented Generation (RAG),
where each module may involve a distinct Language Model (LM) and an associated
prompt template. These compound systems often lack intermediate labels or
gradient flow to optimize each module, making their end-to-end optimization
challenging. Here we seek strategies to optimize both the module-level LM
weights and the associated prompt templates of such systems to maximize a
downstream task metric. We propose for the first time combining the weight and
prompt optimization strategies to optimize a modular LM pipeline by alternating
between the two to get the same LM to teach itself. In experiments with
multi-hop QA, mathematical reasoning, and feature-based classification using
mistral-7b, llama-2-7b, and llama-3-8b, these BetterTogether strategies
optimizing the weights and prompts of a pipeline together outperform directly
optimizing weights alone and prompts alone by up to 60% and 6%, respectively,
on average across LMs and tasks. BetterTogether optimizer is released in DSPy
at http://dspy.ai
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Memory-Enhanced Neural Solvers for Efficient Adaptation in Combinatorial
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16424v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16424v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Chalumeau, Refiloe Shabe, Noah De Nicola, Arnu Pretorius, Thomas D. Barrett, Nathan Grinsztajn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combinatorial Optimization is crucial to numerous real-world applications,
yet still presents challenges due to its (NP-)hard nature. Amongst existing
approaches, heuristics often offer the best trade-off between quality and
scalability, making them suitable for industrial use. While Reinforcement
Learning (RL) offers a flexible framework for designing heuristics, its
adoption over handcrafted heuristics remains incomplete within industrial
solvers. Existing learned methods still lack the ability to adapt to specific
instances and fully leverage the available computational budget. The current
best methods either rely on a collection of pre-trained policies, or on
data-inefficient fine-tuning; hence failing to fully utilize newly available
information within the constraints of the budget. In response, we present
MEMENTO, an approach that leverages memory to improve the adaptation of neural
solvers at inference time. MEMENTO enables updating the action distribution
dynamically based on the outcome of previous decisions. We validate its
effectiveness on benchmark problems, in particular Traveling Salesman and
Capacitated Vehicle Routing, demonstrating its superiority over tree-search and
policy-gradient fine-tuning; and showing it can be zero-shot combined with
diversity-based solvers. We successfully train all RL auto-regressive solvers
on large instances, and show that MEMENTO can scale and is data-efficient.
Overall, MEMENTO enables to push the state-of-the-art on 11 out of 12 evaluated
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LayerKV: Optimizing Large Language Model Serving with Layer-wise KV
  Cache Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Xiong, Hao Wu, Changxu Shao, Ziqing Wang, Rui Zhang, Yuhong Guo, Junping Zhao, Ke Zhang, Zhenxuan Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The expanding context windows in large language models (LLMs) have greatly
enhanced their capabilities in various applications, but they also introduce
significant challenges in maintaining low latency, particularly in Time to
First Token (TTFT). This paper identifies that the sharp rise in TTFT as
context length increases is predominantly driven by queuing delays, which are
caused by the growing demands for GPU Key-Value (KV) cache allocation clashing
with the limited availability of KV cache blocks. To address this issue, we
propose LayerKV, a simple yet effective plug-in method that effectively reduces
TTFT without requiring additional hardware or compromising output performance,
while seamlessly integrating with existing parallelism strategies and
scheduling techniques. Specifically, LayerKV introduces layer-wise KV block
allocation, management, and offloading for fine-grained control over system
memory, coupled with an SLO-aware scheduler to optimize overall Service Level
Objectives (SLOs). Comprehensive evaluations on representative models, ranging
from 7B to 70B parameters, across various GPU configurations, demonstrate that
LayerKV improves TTFT latency up to 69x and reduces SLO violation rates by
28.7%, significantly enhancing the user experience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autoregressive Image Diffusion: Generation of Image Sequence and
  Application in MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14327v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14327v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanxiong Luo, Shoujin Huang, Martin Uecker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic resonance imaging (MRI) is a widely used non-invasive imaging
modality. However, a persistent challenge lies in balancing image quality with
imaging speed. This trade-off is primarily constrained by k-space measurements,
which traverse specific trajectories in the spatial Fourier domain (k-space).
These measurements are often undersampled to shorten acquisition times,
resulting in image artifacts and compromised quality. Generative models learn
image distributions and can be used to reconstruct high-quality images from
undersampled k-space data. In this work, we present the autoregressive image
diffusion (AID) model for image sequences and use it to sample the posterior
for accelerated MRI reconstruction. The algorithm incorporates both
undersampled k-space and pre-existing information. Models trained with fastMRI
dataset are evaluated comprehensively. The results show that the AID model can
robustly generate sequentially coherent image sequences. In MRI applications,
the AID can outperform the standard diffusion model and reduce hallucinations,
due to the learned inter-image dependencies. The project code is available at
https://github.com/mrirecon/aid.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoding Intelligence: A Framework for Certifying Knowledge
  Comprehension in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isha Chaudhary, Vedaant V. Jain, Gagandeep Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge comprehension capability is an important aspect of human
intelligence. As Large Language Models (LLMs) are being envisioned as
superhuman agents, it is crucial for them to be proficient at knowledge
comprehension. However, existing benchmarking studies do not provide
consistent, generalizable, and formal guarantees on the knowledge comprehension
capabilities of LLMs. In this work, we propose the first framework to certify
knowledge comprehension in LLMs with formal probabilistic guarantees. Our
certificates are quantitative -- they consist of high-confidence, tight bounds
on the probability that a target LLM gives the correct answer on any knowledge
comprehension prompt sampled from a distribution. We design and certify novel
specifications that precisely represent distributions of knowledge
comprehension prompts leveraging knowledge graphs. We certify SOTA LLMs for
specifications over the Wikidata5m knowledge graph. We find that the knowledge
comprehension capability improves significantly with scaling the size of the
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs Know More Than They Show: On the Intrinsic Representation of LLM
  Hallucinations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02707v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02707v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often produce errors, including factual
inaccuracies, biases, and reasoning failures, collectively referred to as
"hallucinations". Recent studies have demonstrated that LLMs' internal states
encode information regarding the truthfulness of their outputs, and that this
information can be utilized to detect errors. In this work, we show that the
internal representations of LLMs encode much more information about
truthfulness than previously recognized. We first discover that the
truthfulness information is concentrated in specific tokens, and leveraging
this property significantly enhances error detection performance. Yet, we show
that such error detectors fail to generalize across datasets, implying that --
contrary to prior claims -- truthfulness encoding is not universal but rather
multifaceted. Next, we show that internal representations can also be used for
predicting the types of errors the model is likely to make, facilitating the
development of tailored mitigation strategies. Lastly, we reveal a discrepancy
between LLMs' internal encoding and external behavior: they may encode the
correct answer, yet consistently generate an incorrect one. Taken together,
these insights deepen our understanding of LLM errors from the model's internal
perspective, which can guide future research on enhancing error analysis and
mitigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of
  Large Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14768v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14768v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) need knowledge updates to meet the ever-growing
world facts and correct the hallucinated responses, facilitating the methods of
lifelong model editing. Where the updated knowledge resides in memories is a
fundamental question for model editing. In this paper, we find that editing
either long-term memory (direct model parameters) or working memory
(non-parametric knowledge of neural network activations/representations by
retrieval) will result in an impossible triangle -- reliability,
generalization, and locality can not be realized together in the lifelong
editing settings. For long-term memory, directly editing the parameters will
cause conflicts with irrelevant pretrained knowledge or previous edits (poor
reliability and locality). For working memory, retrieval-based activations can
hardly make the model understand the edits and generalize (poor
generalization). Therefore, we propose WISE to bridge the gap between memories.
In WISE, we design a dual parametric memory scheme, which consists of the main
memory for the pretrained knowledge and a side memory for the edited knowledge.
We only edit the knowledge in the side memory and train a router to decide
which memory to go through when given a query. For continual editing, we devise
a knowledge-sharding mechanism where different sets of edits reside in distinct
subspaces of parameters, and are subsequently merged into a shared memory
without conflicts. Extensive experiments show that WISE can outperform previous
model editing methods and overcome the impossible triangle under lifelong model
editing of question answering, hallucination, and out-of-distribution settings
across trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is
available at https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Embedding Dynamic Personas in Interactive Robots: Masquerading
  Animated Social Kinematics (MASK) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10041v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10041v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongeun Park, Taemoon Jeong, Hyeonseong Kim, Taehyun Byun, Seungyoon Shin, Keunjun Choi, Jaewoon Kwon, Taeyoon Lee, Matthew Pan, Sungjoon Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the design and development of an innovative interactive
robotic system to enhance audience engagement using character-like personas.
Built upon the foundations of persona-driven dialog agents, this work extends
the agent's application to the physical realm, employing robots to provide a
more captivating and interactive experience. The proposed system, named the
Masquerading Animated Social Kinematic (MASK), leverages an anthropomorphic
robot which interacts with guests using non-verbal interactions, including
facial expressions and gestures. A behavior generation system based upon a
finite-state machine structure effectively conditions robotic behavior to
convey distinct personas. The MASK framework integrates a perception engine, a
behavior selection engine, and a comprehensive action library to enable
real-time, dynamic interactions with minimal human intervention in behavior
design. Throughout the user subject studies, we examined whether the users
could recognize the intended character in both personality- and
film-character-based persona conditions. We conclude by discussing the role of
personas in interactive agents and the factors to consider for creating an
engaging user experience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Robotics and Automation Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Centric Foundation Models in Computational Healthcare: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02458v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02458v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunkun Zhang, Jin Gao, Zheling Tan, Lingfeng Zhou, Kexin Ding, Mu Zhou, Shaoting Zhang, Dequan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of foundation models (FMs) as an emerging suite of AI techniques
has struck a wave of opportunities in computational healthcare. The interactive
nature of these models, guided by pre-training data and human instructions, has
ignited a data-centric AI paradigm that emphasizes better data
characterization, quality, and scale. In healthcare AI, obtaining and
processing high-quality clinical data records has been a longstanding
challenge, ranging from data quantity, annotation, patient privacy, and ethics.
In this survey, we investigate a wide range of data-centric approaches in the
FM era (from model pre-training to inference) towards improving the healthcare
workflow. We discuss key perspectives in AI security, assessment, and alignment
with human values. Finally, we offer a promising outlook of FM-based analytics
to enhance the performance of patient outcome and clinical workflow in the
evolving landscape of healthcare and medicine. We provide an up-to-date list of
healthcare-related foundation models and datasets at
https://github.com/Yunkun-Zhang/Data-Centric-FM-Healthcare .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Survey content updated to include recent research work and progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autonomous Evaluation and Refinement of Digital Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06474v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06474v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, Alane Suhr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that domain-general automatic evaluators can significantly improve
the performance of agents for web navigation and device control. We experiment
with multiple evaluation models that trade off between inference cost,
modularity of design, and accuracy. We validate the performance of these models
in several popular benchmarks for digital agents, finding between 74.4 and
92.9% agreement with oracle evaluation metrics. Finally, we use these
evaluators to improve the performance of existing agents via fine-tuning and
inference-time guidance. Without any additional supervision, we improve
state-of-the-art performance by 29% on the popular benchmark WebArena, and
achieve around 75% relative improvement in device control settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at COLM 2024. Code at
  https://github.com/Berkeley-NLP/Agent-Eval-Refine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances
  Retrieval-Augmented Generation with Zero Inference Overhead 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19745v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19745v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Tan, Yining Qian, Ang Lv, Hongzhan Lin, Songhao Wu, Yongbo Wang, Feng Wang, Jingtong Wu, Xin Lu, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) enhanced with retrieval-augmented generation
(RAG) have introduced a new paradigm for web search. However, the limited
context awareness of LLMs degrades their performance on RAG tasks. Existing
methods to enhance context awareness are often inefficient, incurring time or
memory overhead during inference, and many are tailored to specific position
embeddings. In this paper, we propose Position-Embedding-Agnostic attention
Re-weighting (PEAR), which enhances the context awareness of LLMs with zero
inference overhead. Specifically, on a proxy task focused on context copying,
we first detect heads which suppress the models' context awareness thereby
diminishing RAG performance. To weaken the impact of these heads, we re-weight
their outputs with learnable coefficients. The LLM (with frozen parameters) is
optimized by adjusting these coefficients to minimize loss on the proxy task.
As a result, the coefficients are optimized to values less than one, thereby
reducing their tendency to suppress RAG performance. During inference, the
optimized coefficients are fixed to re-weight these heads, regardless of the
specific task at hand. Our proposed PEAR offers two major advantages over
previous approaches: (1) It introduces zero additional inference overhead in
terms of memory usage or inference time, while outperforming competitive
baselines in accuracy and efficiency across various RAG tasks. (2) It is
independent of position embedding algorithms, ensuring broader applicability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLAME: Adaptive and Reactive Concept Drift Mitigation for Federated
  Learning Deployments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01386v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01386v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioannis Mavromatis, Stefano De Feo, Aftab Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Federated Learning with Adaptive Monitoring and
Elimination (FLAME), a novel solution capable of detecting and mitigating
concept drift in Federated Learning (FL) Internet of Things (IoT) environments.
Concept drift poses significant challenges for FL models deployed in dynamic
and real-world settings. FLAME leverages an FL architecture, considers a
real-world FL pipeline, and proves capable of maintaining model performance and
accuracy while addressing bandwidth and privacy constraints. Introducing
various features and extensions on previous works, FLAME offers a robust
solution to concept drift, significantly reducing computational load and
communication overhead. Compared to well-known lightweight mitigation methods,
FLAME demonstrates superior performance in maintaining high F1 scores and
reducing resource utilisation in large-scale IoT deployments, making it a
promising approach for real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for Publication at ACM EWSN 2024 - EMERGE Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WellDunn: On the Robustness and Explainability of Language Models and
  Large Language Models in Identifying Wellness Dimensions <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12058v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12058v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyedali Mohammadi, Edward Raff, Jinendra Malekar, Vedant Palit, Francis Ferraro, Manas Gaur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Models (LMs) are being proposed for mental health applications where
the heightened risk of adverse outcomes means predictive performance may not be
a sufficient litmus test of a model's utility in clinical practice. A model
that can be trusted for practice should have a correspondence between
explanation and clinical determination, yet no prior research has examined the
attention fidelity of these models and their effect on ground truth
explanations. We introduce an evaluation design that focuses on the robustness
and explainability of LMs in identifying Wellness Dimensions (WDs). We focus on
two existing mental health and well-being datasets: (a) Multi-label
Classification-based MultiWD, and (b) WellXplain for evaluating attention
mechanism veracity against expert-labeled explanations. The labels are based on
Halbert Dunn's theory of wellness, which gives grounding to our evaluation. We
reveal four surprising results about LMs/LLMs: (1) Despite their human-like
capabilities, GPT-3.5/4 lag behind RoBERTa, and MedAlpaca, a fine-tuned LLM on
WellXplain fails to deliver any remarkable improvements in performance or
explanations. (2) Re-examining LMs' predictions based on a confidence-oriented
loss function reveals a significant performance drop. (3) Across all LMs/LLMs,
the alignment between attention and explanations remains low, with LLMs scoring
a dismal 0.0. (4) Most mental health-specific LMs/LLMs overlook domain-specific
knowledge and undervalue explanations, causing these discrepancies. This study
highlights the need for further research into their consistency and
explanations in mental health and well-being.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in BlackboxNLP @ EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MFE-ETP: A Comprehensive Evaluation Benchmark for Multi-modal Foundation
  Models on Embodied Task Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05047v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05047v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Zhang, Xian Fu, Jianye Hao, Peilong Han, Hao Zhang, Lei Shi, Hongyao Tang, Yan Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Multi-modal Foundation Models (MFMs) and Embodied Artificial
Intelligence (EAI) have been advancing side by side at an unprecedented pace.
The integration of the two has garnered significant attention from the AI
research community. In this work, we attempt to provide an in-depth and
comprehensive evaluation of the performance of MFM s on embodied task planning,
aiming to shed light on their capabilities and limitations in this domain. To
this end, based on the characteristics of embodied task planning, we first
develop a systematic evaluation framework, which encapsulates four crucial
capabilities of MFMs: object understanding, spatio-temporal perception, task
understanding, and embodied reasoning. Following this, we propose a new
benchmark, named MFE-ETP, characterized its complex and variable task
scenarios, typical yet diverse task types, task instances of varying
difficulties, and rich test case types ranging from multiple embodied question
answering to embodied task reasoning. Finally, we offer a simple and
easy-to-use automatic evaluation platform that enables the automated testing of
multiple MFMs on the proposed benchmark. Using the benchmark and evaluation
platform, we evaluated several state-of-the-art MFMs and found that they
significantly lag behind human-level performance. The MFE-ETP is a
high-quality, large-scale, and challenging benchmark relevant to real-world
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Can <span class="highlight-title">Transformer</span>s Count to n? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15160v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15160v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gilad Yehudai, Haim Kaplan, Asma Ghandeharioun, Mor Geva, Amir Globerson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models based on the transformer architectures can solve highly
complex tasks. But are there simple tasks that such models cannot solve? Here
we focus on very simple counting tasks, that involve counting how many times a
token in the vocabulary have appeared in a string. We show that if the
dimension of the transformer state is linear in the context length, this task
can be solved. However, the solution we propose does not scale beyond this
limit, and we provide theoretical arguments for why it is likely impossible for
a size limited transformer to implement this task. Our empirical results
demonstrate the same phase-transition in performance, as anticipated by the
theoretical argument. Our results demonstrate the importance of understanding
how transformers can solve simple tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tokenization Is More Than Compression <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18376v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18376v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Craig W. Schmidt, Varshini Reddy, Haoran Zhang, Alec Alameddine, Omri Uzan, Yuval Pinter, Chris Tanner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tokenization is a foundational step in natural language processing (NLP)
tasks, bridging raw text and language models. Existing tokenization approaches
like Byte-Pair Encoding (BPE) originate from the field of data compression, and
it has been suggested that the effectiveness of BPE stems from its ability to
condense text into a relatively small number of tokens. We test the hypothesis
that fewer tokens lead to better downstream performance by introducing
PathPiece, a new tokenizer that segments a document's text into the minimum
number of tokens for a given vocabulary. Through extensive experimentation we
find this hypothesis not to be the case, casting doubt on the understanding of
the reasons for effective tokenization. To examine which other factors play a
role, we evaluate design decisions across all three phases of tokenization:
pre-tokenization, vocabulary construction, and segmentation, offering new
insights into the design of effective tokenizers. Specifically, we illustrate
the importance of pre-tokenization and the benefits of using BPE to initialize
vocabulary construction. We train 64 language models with varying tokenization,
ranging in size from 350M to 2.4B parameters, all of which are made publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ softmax is not enough (for sharp out-of-distribution) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01104v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01104v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Petar Veličković, Christos Perivolaropoulos, Federico Barbero, Razvan Pascanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key property of reasoning systems is the ability to make sharp decisions on
their input data. For contemporary AI systems, a key carrier of sharp behaviour
is the softmax function, with its capability to perform differentiable
query-key lookups. It is a common belief that the predictive power of networks
leveraging softmax arises from "circuits" which sharply perform certain kinds
of computations consistently across many diverse inputs. However, for these
circuits to be robust, they would need to generalise well to arbitrary valid
inputs. In this paper, we dispel this myth: even for tasks as simple as finding
the maximum key, any learned circuitry must disperse as the number of items
grows at test time. We attribute this to a fundamental limitation of the
softmax function to robustly approximate sharp functions, prove this phenomenon
theoretically, and propose adaptive temperature as an ad-hoc technique for
improving the sharpness of softmax at inference time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Comments welcome. 15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How to Exhibit More Predictable Behaviors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11296v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11296v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salomé Lepers, Sophie Lemonnier, Vincent Thomas, Olivier Buffet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper looks at predictability problems, i.e., wherein an agent must
choose its strategy in order to optimize the predictions that an external
observer could make. We address these problems while taking into account
uncertainties on the environment dynamics and on the observed agent's policy.
To that end, we assume that the observer 1. seeks to predict the agent's future
action or state at each time step, and 2. models the agent using a stochastic
policy computed from a known underlying problem, and we leverage on the
framework of observer-aware Markov decision processes (OAMDPs). We propose
action and state predictability performance criteria through reward functions
built on the observer's belief about the agent policy; show that these induced
predictable OAMDPs can be represented by goal-oriented or discounted MDPs; and
analyze the properties of the proposed reward functions both theoretically and
empirically on two types of grid-world problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 14 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reconstruct Your Previous Conversations! Comprehensively Investigating
  Privacy Leakage Risks in Conversations with <span class="highlight-title">GPT</span> Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02987v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02987v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Chu, Zeyang Sha, Michael Backes, Yang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant advancements have recently been made in large language models
represented by GPT models. Users frequently have multi-round private
conversations with cloud-hosted GPT models for task optimization. Yet, this
operational paradigm introduces additional attack surfaces, particularly in
custom GPTs and hijacked chat sessions. In this paper, we introduce a
straightforward yet potent Conversation Reconstruction Attack. This attack
targets the contents of previous conversations between GPT models and benign
users, i.e., the benign users' input contents during their interaction with GPT
models. The adversary could induce GPT models to leak such contents by querying
them with designed malicious prompts. Our comprehensive examination of privacy
risks during the interactions with GPT models under this attack reveals GPT-4's
considerable resilience. We present two advanced attacks targeting improved
reconstruction of past conversations, demonstrating significant privacy leakage
across all models under these advanced techniques. Evaluating various defense
mechanisms, we find them ineffective against these attacks. Our findings
highlight the ease with which privacy can be compromised in interactions with
GPT models, urging the community to safeguard against potential abuses of these
models' capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in EMNLP 2024. 14 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language
  Models for Robotic Garment Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18082v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18082v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Li, Siyuan Huang, Qiaojun Yu, Zhengkai Jiang, Ce Hao, Yimeng Zhu, Hongsheng Li, Peng Gao, Cewu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automating garment manipulation poses a significant challenge for assistive
robotics due to the diverse and deformable nature of garments. Traditional
approaches typically require separate models for each garment type, which
limits scalability and adaptability. In contrast, this paper presents a unified
approach using vision-language models (VLMs) to improve keypoint prediction
across various garment categories. By interpreting both visual and semantic
information, our model enables robots to manage different garment states with a
single model. We created a large-scale synthetic dataset using advanced
simulation techniques, allowing scalable training without extensive real-world
data. Experimental results indicate that the VLM-based method significantly
enhances keypoint detection accuracy and task success rates, providing a more
flexible and general solution for robotic garment manipulation. In addition,
this research also underscores the potential of VLMs to unify various garment
manipulation tasks within a single framework, paving the way for broader
applications in home automation and assistive robotics for future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Question Decomposition on Multimodal Large Language Models <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19339v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19339v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Zhang, Jianzhe Liu, Zhen Han, Shuo Chen, Bailan He, Volker Tresp, Zhiqiang Xu, Jindong Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question decomposition has emerged as an effective strategy for prompting
Large Language Models (LLMs) to answer complex questions. However, while
existing methods primarily focus on unimodal language models, the question
decomposition capability of Multimodal Large Language Models (MLLMs) has yet to
be explored. To this end, this paper explores visual question decomposition on
MLLMs. Specifically, we introduce a systematic evaluation framework including a
dataset and several evaluation criteria to assess the quality of the decomposed
sub-questions, revealing that existing MLLMs struggle to produce high-quality
sub-questions. To address this limitation, we propose a specific finetuning
dataset, DecoVQA+, for enhancing the model's question decomposition capability.
Aiming at enabling models to perform appropriate selective decomposition, we
propose an efficient finetuning pipeline. The finetuning pipeline consists of
our proposed dataset and a training objective for selective decomposition.
Finetuned MLLMs demonstrate significant improvements in the quality of
sub-questions and the policy of selective question decomposition. Additionally,
the models also achieve higher accuracy with selective decomposition on VQA
benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty-Aware Decision <span class="highlight-title">Transformer</span> for Stochastic Driving
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16397v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16397v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zenan Li, Fan Nie, Qiao Sun, Fang Da, Hang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline Reinforcement Learning (RL) enables policy learning without active
interactions, making it especially appealing for self-driving tasks. Recent
successes of Transformers inspire casting offline RL as sequence modeling,
which, however, fails in stochastic environments with incorrect assumptions
that identical actions can consistently achieve the same goal. In this paper,
we introduce an UNcertainty-awaRE deciSion Transformer (UNREST) for planning in
stochastic driving environments without introducing additional transition or
complex generative models. Specifically, UNREST estimates uncertainties by
conditional mutual information between transitions and returns. Discovering
'uncertainty accumulation' and 'temporal locality' properties of driving
environments, we replace the global returns in decision transformers with
truncated returns less affected by environments to learn from actual outcomes
of actions rather than environment transitions. We also dynamically evaluate
uncertainty at inference for cautious planning. Extensive experiments
demonstrate UNREST's superior performance in various driving scenarios and the
power of our uncertainty estimation strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17023v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17023v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Vera Marjanović, Haeun Yu, Pepa Atanasova, Maria Maistro, Christina Lioma, Isabelle Augenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge-intensive language understanding tasks require Language Models
(LMs) to integrate relevant context, mitigating their inherent weaknesses, such
as incomplete or outdated knowledge. However, conflicting knowledge can be
present in the LM's parameters, termed intra-memory conflict, which can affect
a model's propensity to accept contextual knowledge. To study the effect of
intra-memory conflict on an LM's ability to accept relevant context, we utilize
two knowledge conflict measures and a novel dataset containing inherently
conflicting data, DynamicQA. This dataset includes facts with a temporal
dynamic nature where facts can change over time and disputable dynamic facts,
which can change depending on the viewpoint. DynamicQA is the first to include
real-world knowledge conflicts and provide context to study the link between
the different types of knowledge conflicts. We also evaluate several measures
on their ability to reflect the presence of intra-memory conflict: semantic
entropy and a novel coherent persuasion score. With our extensive experiments,
we verify that LMs exhibit a greater degree of intra-memory conflict with
dynamic facts compared to facts that have a single truth value. Furthermore, we
reveal that facts with intra-memory conflict are harder to update with context,
suggesting that retrieval-augmented generation will struggle with the most
commonly adapted facts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures, Accepted to Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entropy-Based Uncertainty Modeling for Trajectory Prediction in
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01628v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01628v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aron Distelzweig, Andreas Look, Eitan Kosman, Faris Janjoš, Jörg Wagner, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In autonomous driving, accurate motion prediction is essential for safe and
efficient motion planning. To ensure safety, planners must rely on reliable
uncertainty information about the predicted future behavior of surrounding
agents, yet this aspect has received limited attention. This paper addresses
the so-far neglected problem of uncertainty modeling in trajectory prediction.
We adopt a holistic approach that focuses on uncertainty quantification,
decomposition, and the influence of model composition. Our method is based on a
theoretically grounded information-theoretic approach to measure uncertainty,
allowing us to decompose total uncertainty into its aleatoric and epistemic
components. We conduct extensive experiments on the nuScenes dataset to assess
how different model architectures and configurations affect uncertainty
quantification and model robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, submitted to International Conference on
  Learning Representations (2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning
  with Knowledge-guided Retrieval Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10805v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10805v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Cehao Yang, Jiaxin Mao, Jian Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has enhanced large language models
(LLMs) by using knowledge retrieval to address knowledge gaps. However,
existing RAG approaches often fail to ensure the depth and completeness of the
information retrieved, which is essential for complex reasoning tasks. In this
work, we present Think-on-Graph 2.0 (ToG-2), a hybrid RAG framework that
iteratively retrieves information from both unstructured and structured
knowledge sources in a tightly integrated manner. Specifically, ToG-2 leverages
knowledge graphs (KGs) to connect documents via entities, facilitating deep and
knowledge-guided context retrieval. Simultaneously, it uses documents as entity
contexts to enable precise and efficient graph retrieval.
  ToG-2 alternates between graph retrieval and context retrieval to search for
in-depth clues relevant to the question, enabling LLMs to generate accurate
answers. We conduct a series of experiments to demonstrate the following
advantages of ToG-2: (1) ToG-2 tightly integrates context retrieval and graph
retrieval, enhancing context retrieval through the KG while enabling reliable
graph retrieval based on contexts; (2) it achieves deep and faithful reasoning
in LLMs through an iterative knowledge retrieval process that integrates
contexts and the KG; and (3) ToG-2 is training-free and compatible with various
LLMs as a plug-and-play solution. Extensive experiments show that ToG-2
achieves state-of-the-art (SOTA) performance on 6 out of 7 knowledge-intensive
datasets with GPT-3.5, and can elevate the performance of smaller models (e.g.,
LLAMA-2-13B) to the level of GPT-3.5's direct reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine listening in a neonatal intensive care unit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11439v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11439v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Modan Tailleur, Vincent Lostanlen, Jean-Philippe Rivière, Pierre Aumond
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Oxygenators, alarm devices, and footsteps are some of the most common sound
sources in a hospital. Detecting them has scientific value for environmental
psychology but comes with challenges of its own: namely, privacy preservation
and limited labeled data. In this paper, we address these two challenges via a
combination of edge computing and cloud computing. For privacy preservation, we
have designed an acoustic sensor which computes third-octave spectrograms on
the fly instead of recording audio waveforms. For sample-efficient machine
learning, we have repurposed a pretrained audio neural network (PANN) via
spectral transcoding and label space adaptation. A small-scale study in a
neonatological intensive care unit (NICU) confirms that the time series of
detected events align with another modality of measurement: i.e., electronic
badges for parents and healthcare professionals. Hence, this paper demonstrates
the feasibility of polyphonic machine listening in a hospital ward while
guaranteeing privacy by design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Novel Mathematical Framework for Objective Characterization of Ideas
  through Vector Embeddings in LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07578v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07578v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        B. Sankar, Dibakar Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The demand for innovation in product design necessitates a prolific ideation
phase. Conversational AI (CAI) systems that use Large Language Models (LLMs)
such as GPT (Generative Pre-trained Transformer) have been shown to be fruitful
in augmenting human creativity, providing numerous novel and diverse ideas.
Despite the success in ideation quantity, the qualitative assessment of these
ideas remains challenging and traditionally reliant on expert human evaluation.
This method suffers from limitations such as human judgment errors, bias, and
oversight. Addressing this gap, our study introduces a comprehensive
mathematical framework for automated analysis to objectively evaluate the
plethora of ideas generated by CAI systems and/or humans. This framework is
particularly advantageous for novice designers who lack experience in selecting
promising ideas. By converting the ideas into higher dimensional vectors and
quantitatively measuring the diversity between them using tools such as UMAP,
DBSCAN and PCA, the proposed method provides a reliable and objective way of
selecting the most promising ideas, thereby enhancing the efficiency of the
ideation phase.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 12 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Consistency Trajectory Models for Image Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12510v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12510v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beomsu Kim, Jaemin Kim, Jeongsol Kim, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) excel in unconditional generation, as well as on
applications such as image editing and restoration. The success of DMs lies in
the iterative nature of diffusion: diffusion breaks down the complex process of
mapping noise to data into a sequence of simple denoising tasks. Moreover, we
are able to exert fine-grained control over the generation process by injecting
guidance terms into each denoising step. However, the iterative process is also
computationally intensive, often taking from tens up to thousands of function
evaluations. Although consistency trajectory models (CTMs) enable traversal
between any time points along the probability flow ODE (PFODE) and score
inference with a single function evaluation, CTMs only allow translation from
Gaussian noise to data. This work aims to unlock the full potential of CTMs by
proposing generalized CTMs (GCTMs), which translate between arbitrary
distributions via ODEs. We discuss the design space of GCTMs and demonstrate
their efficacy in various image manipulation tasks such as image-to-image
translation, restoration, and editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-agent reinforcement learning using echo-state network and its
  application to pedestrian dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11834v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11834v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hisato Komatsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, simulations of pedestrians using the multi-agent
reinforcement learning (MARL) have been studied. This study considered the
roads on a grid-world environment, and implemented pedestrians as MARL agents
using an echo-state network and the least squares policy iteration method.
Under this environment, the ability of these agents to learn to move forward by
avoiding other agents was investigated. Specifically, we considered two types
of tasks: the choice between a narrow direct route and a broad detour, and the
bidirectional pedestrian flow in a corridor. The simulations results indicated
that the learning was successful when the density of the agents was not that
high.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Contrastive Feature Representations for Facial Action Unit
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06165v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06165v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiao Shang, Bin Liu, Fengmao Lv, Fei Teng, Tianrui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial action unit (AU) detection has long encountered the challenge of
detecting subtle feature differences when AUs activate. Existing methods often
rely on encoding pixel-level information of AUs, which not only encodes
additional redundant information but also leads to increased model complexity
and limited generalizability. Additionally, the accuracy of AU detection is
negatively impacted by the class imbalance issue of each AU type, and the
presence of noisy and false AU labels. In this paper, we introduce a novel
contrastive learning framework aimed for AU detection that incorporates both
self-supervised and supervised signals, thereby enhancing the learning of
discriminative features for accurate AU detection. To tackle the class
imbalance issue, we employ a negative sample re-weighting strategy that adjusts
the step size of updating parameters for minority and majority class samples.
Moreover, to address the challenges posed by noisy and false AU labels, we
employ a sampling technique that encompasses three distinct types of positive
sample pairs. This enables us to inject self-supervised signals into the
supervised signal, effectively mitigating the adverse effects of noisy labels.
Our experimental assessments, conducted on four widely-utilized benchmark
datasets (BP4D, DISFA, GFT and Aff-Wild2), underscore the superior performance
of our approach compared to state-of-the-art methods of AU detection. Our code
is available at \url{https://github.com/Ziqiao-Shang/AUNCE}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 17 figures, submitted to IEEE Transactions on Circuits and
  Systems for Video Technology (TCSVT)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Optimization-based Task and Motion Planning: From Classical
  To Learning Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02817v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02817v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhigen Zhao, Shuo Cheng, Yan Ding, Ziyi Zhou, Shiqi Zhang, Danfei Xu, Ye Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task and Motion Planning (TAMP) integrates high-level task planning and
low-level motion planning to equip robots with the autonomy to effectively
reason over long-horizon, dynamic tasks. Optimization-based TAMP focuses on
hybrid optimization approaches that define goal conditions via objective
functions and are capable of handling open-ended goals, robotic dynamics, and
physical interaction between the robot and the environment. Therefore,
optimization-based TAMP is particularly suited to solve highly complex,
contact-rich locomotion and manipulation problems. This survey provides a
comprehensive review on optimization-based TAMP, covering (i) planning domain
representations, including action description languages and temporal logic,
(ii) individual solution strategies for components of TAMP, including AI
planning and trajectory optimization (TO), and (iii) the dynamic interplay
between logic-based task planning and model-based TO. A particular focus of
this survey is to highlight the algorithm structures to efficiently solve TAMP,
especially hierarchical and distributed approaches. Additionally, the survey
emphasizes the synergy between the classical methods and contemporary
learning-based innovations such as large language models. Furthermore, the
future research directions for TAMP is discussed in this survey, highlighting
both algorithmic and application-specific challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 13 figures, published at IEEE/ASME Transactions on
  Mechatronics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QMP: Q-switch Mixture of Policies for Multi-Task Behavior Sharing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00671v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00671v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grace Zhang, Ayush Jain, Injune Hwang, Shao-Hua Sun, Joseph J. Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task reinforcement learning (MTRL) aims to learn several tasks
simultaneously for better sample efficiency than learning them separately.
Traditional methods achieve this by sharing parameters or relabeled data
between tasks. In this work, we introduce a new framework for sharing
behavioral policies across tasks, which can be used in addition to existing
MTRL methods. The key idea is to improve each task's off-policy data collection
by employing behaviors from other task policies. Selectively sharing helpful
behaviors acquired in one task to collect training data for another task can
lead to higher-quality trajectories, leading to more sample-efficient MTRL.
Thus, we introduce a simple and principled framework called Q-switch mixture of
policies (QMP) that selectively shares behavior between different task policies
by using the task's Q-function to evaluate and select useful shareable
behaviors. We theoretically analyze how QMP improves the sample efficiency of
the underlying RL algorithm. Our experiments show that QMP's behavioral policy
sharing provides complementary gains over many popular MTRL algorithms and
outperforms alternative ways to share behaviors in various manipulation,
locomotion, and navigation environments. Videos are available at
https://qmp-mtrl.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.05930v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.05930v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jushi Kai, Tianhang Zhang, Hai Hu, Zhouhan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate great performance in text
generation. However, LLMs are still suffering from hallucinations. In this
work, we propose an inference-time method, Self-Highlighted Hesitation (SH2),
to help LLMs decode more truthfully. SH2 is based on a simple fact rooted in
information theory that for an LLM, the tokens predicted with lower
probabilities are prone to be more informative than others. Our analysis shows
that the tokens assigned with lower probabilities by an LLM are more likely to
be closely related to factual information, such as nouns, proper nouns, and
adjectives. Therefore, we propose to ''highlight'' the factual information by
selecting the tokens with the lowest probabilities and concatenating them to
the original context, thus forcing the model to repeatedly read and hesitate on
these tokens before generation. During decoding, we also adopt contrastive
decoding to emphasize the difference in the output probabilities brought by the
hesitation. Experimental results demonstrate that our SH2, requiring no
additional data or models, can effectively help LLMs elicit factual knowledge
and distinguish hallucinated contexts. Significant and consistent improvements
are achieved by SH2 for LLaMA-7b, LLaMA2-7b and Mistral-7b on multiple
hallucination tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DALL-M: Context-Aware Clinical Data Augmentation with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08227v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08227v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chihcheng Hsieh, Catarina Moreira, Isabel Blanco Nobre, Sandra Costa Sousa, Chun Ouyang, Margot Brereton, Joaquim Jorge, Jacinto C. Nascimento
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  X-ray images are vital in medical diagnostics, but their effectiveness is
limited without clinical context. Radiologists often find chest X-rays
insufficient for diagnosing underlying diseases, necessitating comprehensive
clinical features and data integration. We present a novel framework to enhance
the clinical context through augmentation techniques with clinical tabular
data, thereby improving its applicability and reliability in AI medical
diagnostics. We introduce a pioneering approach to clinical data augmentation
that employs large language models to generate patient contextual synthetic
data. This methodology is crucial for training more robust deep learning models
in healthcare. It preserves the integrity of real patient data while enriching
the dataset with contextually relevant synthetic features, significantly
enhancing model performance. Our methodology, termed DALL-M, uses a three-phase
feature generation process: (i)clinical context storage, (ii)expert query
generation, and (iii)context-aware feature augmentation. DALL-M generates new,
clinically relevant features by synthesizing chest X-ray images and reports.
Applied to 799 cases using nine features from the MIMIC-IV dataset, it created
an augmented set of 91 features. This is the first work to generate contextual
values for patients' X-ray reports. Specifically, we provide (i)the capacity of
LLMs to generate contextual synthetic values for existing clinical features and
(ii)their ability to create entirely new clinically relevant features.
Empirical validation with machine learning models showed significant
performance improvements. Incorporating augmented features increased the F1
score by 16.5% and Precision and Recall by approximately 25%. DALL-M addresses
a critical gap in clinical data augmentation, offering a robust framework for
generating contextually enriched datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>we introduce a pioneering approach to clinical data augmentation that
  employs large language models (LLMs) to generate patient contextual synthetic
  data. It preserves the integrity of real patient data while enriching the
  dataset with contextually relevant synthetic features, significantly
  enhancing model performance</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CBF-LLM: Safe Control for LLM Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15625v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15625v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuya Miyaoka, Masaki Inoue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a control-based framework for aligning large language
models (LLMs) by leveraging a control barrier function (CBF) to ensure
user-desirable text generation. The presented framework applies the safety
filter, designed based on the CBF, to the output generation of the baseline
LLM, i.e., the sequence of the token, with the aim of intervening in the
generated text. The overall text-generation system is implemented with Llama 3
and a RoBERTa model, and the source code is available at
https://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control
ability and effectiveness in reducing the number of interventions needed for
user-specified alignment tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Framework for Pupil Tracking with Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khadija Iddrisu, Waseem Shariff, Suzanne Little
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Saccades are extremely rapid movements of both eyes that occur
simultaneously, typically observed when an individual shifts their focus from
one object to another. These movements are among the swiftest produced by
humans and possess the potential to achieve velocities greater than that of
blinks. The peak angular speed of the eye during a saccade can reach as high as
700{\deg}/s in humans, especially during larger saccades that cover a visual
angle of 25{\deg}. Previous research has demonstrated encouraging outcomes in
comprehending neurological conditions through the study of saccades. A
necessary step in saccade detection involves accurately identifying the precise
location of the pupil within the eye, from which additional information such as
gaze angles can be inferred. Conventional frame-based cameras often struggle
with the high temporal precision necessary for tracking very fast movements,
resulting in motion blur and latency issues. Event cameras, on the other hand,
offer a promising alternative by recording changes in the visual scene
asynchronously and providing high temporal resolution and low latency. By
bridging the gap between traditional computer vision and event-based vision, we
present events as frames that can be readily utilized by standard deep learning
algorithms. This approach harnesses YOLOv8, a state-of-the-art object detection
technology, to process these frames for pupil tracking using the publicly
accessible Ev-Eye dataset. Experimental results demonstrate the framework's
effectiveness, highlighting its potential applications in neuroscience,
ophthalmology, and human-computer interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is a preprint of a paper submitted to the 26th Irish
  Machine Vision and Image Processing Conference (IMVIP 2024). If accepted, the
  copy of record will be available at IET Digital Library</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Odyssey: Empowering Minecraft Agents with Open-World Skills 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunyu Liu, Yaoru Li, Kongcheng Zhang, Zhenyu Cui, Wenkai Fang, Yuxuan Zheng, Tongya Zheng, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have delved into constructing generalist agents for open-world
environments like Minecraft. Despite the encouraging results, existing efforts
mainly focus on solving basic programmatic tasks, e.g., material collection and
tool-crafting following the Minecraft tech-tree, treating the ObtainDiamond
task as the ultimate goal. This limitation stems from the narrowly defined set
of actions available to agents, requiring them to learn effective long-horizon
strategies from scratch. Consequently, discovering diverse gameplay
opportunities in the open world becomes challenging. In this work, we introduce
Odyssey, a new framework that empowers Large Language Model (LLM)-based agents
with open-world skills to explore the vast Minecraft world. Odyssey comprises
three key parts: (1) An interactive agent with an open-world skill library that
consists of 40 primitive skills and 183 compositional skills. (2) A fine-tuned
LLaMA-3 model trained on a large question-answering dataset with 390k+
instruction entries derived from the Minecraft Wiki. (3) A new agent capability
benchmark includes the long-term planning task, the dynamic-immediate planning
task, and the autonomous exploration task. Extensive experiments demonstrate
that the proposed Odyssey framework can effectively evaluate different
capabilities of LLM-based agents. All datasets, model weights, and code are
publicly available to motivate future research on more advanced autonomous
agent solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AlignedCoT: <span class="highlight-title">Prompt</span>ing Large Language Models via Native-Speaking
  Demonstrations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13538v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13538v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicheng Yang, Yinya Huang, Jing Xiong, Liang Feng, Xiaodan Liang, Yiwei Wang, Jing Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models prompting, such as using in-context demonstrations, is
a mainstream technique for invoking LLMs to perform high-performance and solid
complex reasoning (e.g., mathematical reasoning, commonsense reasoning), and
has the potential for further human-machine collaborative scientific findings.
However, current LLMs are delicate and elusive in prompt words and styles. And
there is an unseen gap between LLM understanding and human-written prompts.
This paper introduces Alignedcot, an LLM-acquainted prompting technique that
includes proficient ``native-speaking'' in in-context learning for the LLMs.
Specifically, it achieves consistent and correct step-wise prompts in zero-shot
scenarios by progressively probing, refining, and formatting the LLM chain of
thoughts so that free from handcrafted few-shot demonstrations while
maintaining the prompt quality. We conduct experiments on mathematical
reasoning and commonsense reasoning. We find that LLMs with Alignedcot perform
significantly superior to them with human-crafted demonstrations. We further
apply Alignedcot for rewriting the GSM8K training set, resulting in a
GSM8K-Align dataset. We observe its benefits for retrieval augmented
generation. The code and data can be found at
https://github.com/yangzhch6/AlignedCoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Large Language Models Understand Symbolic Graphics Programs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08313v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08313v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Z. Xiao, Katherine M. Collins, Joshua B. Tenenbaum, Adrian Weller, Michael J. Black, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Against the backdrop of enthusiasm for large language models (LLMs), there is
an urgent need to scientifically assess their capabilities and shortcomings.
This is nontrivial in part because it is difficult to find tasks which the
models have not encountered during training. Utilizing symbolic graphics
programs, we propose a domain well-suited to test multiple spatial-semantic
reasoning skills of LLMs. Popular in computer graphics, these programs
procedurally generate visual data. While LLMs exhibit impressive skills in
general program synthesis and analysis, symbolic graphics programs offer a new
layer of evaluation: they allow us to test an LLM's ability to answer
different-grained semantic-level questions of the images or 3D geometries
without a vision encoder. To semantically understand the symbolic programs,
LLMs would need to possess the ability to "imagine" and reason how the
corresponding graphics content would look with only the symbolic description.
We use this task to evaluate LLMs by creating a large benchmark for the
semantic visual understanding of symbolic graphics programs, built procedurally
with minimal human effort. Particular emphasis is placed on transformations of
images that leave the image level semantics invariant while introducing
significant changes to the underlying program. We evaluate commercial and
open-source LLMs on our benchmark to assess their ability to reason about
visual output of programs, finding that LLMs considered stronger at reasoning
generally perform better. Lastly, we introduce a novel method to improve this
ability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned
with pre-collected instruction data on symbolic graphics programs.
Interestingly, we find that SIT not only improves LLM's understanding on
symbolic programs, but it also improves general reasoning ability on various
other benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report v2 (46 pages, 24 figures, project page:
  https://sgp-bench.github.io/, substantial update from v1)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TD-NeRF: Novel Truncated Depth Prior for Joint Camera Pose and Neural
  Radiance Field Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Tan, Zongtan Zhou, Yangbing Ge, Zi Wang, Xieyuanli Chen, Dewen Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reliance on accurate camera poses is a significant barrier to the
widespread deployment of Neural Radiance Fields (NeRF) models for 3D
reconstruction and SLAM tasks. The existing method introduces monocular depth
priors to jointly optimize the camera poses and NeRF, which fails to fully
exploit the depth priors and neglects the impact of their inherent noise. In
this paper, we propose Truncated Depth NeRF (TD-NeRF), a novel approach that
enables training NeRF from unknown camera poses - by jointly optimizing
learnable parameters of the radiance field and camera poses. Our approach
explicitly utilizes monocular depth priors through three key advancements: 1)
we propose a novel depth-based ray sampling strategy based on the truncated
normal distribution, which improves the convergence speed and accuracy of pose
estimation; 2) to circumvent local minima and refine depth geometry, we
introduce a coarse-to-fine training strategy that progressively improves the
depth precision; 3) we propose a more robust inter-frame point constraint that
enhances robustness against depth noise during training. The experimental
results on three datasets demonstrate that TD-NeRF achieves superior
performance in the joint optimization of camera pose and NeRF, surpassing prior
works, and generates more accurate depth geometry. The implementation of our
method has been released at https://github.com/nubot-nudt/TD-NeRF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Dynamic Model of Performative Human-ML Collaboration: Theory and
  Empirical Evidence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13753v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13753v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Sühr, Samira Samadi, Chiara Farronato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) models are increasingly used in various applications,
from recommendation systems in e-commerce to diagnosis prediction in
healthcare. In this paper, we present a novel dynamic framework for thinking
about the deployment of ML models in a performative, human-ML collaborative
system. In our framework, the introduction of ML recommendations changes the
data-generating process of human decisions, which are only a proxy to the
ground truth and which are then used to train future versions of the model. We
show that this dynamic process in principle can converge to different stable
points, i.e. where the ML model and the Human+ML system have the same
performance. Some of these stable points are suboptimal with respect to the
actual ground truth. As a proof of concept, we conduct an empirical user study
with 1,408 participants. In the study, humans solve instances of the knapsack
problem with the help of machine learning predictions of varying performance.
This is an ideal setting because we can identify the actual ground truth, and
evaluate the performance of human decisions supported by ML recommendations. We
find that for many levels of ML performance, humans can improve upon the ML
predictions. We also find that the improvement could be even higher if humans
rationally followed the ML recommendations. Finally, we test whether monetary
incentives can increase the quality of human decisions, but we fail to find any
positive effect. Using our empirical data to approximate our collaborative
system suggests that the learning process would dynamically reach an
equilibrium performance that is around 92% of the maximum knapsack value. Our
results have practical implications for the deployment of ML models in contexts
where human decisions may deviate from the indisputable ground truth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 Pages and appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProteinBench: A Holistic Evaluation of Protein Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Ye, Zaixiang Zheng, Dongyu Xue, Yuning Shen, Lihao Wang, Yiming Ma, Yan Wang, Xinyou Wang, Xiangxin Zhou, Quanquan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed a surge in the development of protein foundation
models, significantly improving performance in protein prediction and
generative tasks ranging from 3D structure prediction and protein design to
conformational dynamics. However, the capabilities and limitations associated
with these models remain poorly understood due to the absence of a unified
evaluation framework. To fill this gap, we introduce ProteinBench, a holistic
evaluation framework designed to enhance the transparency of protein foundation
models. Our approach consists of three key components: (i) A taxonomic
classification of tasks that broadly encompass the main challenges in the
protein domain, based on the relationships between different protein
modalities; (ii) A multi-metric evaluation approach that assesses performance
across four key dimensions: quality, novelty, diversity, and robustness; and
(iii) In-depth analyses from various user objectives, providing a holistic view
of model performance. Our comprehensive evaluation of protein foundation models
reveals several key findings that shed light on their current capabilities and
limitations. To promote transparency and facilitate further research, we
release the evaluation dataset, code, and a public leaderboard publicly for
further analysis and a general modular toolkit. We intend for ProteinBench to
be a living benchmark for establishing a standardized, in-depth evaluation
framework for protein foundation models, driving their development and
application while fostering collaboration within the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 2 figures and 15 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Persuasion: Towards Conversational Recommender System with
  Credible Explanations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14399v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14399v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peixin Qin, Chen Huang, Yang Deng, Wenqiang Lei, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the aid of large language models, current conversational recommender
system (CRS) has gaining strong abilities to persuade users to accept
recommended items. While these CRSs are highly persuasive, they can mislead
users by incorporating incredible information in their explanations, ultimately
damaging the long-term trust between users and the CRS. To address this, we
propose a simple yet effective method, called PC-CRS, to enhance the
credibility of CRS's explanations during persuasion. It guides the explanation
generation through our proposed credibility-aware persuasive strategies and
then gradually refines explanations via post-hoc self-reflection. Experimental
results demonstrate the efficacy of PC-CRS in promoting persuasive and credible
explanations. Further analysis reveals the reason behind current methods
producing incredible explanations and the potential of credible explanations to
improve recommendation accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2024. Our code is available at
  https://github.com/mumen798/PC-CRS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Shield Synthesis via State-Space Transformation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19911v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19911v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asger Horn Brorholt, Andreas Holck Høeg-Petersen, Kim Guldstrand Larsen, Christian Schilling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of synthesizing safety strategies for control
systems, also known as shields. Since the state space is infinite, shields are
typically computed over a finite-state abstraction, with the most common
abstraction being a rectangular grid. However, for many systems, such a grid
does not align well with the safety property or the system dynamics. That is
why a coarse grid is rarely sufficient, but a fine grid is typically
computationally infeasible to obtain. In this paper, we show that appropriate
state-space transformations can still allow to use a coarse grid at almost no
computational overhead. We demonstrate in three case studies that our
transformation-based synthesis outperforms a standard synthesis by several
orders of magnitude. In the first two case studies, we use domain knowledge to
select a suitable transformation. In the third case study, we instead report on
results in engineering a transformation without domain knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI Delegates with a Dual Focus: Ensuring Privacy and Strategic
  Self-Disclosure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17642v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17642v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Chen, Zhiyang Zhang, Fangkai Yang, Xiaoting Qin, Chao Du, Xi Cheng, Hangxin Liu, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM)-based AI delegates are increasingly utilized to
act on behalf of users, assisting them with a wide range of tasks through
conversational interfaces. Despite their advantages, concerns arise regarding
the potential risk of privacy leaks, particularly in scenarios involving social
interactions. While existing research has focused on protecting privacy by
limiting the access of AI delegates to sensitive user information, many social
scenarios require disclosing private details to achieve desired outcomes,
necessitating a balance between privacy protection and disclosure. To address
this challenge, we conduct a pilot study to investigate user preferences for AI
delegates across various social relations and task scenarios, and then propose
a novel AI delegate system that enables privacy-conscious self-disclosure. Our
user study demonstrates that the proposed AI delegate strategically protects
privacy, pioneering its use in diverse and dynamic social interactions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PhysDreamer: Physics-Based Interaction with 3D Objects via Video
  Generation <span class="chip">ECCV
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13026v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13026v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Y. Feng, Changxi Zheng, Noah Snavely, Jiajun Wu, William T. Freeman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Realistic object interactions are crucial for creating immersive virtual
experiences, yet synthesizing realistic 3D object dynamics in response to novel
interactions remains a significant challenge. Unlike unconditional or
text-conditioned dynamics generation, action-conditioned dynamics requires
perceiving the physical material properties of objects and grounding the 3D
motion prediction on these properties, such as object stiffness. However,
estimating physical material properties is an open problem due to the lack of
material ground-truth data, as measuring these properties for real objects is
highly difficult. We present PhysDreamer, a physics-based approach that endows
static 3D objects with interactive dynamics by leveraging the object dynamics
priors learned by video generation models. By distilling these priors,
PhysDreamer enables the synthesis of realistic object responses to novel
interactions, such as external forces or agent manipulations. We demonstrate
our approach on diverse examples of elastic objects and evaluate the realism of
the synthesized interactions through a user study. PhysDreamer takes a step
towards more engaging and realistic virtual experiences by enabling static 3D
objects to dynamically respond to interactive stimuli in a physically plausible
manner. See our project page at https://physdreamer.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website at: https://physdreamer.github.io/ Appear on ECCV
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nonparametric Strategy Test 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10695v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10695v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Ganzfried
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a nonparametric statistical test for determining whether an agent
is following a given mixed strategy in a repeated strategic-form game given
samples of the agent's play. This involves two components: determining whether
the agent's frequencies of pure strategies are sufficiently close to the target
frequencies, and determining whether the pure strategies selected are
independent between different game iterations. Our integrated test involves
applying a chi-squared goodness of fit test for the first component and a
generalized Wald-Wolfowitz runs test for the second component. The results from
both tests are combined using Bonferroni correction to produce a complete test
for a given significance level $\alpha.$ We applied the test to publicly
available data of human rock-paper-scissors play. The data consists of 50
iterations of play for 500 human players. We test with a null hypothesis that
the players are following a uniform random strategy independently at each game
iteration. Using a significance level of $\alpha = 0.05$, we conclude that 305
(61%) of the subjects are following the target strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ sDPO: Don't Use Your Data All at Once 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dahyun Kim, Yungi Kim, Wonho Song, Hyeonwoo Kim, Yunsu Kim, Sanghoon Kim, Chanjun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As development of large language models (LLM) progresses, aligning them with
human preferences has become increasingly important. We propose stepwise DPO
(sDPO), an extension of the recently popularized direct preference optimization
(DPO) for alignment tuning. This approach involves dividing the available
preference datasets and utilizing them in a stepwise manner, rather than
employing it all at once. We demonstrate that this method facilitates the use
of more precisely aligned reference models within the DPO training framework.
Furthermore, sDPO trains the final model to be more performant, even
outperforming other popular LLMs with more parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mind Scramble: Unveiling Large Language Model Psychology Via
  Typoglycemia 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01677v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01677v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miao Yu, Junyuan Mao, Guibin Zhang, Jingheng Ye, Junfeng Fang, Aoxiao Zhong, Yang Liu, Yuxuan Liang, Kun Wang, Qingsong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research into the external behaviors and internal mechanisms of large
language models (LLMs) has shown promise in addressing complex tasks in the
physical world. Studies suggest that powerful LLMs, like GPT-4, are beginning
to exhibit human-like cognitive abilities, including planning, reasoning, and
reflection. In this paper, we introduce a research line and methodology called
LLM Psychology, leveraging human psychology experiments to investigate the
cognitive behaviors and mechanisms of LLMs. We migrate the Typoglycemia
phenomenon from psychology to explore the "mind" of LLMs. Unlike human brains,
which rely on context and word patterns to comprehend scrambled text, LLMs use
distinct encoding and decoding processes. Through Typoglycemia experiments at
the character, word, and sentence levels, we observe: (I) LLMs demonstrate
human-like behaviors on a macro scale, such as lower task accuracy and higher
token/time consumption; (II) LLMs exhibit varying robustness to scrambled
input, making Typoglycemia a benchmark for model evaluation without new
datasets; (III) Different task types have varying impacts, with complex logical
tasks (e.g., math) being more challenging in scrambled form; (IV) Each LLM has
a unique and consistent "cognitive pattern" across tasks, revealing general
mechanisms in its psychology process. We provide an in-depth analysis of hidden
layers to explain these phenomena, paving the way for future research in LLM
Psychology and deeper interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and
  Iterative Sub-SQL Refinement for Text-to-SQL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07930v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07930v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Xie, Gaochen Wu, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent In-Context Learning based methods have achieved remarkable success in
Text-to-SQL task. However, there is still a large gap between the performance
of these models and human performance on datasets with complex database schema
and difficult questions, such as BIRD. Besides, existing work has neglected to
supervise intermediate steps when solving questions iteratively with question
decomposition methods, and the schema linking methods used in these works are
very rudimentary. To address these issues, we propose MAG-SQL, a multi-agent
generative approach with soft schema linking and iterative Sub-SQL refinement.
In our framework, an entity-based method with tables' summary is used to select
the columns in database, and a novel targets-conditions decomposition method is
introduced to decompose those complex questions. Additionally, we build a
iterative generating module which includes a Sub-SQL Generator and Sub-SQL
Refiner, introducing external oversight for each step of generation. Through a
series of ablation studies, the effectiveness of each agent in our framework
has been demonstrated. When evaluated on the BIRD benchmark with GPT-4, MAG-SQL
achieves an execution accuracy of 61.08%, compared to the baseline accuracy of
46.35% for vanilla GPT-4 and the baseline accuracy of 57.56% for MAC-SQL.
Besides, our approach makes similar progress on Spider.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Framework for Guided Motion Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03133v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03133v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amnon Attali, Stav Ashur, Isaac Burton Love, Courtney McBeth, James Motes, Marco Morales, Nancy M. Amato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Randomized sampling based algorithms are widely used in robot motion planning
due to the problem's intractability, and are experimentally effective on a wide
range of problem instances. Most variants bias their sampling using various
heuristics related to the known underlying structure of the search space. In
this work, we formalize the intuitive notion of guided search by defining the
concept of a guiding space. This new language encapsulates many seemingly
distinct prior methods under the same framework, and allows us to reason about
guidance, a previously obscured core contribution of different algorithms. We
suggest an information theoretic method to evaluate guidance, which
experimentally matches intuition when tested on known algorithms in a variety
of environments. The language and evaluation of guidance suggests improvements
to existing methods, and allows for simple hybrid algorithms that combine
guidance from multiple sources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability
  of Large Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17169v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17169v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nisarg Patel, Mohith Kulkarni, Mihir Parmar, Aashna Budhiraja, Mutsumi Nakamura, Neeraj Varshney, Chitta Baral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) continue to exhibit remarkable performance in
natural language understanding tasks, there is a crucial need to measure their
ability for human-like multi-step logical reasoning. Existing logical reasoning
evaluation benchmarks often focus primarily on simplistic single-step or
multi-step reasoning with a limited set of inference rules. Furthermore, the
lack of datasets for evaluating non-monotonic reasoning represents a crucial
gap since it aligns more closely with human-like reasoning. To address these
limitations, we propose Multi-LogiEval, a comprehensive evaluation dataset
encompassing multi-step logical reasoning with various inference rules and
depths. Multi-LogiEval covers three logic types--propositional, first-order,
and non-monotonic--consisting of more than 30 inference rules and more than 60
of their combinations with various depths. Leveraging this dataset, we conduct
evaluations on a range of LLMs including GPT-4, ChatGPT, Gemini-Pro, Yi, Orca,
and Mistral, employing a zero-shot chain-of-thought. Experimental results show
that there is a significant drop in the performance of LLMs as the reasoning
steps/depth increases (average accuracy of ~68% at depth-1 to ~43% at depth-5).
We further conduct a thorough investigation of reasoning chains generated by
LLMs which reveals several important findings. We believe that Multi-LogiEval
facilitates future research for evaluating and enhancing the logical reasoning
ability of LLMs. Data is available at
https://github.com/Mihir3009/Multi-LogiEval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SplatSim: Zero-Shot Sim2Real Transfer of RGB Manipulation Policies Using
  Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10161v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10161v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Nomaan Qureshi, Sparsh Garg, Francisco Yandun, David Held, George Kantor, Abhisesh Silwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sim2Real transfer, particularly for manipulation policies relying on RGB
images, remains a critical challenge in robotics due to the significant domain
shift between synthetic and real-world visual data. In this paper, we propose
SplatSim, a novel framework that leverages Gaussian Splatting as the primary
rendering primitive to reduce the Sim2Real gap for RGB-based manipulation
policies. By replacing traditional mesh representations with Gaussian Splats in
simulators, SplatSim produces highly photorealistic synthetic data while
maintaining the scalability and cost-efficiency of simulation. We demonstrate
the effectiveness of our framework by training manipulation policies within
SplatSim and deploying them in the real world in a zero-shot manner, achieving
an average success rate of 86.25%, compared to 97.5% for policies trained on
real-world data. Videos can be found on our project page:
https://splatsim.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaAligner: Towards Generalizable Multi-Objective Alignment of Language
  Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17141v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17141v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kailai Yang, Zhiwei Liu, Qianqian Xie, Jimin Huang, Tianlin Zhang, Sophia Ananiadou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) focus on aligning to
heterogeneous human expectations and values via multi-objective preference
alignment. However, existing methods are dependent on the policy model
parameters, which require high-cost repetition of their alignment algorithms
for each new policy model, and they cannot expand to unseen objectives due to
their static alignment objectives. In this work, we propose Meta-Objective
Aligner (MetaAligner), the first policy-agnostic and generalizable method for
multi-objective preference alignment. MetaAligner models multi-objective
alignment into three stages: (1) dynamic objectives reformulation algorithm
reorganizes traditional alignment datasets to supervise the model on performing
flexible alignment across different objectives; (2) conditional weak-to-strong
correction paradigm aligns the weak outputs of fixed policy models to approach
strong outputs with higher preferences in the corresponding alignment
objectives, enabling plug-and-play inferences on any policy models, which
significantly reduces training costs and facilitates alignment on close-source
policy models; (3) generalizable inference method flexibly adjusts target
objectives by updating their text descriptions in the prompts, facilitating
generalizable alignment to unseen objectives. Experimental results show that
MetaAligner achieves significant and balanced improvements in multi-objective
alignments on 10 state-of-the-art policy models, and saves up to 93.63% of GPU
training hours compared to previous alignment methods. The model also
effectively aligns unseen objectives, marking the first step towards
generalizable multi-objective preference alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 main track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Influence-based Attributions can be Manipulated 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05208v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05208v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chhavi Yadav, Ruihan Wu, Kamalika Chaudhuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Influence Functions are a standard tool for attributing predictions to
training data in a principled manner and are widely used in applications such
as data valuation and fairness. In this work, we present realistic incentives
to manipulate influence-based attributions and investigate whether these
attributions can be \textit{systematically} tampered by an adversary. We show
that this is indeed possible for logistic regression models trained on ResNet
feature embeddings and standard tabular fairness datasets and provide efficient
attacks with backward-friendly implementations. Our work raises questions on
the reliability of influence-based attributions in adversarial circumstances.
Code is available at :
\url{https://github.com/infinite-pursuits/influence-based-attributions-can-be-manipulated}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12327v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12327v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Kaushal, Tejas Vaidhya, Arnab Kumar Mondal, Tejas Pandey, Aaryan Bhagat, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid advancements in GPU computational power has outpaced memory capacity
and bandwidth growth, creating bottlenecks in Large Language Model (LLM)
inference. Post-training quantization is the leading method for addressing
memory-related bottlenecks in LLM inference, but it suffers from significant
performance degradation below 4-bit precision. This paper addresses these
challenges by investigating the pretraining of low-bitwidth models specifically
Ternary Language Models (TriLMs) as an alternative to traditional
floating-point models (FloatLMs) and their post-training quantized versions
(QuantLMs). We present Spectra LLM suite, the first open suite of LLMs spanning
multiple bit-widths, including FloatLMs, QuantLMs, and TriLMs, ranging from 99M
to 3.9B parameters trained on 300B tokens. Our comprehensive evaluation
demonstrates that TriLMs offer superior scaling behavior in terms of model size
(in bits). Surprisingly, at scales exceeding one billion parameters, TriLMs
consistently outperform their QuantLM and FloatLM counterparts for a given bit
size across various benchmarks. Notably, the 3.9B parameter TriLM matches the
performance of the FloatLM 3.9B across all benchmarks, despite having fewer
bits than FloatLM 830M. Overall, this research provides valuable insights into
the feasibility and scalability of low-bitwidth language models, paving the way
for the development of more efficient LLMs.
  To enhance understanding of low-bitwidth models, we are releasing 500+
intermediate checkpoints of the Spectra suite at
\href{https://github.com/NolanoOrg/SpectraSuite}{https://github.com/NolanoOrg/SpectraSuite}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 21 figures, and 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating and Safeguarding the Adversarial Robustness of
  Retrieval-Based In-Context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15984v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15984v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Yu, Jie He, Pasquale Minervini, Jeff Z. Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of large language models, such as LLaMA and OpenAI GPT-3,
In-Context Learning (ICL) gained significant attention due to its effectiveness
and efficiency. However, ICL is very sensitive to the choice, order, and
verbaliser used to encode the demonstrations in the prompt. Retrieval-Augmented
ICL methods try to address this problem by leveraging retrievers to extract
semantically related examples as demonstrations. While this approach yields
more accurate results, its robustness against various types of adversarial
attacks, including perturbations on test samples, demonstrations, and retrieved
data, remains under-explored. Our study reveals that retrieval-augmented models
can enhance robustness against test sample attacks, outperforming vanilla ICL
with a 4.87% reduction in Attack Success Rate (ASR); however, they exhibit
overconfidence in the demonstrations, leading to a 2% increase in ASR for
demonstration attacks. Adversarial training can help improve the robustness of
ICL methods to adversarial attacks; however, such a training scheme can be too
costly in the context of LLMs. As an alternative, we introduce an effective
training-free adversarial defence method, DARD, which enriches the example pool
with those attacked samples. We show that DARD yields improvements in
performance and robustness, achieving a 15% reduction in ASR over the
baselines. Code and data are released to encourage further research:
https://github.com/simonucl/adv-retreival-icl
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLM 2024, 30 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evalverse: Unified and Accessible Library for Large Language Model
  Evaluation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihoo Kim, Wonho Song, Dahyun Kim, Yunsu Kim, Yungi Kim, Chanjun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Evalverse, a novel library that streamlines the
evaluation of Large Language Models (LLMs) by unifying disparate evaluation
tools into a single, user-friendly framework. Evalverse enables individuals
with limited knowledge of artificial intelligence to easily request LLM
evaluations and receive detailed reports, facilitated by an integration with
communication platforms like Slack. Thus, Evalverse serves as a powerful tool
for the comprehensive assessment of LLMs, offering both researchers and
practitioners a centralized and easily accessible evaluation framework.
Finally, we also provide a demo video for Evalverse, showcasing its
capabilities and implementation in a two-minute format.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Demo Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05010v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05010v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Liu, Dongyang Dai, Zhiyong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in generative modeling have significantly enhanced the
reconstruction of audio waveforms from various representations. While diffusion
models are adept at this task, they are hindered by latency issues due to their
operation at the individual sample point level and the need for numerous
sampling steps. In this study, we introduce RFWave, a cutting-edge multi-band
Rectified Flow approach designed to reconstruct high-fidelity audio waveforms
from Mel-spectrograms or discrete acoustic tokens. RFWave uniquely generates
complex spectrograms and operates at the frame level, processing all subbands
simultaneously to boost efficiency. Leveraging Rectified Flow, which targets a
straight transport trajectory, RFWave achieves reconstruction with just 10
sampling steps. Our empirical evaluations show that RFWave not only provides
outstanding reconstruction quality but also offers vastly superior
computational efficiency, enabling audio generation at speeds up to 160 times
faster than real-time on a GPU. An online demonstration is available at:
https://rfwave-demo.github.io/rfwave/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Trustworthiness in Foundation Models for Medical Image
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15851v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15851v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Congzhen Shi, Ryan Rezai, Jiaxi Yang, Qi Dou, Xiaoxiao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of foundation models in medical imaging represents a
significant leap toward enhancing diagnostic accuracy and personalized
treatment. However, the deployment of foundation models in healthcare
necessitates a rigorous examination of their trustworthiness, encompassing
privacy, robustness, reliability, explainability, and fairness. The current
body of survey literature on foundation models in medical imaging reveals
considerable gaps, particularly in the area of trustworthiness. Additionally,
existing surveys on the trustworthiness of foundation models do not adequately
address their specific variations and applications within the medical imaging
domain. This survey aims to fill that gap by presenting a novel taxonomy of
foundation models used in medical imaging and analyzing the key motivations for
ensuring their trustworthiness. We review current research on foundation models
in major medical imaging applications, focusing on segmentation, medical report
generation, medical question and answering (Q\&A), and disease diagnosis. These
areas are highlighted because they have seen a relatively mature and
substantial number of foundation models compared to other applications. We
focus on literature that discusses trustworthiness in medical image analysis
manuscripts. We explore the complex challenges of building trustworthy
foundation models for each application, summarizing current concerns and
strategies for enhancing trustworthiness. Furthermore, we examine the potential
of these models to revolutionize patient care. Our analysis underscores the
imperative for advancing towards trustworthy AI in medical image analysis,
advocating for a balanced approach that fosters innovation while ensuring
ethical and equitable healthcare delivery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Magpie: Alignment Data Synthesis from Scratch by <span class="highlight-title">Prompt</span>ing Aligned LLMs
  with Nothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, Bill Yuchen Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality instruction data is critical for aligning large language models
(LLMs). Although some models, such as Llama-3-Instruct, have open weights,
their alignment data remain private, which hinders the democratization of AI.
High human labor costs and a limited, predefined scope for prompting prevent
existing open-source data creation methods from scaling effectively,
potentially limiting the diversity and quality of public alignment datasets. Is
it possible to synthesize high-quality instruction data at scale by extracting
it directly from an aligned LLM? We present a self-synthesis method for
generating large-scale alignment data named Magpie. Our key observation is that
aligned LLMs like Llama-3-Instruct can generate a user query when we input only
the left-side templates up to the position reserved for user messages, thanks
to their auto-regressive nature. We use this method to prompt Llama-3-Instruct
and generate 4 million instructions along with their corresponding responses.
We perform a comprehensive analysis of the extracted data and select 300K
high-quality instances. To compare Magpie data with other public instruction
datasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate the
performance of the fine-tuned models. Our results indicate that in some tasks,
models fine-tuned with Magpie perform comparably to the official
Llama-3-8B-Instruct, despite the latter being enhanced with 10 million data
points through supervised fine-tuning (SFT) and subsequent feedback learning.
We also show that using Magpie solely for SFT can surpass the performance of
previous public datasets utilized for both SFT and preference optimization,
such as direct preference optimization with UltraFeedback. This advantage is
evident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Link: https://magpie-align.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-World Cooking Robot System from Recipes Based on Food State
  Recognition Using Foundation Models and PDDL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02874v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02874v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoaki Kanazawa, Kento Kawaharazuka, Yoshiki Obinata, Kei Okada, Masayuki Inaba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although there is a growing demand for cooking behaviours as one of the
expected tasks for robots, a series of cooking behaviours based on new recipe
descriptions by robots in the real world has not yet been realised. In this
study, we propose a robot system that integrates real-world executable robot
cooking behaviour planning using the Large Language Model (LLM) and classical
planning of PDDL descriptions, and food ingredient state recognition learning
from a small number of data using the Vision-Language model (VLM). We succeeded
in experiments in which PR2, a dual-armed wheeled robot, performed cooking from
arranged new recipes in a real-world environment, and confirmed the
effectiveness of the proposed system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Advanced Robotics, website -
  https://kanazawanaoaki.github.io/cook-from-recipe-pddl/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpinQuant: LLM quantization with learned rotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16406v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16406v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, Tijmen Blankevoort
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization (PTQ) techniques applied to weights, activations,
and the KV cache greatly reduce memory usage, latency, and power consumption of
Large Language Models (LLMs), but may lead to large quantization errors when
outliers are present. Rotating activation or weight matrices helps remove
outliers and benefits quantization. In this work, we identify a collection of
applicable rotation parameterizations that lead to identical outputs in
full-precision Transformer architectures while enhancing quantization accuracy.
In addition, we find that some random rotations lead to much better
quantization than others, with an up to 13 points difference in downstream
zero-shot reasoning performance. As a result, we propose SpinQuant, a novel
approach that incorporates learned rotation matrices for optimal quantized
network accuracy. With 4-bit quantization of weight, activation, and KV-cache,
SpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full
precision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by
19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also
outperforms concurrent work QuaRot, which applies random rotations to remove
outliers. In particular, for LLaMA-3 8B models that are hard to quantize,
SpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Software Engineering <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Block MedCare: Advancing healthcare through blockchain integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Simonoski, Dijana Capeska Bogatinoska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an era driven by information exchange, transparency and security hold
crucial importance, particularly within the healthcare industry, where data
integrity and confidentiality are paramount. This paper investigates the
integration of blockchain technology in healthcare, focusing on its potential
to revolutionize Electronic Health Records (EHR) management and data sharing.
By leveraging Ethereum-based blockchain implementations and smart contracts, we
propose a novel system that empowers patients to securely store and manage
their medical data. Our research addresses critical challenges in implementing
blockchain in healthcare, including scalability, user privacy, and regulatory
compliance. We propose a solution that combines digital signatures, Role-Based
Access Control, and a multi-layered architecture to enhance security and ensure
controlled access. The system's key functions, including user registration,
data append, and data retrieval, are facilitated through smart contracts,
providing a secure and efficient mechanism for managing health information. To
validate our approach, we developed a decentralized application (dApp) that
demonstrates the practical implementation of our blockchain-based healthcare
solution. The dApp incorporates user-friendly interfaces for patients, doctors,
and administrators, showcasing the system's potential to streamline healthcare
processes while maintaining data security and integrity. Additionally, we
conducted a survey to gain insights into the perceived benefits and challenges
of blockchain adoption in healthcare. The results indicate strong interest
among healthcare professionals and IT experts, while also highlighting concerns
about integration costs and technological complexity. Our findings...
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Modern and Lightweight Rendering Engine for Dynamic Robotic
  Simulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher John Allison, Haoying Zhou, Adnan Munawar, Peter Kazanzides, Juan Antonio Barragan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive dynamic simulators are an accelerator for developing novel
robotic control algorithms and complex systems involving humans and robots. In
user training and synthetic data generation applications, a high-fidelity
visualization of the simulation is essential. Visual fidelity is dependent on
the quality of the computer graphics algorithms used to render the simulated
scene. Furthermore, the rendering algorithms must be implemented on the
graphics processing unit (GPU) to achieve real-time performance, requiring the
use of a graphics application programming interface (API). This paper presents
a performance-focused and lightweight rendering engine supporting the Vulkan
graphics API. The engine is designed to modernize the legacy rendering pipeline
of Asynchronous Multi-Body Framework (AMBF), a dynamic simulation framework
used extensively for interactive robotics simulation development. This new
rendering engine implements graphical features such as physically based
rendering (PBR), anti-aliasing, and ray-traced shadows, significantly improving
the image quality of AMBF. Computational experiments show that the engine can
render a simulated scene with over seven million triangles while maintaining
GPU computation times within two milliseconds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures, submitted to the 2024 IEEE International
  Conference on Robotic Computing (IRC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Finding Safety Violations of AI-Enabled Control Systems through the Lens
  of Synthesized Proxy Programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieke Shi, Zhou Yang, Junda He, Bowen Xu, Dongsun Kim, DongGyun Han, David Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the increasing adoption of modern AI-enabled control systems, ensuring
their safety and reliability has become a critical task in software testing.
One prevalent approach to testing control systems is falsification, which aims
to find an input signal that causes the control system to violate a formal
safety specification using optimization algorithms. However, applying
falsification to AI-enabled control systems poses two significant challenges:
(1)~it requires the system to execute numerous candidate test inputs, which can
be time-consuming, particularly for systems with AI models that have many
parameters, and (2)~multiple safety requirements are typically defined as a
conjunctive specification, which is difficult for existing falsification
approaches to comprehensively cover.
  This paper introduces Synthify, a falsification framework tailored for
AI-enabled control systems. Our approach performs falsification in a two-phase
process. At the start, Synthify synthesizes a program that implements one or a
few linear controllers to serve as a proxy for the AI controller. This proxy
program mimics the AI controller's functionality but is computationally more
efficient. Then, Synthify employs the $\epsilon$-greedy strategy to sample a
promising sub-specification from the conjunctive safety specification. It then
uses a Simulated Annealing-based falsification algorithm to find violations of
the sampled sub-specification for the control system. To evaluate Synthify, we
compare it to PSY-TaLiRo, a state-of-the-art and industrial-strength
falsification tool, on 8 publicly available control systems. On average,
Synthify achieves a 83.5% higher success rate in falsification compared to
PSY-TaLiRo with the same budget of falsification trials. The safety violations
found by Synthify are also more diverse than those found by PSY-TaLiRo,
covering 137.7% more sub-specifications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review by ACM Transactions on Software Engineering and
  Methodology (TOSEM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction
  Diversity on Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Zhang, Justin Wang, Francois Charton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and accurately following instructions is critical for large
language models (LLMs) to be effective across diverse tasks. In this work, we
rigorously examine the key factors that enable models to generalize to unseen
instructions, providing insights to guide the collection of data for
instruction-tuning. Through controlled experiments, inspired by the
Turing-complete Markov algorithm, we demonstrate that such generalization
$\textbf{only emerges}$ when training data is diversified enough across
semantic domains. Our findings also reveal that merely diversifying within
limited domains fails to ensure robust generalization. In contrast,
cross-domain data diversification, even under constrained data budgets,
significantly enhances a model's adaptability. We further extend our analysis
to real-world scenarios, including fine-tuning of
$\textit{$\textbf{specialist}$}$ and $\textit{$\textbf{generalist}$}$ models.
In both cases, we demonstrate that 1) better performance can be achieved by
increasing the diversity of an established dataset while keeping the data size
constant, and 2) when scaling up the data, diversifying the semantics of
instructions is more effective than simply increasing the quantity of similar
data. Our research provides important insights for dataset collation,
particularly when optimizing model performance by expanding training data for
both specialist and generalist scenarios. We show that careful consideration of
data diversification is key: training specialist models with data extending
beyond their core domain leads to significant performance improvements, while
generalist models benefit from diverse data mixtures that enhance their overall
instruction-following capabilities across a wide range of applications. Our
results highlight the critical role of strategic diversification and offer
clear guidelines for improving data quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deciphering Refactoring Branch Dynamics in Modern Code <span class="highlight-title">Review</span>: An
  Empirical Study on Qt 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eman Abdullah AlOmar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context: Modern code review is a widely employed technique in both industrial
and open-source projects, serving to enhance software quality, share knowledge,
and ensure compliance with coding standards and guidelines. While code review
is extensively studied for its general challenges, best practices, outcomes,
and socio-technical aspects, little attention has been paid to how refactoring
is reviewed and what developers prioritize when reviewing refactored code in
the Refactor branch. Objective: The goal is to understand the review process
for refactoring changes in the Refactor branch and to identify what developers
care about when reviewing code in this branch. Method: In this study, we
present a quantitative and qualitative examination to understand the main
criteria developers use to decide whether to accept or reject refactored code
submissions and identify the challenges inherent in this process. Results:
Analyzing 2,154 refactoring and non-refactoring reviews across Qt open-source
projects, we find that reviews involving refactoring from the Refactor branch
take significantly less time to resolve in terms of code review efforts.
Additionally, documentation of developer intent is notably sparse within the
Refactor branch compared to other branches. Furthermore, through thematic
analysis of a substantial sample of refactoring code review discussions, we
construct a comprehensive taxonomy consisting of 12 refactoring review
criteria.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2203.14404</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BigCodeBench: Benchmarking Code Generation with Diverse Function Calls
  and Complex Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15877v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15877v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, David Lo, Binyuan Hui, Niklas Muennighoff, Daniel Fried, Xiaoning Du, Harm de Vries, Leandro Von Werra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task automation has been greatly empowered by the recent advances in Large
Language Models (LLMs) via Python code, where the tasks ranging from software
engineering development to general-purpose reasoning. While current benchmarks
have shown that LLMs can solve tasks using programs like human developers, the
majority of their evaluations are limited to short and self-contained
algorithmic tasks or standalone function calls. Solving challenging and
practical requires the capability of utilizing diverse function calls as tools
to efficiently implement functionalities like data analysis and web
development. In addition, using multiple tools to solve a task needs
compositional reasoning by accurately understanding complex instructions.
Fulfilling both of these characteristics can pose a great challenge for LLMs.To
assess how well LLMs can solve challenging and practical tasks via programs, we
introduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple
function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained
tasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with
an average branch coverage of 99%. In addition, we propose a
natural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that
automatically transforms the original docstrings into short instructions only
with essential information. Our extensive evaluation of 60 LLMs shows that LLMs
are not yet capable of following complex instructions to use function calls
precisely, with scores up to 60%, significantly lower than the human
performance of 97%. The results underscore the need for further advancements in
this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 14 figures, 7 tables, built with love by the BigCode
  community :)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Full Line Code Completion: Bringing AI to Desktop 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.08704v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.08704v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Semenkin, Vitaliy Bibaev, Yaroslav Sokolov, Kirill Krylov, Alexey Kalina, Anna Khannanova, Danila Savenkov, Darya Rovdo, Igor Davidenko, Kirill Karnaukhov, Maxim Vakhrushev, Mikhail Kostyukov, Mikhail Podvitskii, Petr Surkov, Yaroslav Golubev, Nikita Povarov, Timofey Bryksin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, several industrial solutions for the problem of multi-token
code completion appeared, each making a great advance in the area but mostly
focusing on cloud-based runtime and avoiding working on the end user's device.
  In this work, we describe our approach for building a multi-token code
completion feature for the JetBrains' IntelliJ Platform, which we call Full
Line Code Completion. The feature suggests only syntactically correct code and
works fully locally, i.e., data querying and the generation of suggestions
happens on the end user's machine. We share important time and
memory-consumption restrictions, as well as design principles that a code
completion engine should satisfy. Working entirely on the end user's device,
our code completion engine enriches user experience while being not only fast
and compact but also secure. We share a number of useful techniques to meet the
stated development constraints and also describe offline and online evaluation
pipelines that allowed us to make better decisions.
  Our online evaluation shows that the usage of the tool leads to 1.3 times
more Python code in the IDE being produced by code completion. The described
solution was initially started with a help of researchers and was then bundled
into all JetBrains IDEs where it is now used by millions of users. Thus, we
believe that this work is useful for bridging academia and industry, providing
researchers with the knowledge of what happens when complex research-based
solutions are integrated into real products.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeBS-Flow: Benchmarking Serverless Cloud Function Workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03480v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03480v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Larissa Schmid, Marcin Copik, Alexandru Calotoiu, Laurin Brandner, Anne Koziolek, Torsten Hoefler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Serverless computing has emerged as a prominent paradigm, with a significant
adoption rate among cloud customers. While this model offers advantages such as
abstraction from the deployment and resource scheduling, it also poses
limitations in handling complex use cases due to the restricted nature of
individual functions. Serverless workflows address this limitation by
orchestrating multiple functions into a cohesive application. However, existing
serverless workflow platforms exhibit significant differences in their
programming models and infrastructure, making fair and consistent performance
evaluations difficult in practice. To address this gap, we propose the first
serverless workflow benchmarking suite SeBS-Flow, providing a platform-agnostic
workflow model that enables consistent benchmarking across various platforms.
SeBS-Flow includes six real-world application benchmarks and four
microbenchmarks representing different computational patterns. We conduct
comprehensive evaluations on three major cloud platforms, assessing
performance, cost, scalability, and runtime deviations. We make our benchmark
suite open-source, enabling rigorous and comparable evaluations of serverless
workflows over time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model for Vulnerability Detection and Repair: Literature
  <span class="highlight-title">Review</span> and the Road Ahead 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02525v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02525v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhou, Sicong Cao, Xiaobing Sun, David Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The significant advancements in Large Language Models (LLMs) have resulted in
their widespread adoption across various tasks within Software Engineering
(SE), including vulnerability detection and repair. Numerous studies have
investigated the application of LLMs to enhance vulnerability detection and
repair tasks. Despite the increasing research interest, there is currently no
existing survey that focuses on the utilization of LLMs for vulnerability
detection and repair. In this paper, we aim to bridge this gap by offering a
systematic literature review of approaches aimed at improving vulnerability
detection and repair through the utilization of LLMs. The review encompasses
research work from leading SE, AI, and Security conferences and journals,
encompassing 43 papers published across 25 distinct venues, along with 15
high-quality preprint papers, bringing the total to 58 papers. By answering
three key research questions, we aim to (1) summarize the LLMs employed in the
relevant literature, (2) categorize various LLM adaptation techniques in
vulnerability detection, and (3) classify various LLM adaptation techniques in
vulnerability repair. Based on our findings, we have identified a series of
limitations of existing studies. Additionally, we have outlined a roadmap
highlighting potential opportunities that we believe are pertinent and crucial
for future research endeavors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-Based Multi-Agent Systems for Software Engineering: Vision and the
  Road Ahead 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04834v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04834v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junda He, Christoph Treude, David Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating Large Language Models(LLMs) into autonomous agents marks a
significant shift in the research landscape by offering cognitive abilities
competitive to human planning and reasoning. This paper envisions the evolution
of LLM-based Multi-Agent (LMA) systems in addressing complex and multi-faceted
software engineering challenges. LMA systems introduce numerous benefits,
including enhanced robustness through collaborative cross-examination,
autonomous problem-solving, and scalable solutions to complex software
projects. By examining the role of LMA systems in future software engineering
practices, this vision paper highlights the potential applications and emerging
challenges. We further point to specific opportunities for research and
conclude with a research agenda with a set of research questions to guide
future research directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">139</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Tuning CLIP's Last Visual Projector: A Few-Shot Cornucopia 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick Pérez, Raoul de Charette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of adapting a contrastively pretrained
vision-language model like CLIP (Radford et al., 2021) for few-shot
classification. The existing literature addresses this problem by learning a
linear classifier of the frozen visual features, optimizing word embeddings, or
learning external feature adapters. This paper introduces an alternative way
for CLIP adaptation without adding 'external' parameters to optimize. We find
that simply fine-tuning the last projection matrix of the vision encoder leads
to strong performance compared to the existing baselines. Furthermore, we show
that regularizing training with the distance between the fine-tuned and
pretrained matrices adds reliability for adapting CLIP through this layer.
Perhaps surprisingly, this approach, coined ProLIP, yields performances on par
or better than state of the art on 11 few-shot classification benchmarks,
few-shot domain generalization, cross-dataset transfer and test-time
adaptation. Code will be made available at
https://github.com/astra-vision/ProLIP .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint,under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding Partially-Defined Events in Multimodal Data <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kate Sanders, Reno Kriz, David Etter, Hannah Recknor, Alexander Martin, Cameron Carpenter, Jingyang Lin, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How are we able to learn about complex current events just from short
snippets of video? While natural language enables straightforward ways to
represent under-specified, partially observable events, visual data does not
facilitate analogous methods and, consequently, introduces unique challenges in
event understanding. With the growing prevalence of vision-capable AI agents,
these systems must be able to model events from collections of unstructured
video data. To tackle robust event modeling in multimodal settings, we
introduce a multimodal formulation for partially-defined events and cast the
extraction of these events as a three-stage span retrieval task. We propose a
corresponding benchmark for this task, MultiVENT-G, that consists of 14.5 hours
of densely annotated current event videos and 1,168 text documents, containing
22.8K labeled event-centric entities. We propose a collection of LLM-driven
approaches to the task of multimodal event analysis, and evaluate them on
MultiVENT-G. Results illustrate the challenges that abstract event
understanding poses and demonstrates promise in event-centric video-language
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint; 9 pages; 2024 EMNLP Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Brain Mapping with Dense Features: Grounding Cortical Semantic
  Selectivity in Natural Images With Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew F. Luo, Jacob Yeung, Rushikesh Zawar, Shaurya Dewan, Margaret M. Henderson, Leila Wehbe, Michael J. Tarr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in large-scale artificial neural networks have facilitated novel
insights into the functional topology of the brain. Here, we leverage this
approach to study how semantic categories are organized in the human visual
cortex. To overcome the challenge presented by the co-occurrence of multiple
categories in natural images, we introduce BrainSAIL (Semantic Attribution and
Image Localization), a method for isolating specific neurally-activating visual
concepts in images. BrainSAIL exploits semantically consistent, dense spatial
features from pre-trained vision models, building upon their demonstrated
ability to robustly predict neural activity. This method derives clean,
spatially dense embeddings without requiring any additional training, and
employs a novel denoising process that leverages the semantic consistency of
images under random augmentations. By unifying the space of whole-image
embeddings and dense visual features and then applying voxel-wise encoding
models to these features, we enable the identification of specific subregions
of each image which drive selectivity patterns in different areas of the higher
visual cortex. We validate BrainSAIL on cortical regions with known category
selectivity, demonstrating its ability to accurately localize and disentangle
selectivity to diverse visual concepts. Next, we demonstrate BrainSAIL's
ability to characterize high-level visual selectivity to scene properties and
low-level visual features such as depth, luminance, and saturation, providing
insights into the encoding of complex visual information. Finally, we use
BrainSAIL to directly compare the feature selectivity of different brain
encoding models across different regions of interest in visual cortex. Our
innovative method paves the way for significant advances in mapping and
decomposing high-level visual representations in the human brain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TextHawk2: A Large Vision-Language Model Excels in Bilingual OCR and
  Grounding with 16x Fewer Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ya-Qi Yu, Minghui Liao, Jiwen Zhang, Jihao Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reading dense text and locating objects within images are fundamental
abilities for Large Vision-Language Models (LVLMs) tasked with advanced jobs.
Previous LVLMs, including superior proprietary models like GPT-4o, have
struggled to excel in both tasks simultaneously. Moreover, previous LVLMs with
fine-grained perception cost thousands of tokens per image, making them
resource-intensive. We present TextHawk2, a bilingual LVLM featuring efficient
fine-grained perception and demonstrating cutting-edge performance across
general-purpose, OCR, and grounding tasks with 16 times fewer image tokens.
Critical improvements include: (1) Token Compression: Building on the efficient
architecture of its predecessor, TextHawk2 significantly reduces the number of
tokens per image by 16 times, facilitating training and deployment of the
TextHawk series with minimal resources. (2) Visual Encoder Reinforcement: We
enhance the visual encoder through LVLM co-training, unlocking its potential
for previously unseen tasks like Chinese OCR and grounding. (3) Data Diversity:
We maintain a comparable scale of 100 million samples while diversifying the
sources of pre-training data. We assess TextHawk2 across multiple benchmarks,
where it consistently delivers superior performance and outperforms
closed-source models of similar scale, such as achieving 78.4% accuracy on
OCRBench, 81.4% accuracy on ChartQA, 89.6% ANLS on DocVQA, and 88.1%
accuracy@0.5 on RefCOCOg-test.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DART: A Diffusion-Based Autoregressive Motion Model for Real-Time
  Text-Driven Motion Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaifeng Zhao, Gen Li, Siyu Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-conditioned human motion generation, which allows for user interaction
through natural language, has become increasingly popular. Existing methods
typically generate short, isolated motions based on a single input sentence.
However, human motions are continuous and can extend over long periods,
carrying rich semantics. Creating long, complex motions that precisely respond
to streams of text descriptions, particularly in an online and real-time
setting, remains a significant challenge. Furthermore, incorporating spatial
constraints into text-conditioned motion generation presents additional
challenges, as it requires aligning the motion semantics specified by text
descriptions with geometric information, such as goal locations and 3D scene
geometry. To address these limitations, we propose DART, a Diffusion-based
Autoregressive motion primitive model for Real-time Text-driven motion control.
Our model, DART, effectively learns a compact motion primitive space jointly
conditioned on motion history and text inputs using latent diffusion models. By
autoregressively generating motion primitives based on the preceding history
and current text input, DART enables real-time, sequential motion generation
driven by natural language descriptions. Additionally, the learned motion
primitive space allows for precise spatial motion control, which we formulate
either as a latent noise optimization problem or as a Markov decision process
addressed through reinforcement learning. We present effective algorithms for
both approaches, demonstrating our model's versatility and superior performance
in various motion synthesis tasks. Experiments show our method outperforms
existing baselines in motion realism, efficiency, and controllability. Video
results are available on the project page: https://zkf1997.github.io/DART/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GS-VTON: Controllable 3D Virtual Try-on with Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukang Cao, Masoud Hadi, Liang Pan, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based 2D virtual try-on (VTON) techniques have recently
demonstrated strong performance, while the development of 3D VTON has largely
lagged behind. Despite recent advances in text-guided 3D scene editing,
integrating 2D VTON into these pipelines to achieve vivid 3D VTON remains
challenging. The reasons are twofold. First, text prompts cannot provide
sufficient details in describing clothing. Second, 2D VTON results generated
from different viewpoints of the same 3D scene lack coherence and spatial
relationships, hence frequently leading to appearance inconsistencies and
geometric distortions. To resolve these problems, we introduce an
image-prompted 3D VTON method (dubbed GS-VTON) which, by leveraging 3D Gaussian
Splatting (3DGS) as the 3D representation, enables the transfer of pre-trained
knowledge from 2D VTON models to 3D while improving cross-view consistency. (1)
Specifically, we propose a personalized diffusion model that utilizes low-rank
adaptation (LoRA) fine-tuning to incorporate personalized information into
pre-trained 2D VTON models. To achieve effective LoRA training, we introduce a
reference-driven image editing approach that enables the simultaneous editing
of multi-view images while ensuring consistency. (2) Furthermore, we propose a
persona-aware 3DGS editing framework to facilitate effective editing while
maintaining consistent cross-view appearance and high-quality 3D geometry. (3)
Additionally, we have established a new 3D VTON benchmark, 3D-VTONBench, which
facilitates comprehensive qualitative and quantitative 3D VTON evaluations.
Through extensive experiments and comparative analyses with existing methods,
the proposed \OM has demonstrated superior fidelity and advanced editing
capabilities, affirming its effectiveness for 3D VTON.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SePPO: Semi-Policy Preference Optimization for Diffusion Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daoan Zhang, Guangchen Lan, Dong-Jun Han, Wenlin Yao, Xiaoman Pan, Hongming Zhang, Mingxiao Li, Pengcheng Chen, Yu Dong, Christopher Brinton, Jiebo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning from human feedback (RLHF) methods are emerging as a
way to fine-tune diffusion models (DMs) for visual generation. However,
commonly used on-policy strategies are limited by the generalization capability
of the reward model, while off-policy approaches require large amounts of
difficult-to-obtain paired human-annotated data, particularly in visual
generation tasks. To address the limitations of both on- and off-policy RLHF,
we propose a preference optimization method that aligns DMs with preferences
without relying on reward models or paired human-annotated data. Specifically,
we introduce a Semi-Policy Preference Optimization (SePPO) method. SePPO
leverages previous checkpoints as reference models while using them to generate
on-policy reference samples, which replace "losing images" in preference pairs.
This approach allows us to optimize using only off-policy "winning images."
Furthermore, we design a strategy for reference model selection that expands
the exploration in the policy space. Notably, we do not simply treat reference
samples as negative examples for learning. Instead, we design an anchor-based
criterion to assess whether the reference samples are likely to be winning or
losing images, allowing the model to selectively learn from the generated
reference samples. This approach mitigates performance degradation caused by
the uncertainty in reference sample quality. We validate SePPO across both
text-to-image and text-to-video benchmarks. SePPO surpasses all previous
approaches on the text-to-image benchmarks and also demonstrates outstanding
performance on the text-to-video benchmarks. Code will be released in
https://github.com/DwanZhang-AI/SePPO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoTLIP: Improving Language-Image <span class="highlight-title">Pre-train</span>ing for Long Text
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Wu, Kecheng Zheng, Shuailei Ma, Fan Lu, Yuxin Guo, Yifei Zhang, Wei Chen, Qingpei Guo, Yujun Shen, Zheng-Jun Zha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding long text is of great demands in practice but beyond the reach
of most language-image pre-training (LIP) models. In this work, we empirically
confirm that the key reason causing such an issue is that the training images
are usually paired with short captions, leaving certain tokens easily
overshadowed by salient tokens. Towards this problem, our initial attempt is to
relabel the data with long captions, however, directly learning with which may
lead to performance degradation in understanding short text (e.g., in the image
classification task). Then, after incorporating corner tokens to aggregate
diverse textual information, we manage to help the model catch up to its
original level of short text understanding yet greatly enhance its capability
of long text understanding. We further look into whether the model can
continuously benefit from longer captions and notice a clear trade-off between
the performance and the efficiency. Finally, we validate the effectiveness of
our approach using a self-constructed large-scale dataset, which consists of
100M long caption oriented text-image pairs. It is noteworthy that, on the task
of long-text image retrieval, we beat the competitor using long captions with
11.1% improvement (i.e., from 72.62% to 83.72%). We will release the code, the
model, and the new dataset to facilitate the reproducibility and further
research. The project page is available at https://wuw2019.github.io/lotlip.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Navigating the Digital World as Humans Do: Universal Visual Grounding
  for GUI Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) are transforming the capabilities of
graphical user interface (GUI) agents, facilitating their transition from
controlled simulations to complex, real-world applications across various
platforms. However, the effectiveness of these agents hinges on the robustness
of their grounding capability. Current GUI agents predominantly utilize
text-based representations such as HTML or accessibility trees, which, despite
their utility, often introduce noise, incompleteness, and increased
computational overhead. In this paper, we advocate a human-like embodiment for
GUI agents that perceive the environment entirely visually and directly take
pixel-level operations on the GUI. The key is visual grounding models that can
accurately map diverse referring expressions of GUI elements to their
coordinates on the GUI across different platforms. We show that a simple
recipe, which includes web-based synthetic data and slight adaptation of the
LLaVA architecture, is surprisingly effective for training such visual
grounding models. We collect the largest dataset for GUI visual grounding so
far, containing 10M GUI elements and their referring expressions over 1.3M
screenshots, and use it to train UGround, a strong universal visual grounding
model for GUI agents. Empirical results on six benchmarks spanning three
categories (grounding, offline agent, and online agent) show that 1) UGround
substantially outperforms existing visual grounding models for GUI agents, by
up to 20% absolute, and 2) agents with UGround outperform state-of-the-art
agents, despite the fact that existing agents use additional text-based input
while ours only uses visual perception. These results provide strong support
for the feasibility and promises of GUI agents that navigate the digital world
as humans do.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TuneVLSeg: <span class="highlight-title">Prompt</span> Tuning Benchmark for Vision-Language Segmentation
  Models <span class="chip">ACCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rabin Adhikari, Safal Thapaliya, Manish Dhakal, Bishesh Khanal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) have shown impressive performance in vision
tasks, but adapting them to new domains often requires expensive fine-tuning.
Prompt tuning techniques, including textual, visual, and multimodal prompting,
offer efficient alternatives by leveraging learnable prompts. However, their
application to Vision-Language Segmentation Models (VLSMs) and evaluation under
significant domain shifts remain unexplored. This work presents an open-source
benchmarking framework, TuneVLSeg, to integrate various unimodal and multimodal
prompt tuning techniques into VLSMs, making prompt tuning usable for downstream
segmentation datasets with any number of classes. TuneVLSeg includes $6$ prompt
tuning strategies on various prompt depths used in $2$ VLSMs totaling of $8$
different combinations. We test various prompt tuning on $8$ diverse medical
datasets, including $3$ radiology datasets (breast tumor, echocardiograph,
chest X-ray pathologies) and $5$ non-radiology datasets (polyp, ulcer, skin
cancer), and two natural domain segmentation datasets. Our study found that
textual prompt tuning struggles under significant domain shifts, from
natural-domain images to medical data. Furthermore, visual prompt tuning, with
fewer hyperparameters than multimodal prompt tuning, often achieves performance
competitive to multimodal approaches, making it a valuable first attempt. Our
work advances the understanding and applicability of different prompt-tuning
techniques for robust domain-specific segmentation. The source code is
available at https://github.com/naamiinepal/tunevlseg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACCV 2024 (oral presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffuseReg: Denoising Diffusion Model for Obtaining Deformation Fields
  in Unsupervised Deformable Image Registration <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongtai Zhuo, Yiqing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deformable image registration aims to precisely align medical images from
different modalities or times. Traditional deep learning methods, while
effective, often lack interpretability, real-time observability and adjustment
capacity during registration inference. Denoising diffusion models present an
alternative by reformulating registration as iterative image denoising.
However, existing diffusion registration approaches do not fully harness
capabilities, neglecting the critical sampling phase that enables continuous
observability during the inference. Hence, we introduce DiffuseReg, an
innovative diffusion-based method that denoises deformation fields instead of
images for improved transparency. We also propose a novel denoising network
upon Swin Transformer, which better integrates moving and fixed images with
diffusion time step throughout the denoising process. Furthermore, we enhance
control over the denoising registration process with a novel similarity
consistency regularization. Experiments on ACDC datasets demonstrate DiffuseReg
outperforms existing diffusion registration methods by 1.32 in Dice score. The
sampling process in DiffuseReg enables real-time output observability and
adjustment unmatched by previous deep models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2024, W-AM-067, https://github.com/YutaZhuo/DiffuseReg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimO Loss: Anchor-Free Contrastive Loss for Fine-Grained Supervised
  Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha Bouhsine, Imad El Aaroussi, Atik Faysal, Wang Huaxia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel anchor-free contrastive learning (AFCL) method
leveraging our proposed Similarity-Orthogonality (SimO) loss. Our approach
minimizes a semi-metric discriminative loss function that simultaneously
optimizes two key objectives: reducing the distance and orthogonality between
embeddings of similar inputs while maximizing these metrics for dissimilar
inputs, facilitating more fine-grained contrastive learning. The AFCL method,
powered by SimO loss, creates a fiber bundle topological structure in the
embedding space, forming class-specific, internally cohesive yet orthogonal
neighborhoods. We validate the efficacy of our method on the CIFAR-10 dataset,
providing visualizations that demonstrate the impact of SimO loss on the
embedding space. Our results illustrate the formation of distinct, orthogonal
class neighborhoods, showcasing the method's ability to create well-structured
embeddings that balance class separation with intra-class variability. This
work opens new avenues for understanding and leveraging the geometric
properties of learned representations in various machine learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Dawn of Video Generation: Preliminary Explorations with SORA-like
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ailing Zeng, Yuhang Yang, Weidong Chen, Wei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality video generation, encompassing text-to-video (T2V),
image-to-video (I2V), and video-to-video (V2V) generation, holds considerable
significance in content creation to benefit anyone express their inherent
creativity in new ways and world simulation to modeling and understanding the
world. Models like SORA have advanced generating videos with higher resolution,
more natural motion, better vision-language alignment, and increased
controllability, particularly for long video sequences. These improvements have
been driven by the evolution of model architectures, shifting from UNet to more
scalable and parameter-rich DiT models, along with large-scale data expansion
and refined training strategies. However, despite the emergence of DiT-based
closed-source and open-source models, a comprehensive investigation into their
capabilities and limitations remains lacking. Furthermore, the rapid
development has made it challenging for recent benchmarks to fully cover
SORA-like models and recognize their significant advancements. Additionally,
evaluation metrics often fail to align with human preferences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project: https://ailab-cvc.github.io/VideoGen-Eval/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Precise Model Benchmarking with Only a Few Observations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Fogliato, Pratik Patil, Nil-Jana Akpinar, Mathew Monfort
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we precisely estimate a large language model's (LLM) accuracy on
questions belonging to a specific topic within a larger question-answering
dataset? The standard direct estimator, which averages the model's accuracy on
the questions in each subgroup, may exhibit high variance for subgroups
(topics) with small sample sizes. Synthetic regression modeling, which
leverages the model's accuracy on questions about other topics, may yield
biased estimates that are too unreliable for large subgroups. We prescribe a
simple yet effective solution: an empirical Bayes (EB) estimator that balances
direct and regression estimates for each subgroup separately, improving the
precision of subgroup-level estimates of model performance. Our experiments on
multiple datasets show that this approach consistently provides more precise
estimates of the LLM performance compared to the direct and regression
approaches, achieving substantial reductions in the mean squared error.
Confidence intervals for EB estimates also have near-nominal coverage and are
narrower compared to those for the direct estimator. Additional experiments on
tabular and vision data validate the benefits of this EB approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Organizing Unstructured Image Collections using Natural Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxuan Liu, Zhun Zhong, Jun Li, Gianni Franchi, Subhankar Roy, Elisa Ricci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Organizing unstructured visual data into semantic clusters is a key challenge
in computer vision. Traditional deep clustering (DC) approaches focus on a
single partition of data, while multiple clustering (MC) methods address this
limitation by uncovering distinct clustering solutions. The rise of large
language models (LLMs) and multimodal LLMs (MLLMs) has enhanced MC by allowing
users to define clustering criteria in natural language. However, manually
specifying criteria for large datasets is impractical. In this work, we
introduce the task Semantic Multiple Clustering (SMC) that aims to
automatically discover clustering criteria from large image collections,
uncovering interpretable substructures without requiring human input. Our
framework, Text Driven Semantic Multiple Clustering (TeDeSC), uses text as a
proxy to concurrently reason over large image collections, discover
partitioning criteria, expressed in natural language, and reveal semantic
substructures. To evaluate TeDeSC, we introduce the COCO-4c and Food-4c
benchmarks, each containing four grouping criteria and ground-truth
annotations. We apply TeDeSC to various applications, such as discovering
biases and analyzing social media image popularity, demonstrating its utility
as a tool for automatically organizing image collections and revealing novel
insights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Project webpage: https://oatmealliu.github.io/smc.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preserving Multi-Modal Capabilities of <span class="highlight-title">Pre-train</span>ed VLMs for Improving
  Vision-Linguistic Compositionality <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngtaek Oh, Jae Won Cho, Dong-Jin Kim, In So Kweon, Junmo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new method to enhance compositional understanding
in pre-trained vision and language models (VLMs) without sacrificing
performance in zero-shot multi-modal tasks. Traditional fine-tuning approaches
often improve compositional reasoning at the cost of degrading multi-modal
capabilities, primarily due to the use of global hard negative (HN) loss, which
contrasts global representations of images and texts. This global HN loss
pushes HN texts that are highly similar to the original ones, damaging the
model's multi-modal representations. To overcome this limitation, we propose
Fine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard
negative loss and selective calibrated regularization. These innovations
provide fine-grained negative supervision while preserving the model's
representational integrity. Our extensive evaluations across diverse benchmarks
for both compositionality and multi-modal tasks show that FSC-CLIP not only
achieves compositionality on par with state-of-the-art models but also retains
strong multi-modal capabilities. Code is available at:
https://github.com/ytaek-oh/fsc-clip.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Long, Main). Project page:
  https://ytaek-oh.github.io/fsc-clip</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Studying and Mitigating Biases in Sign Language Understanding Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katherine Atwell, Danielle Bragg, Malihe Alikhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring that the benefits of sign language technologies are distributed
equitably among all community members is crucial. Thus, it is important to
address potential biases and inequities that may arise from the design or use
of these resources. Crowd-sourced sign language datasets, such as the ASL
Citizen dataset, are great resources for improving accessibility and preserving
linguistic diversity, but they must be used thoughtfully to avoid reinforcing
existing biases.
  In this work, we utilize the rich information about participant demographics
and lexical features present in the ASL Citizen dataset to study and document
the biases that may result from models trained on crowd-sourced sign datasets.
Further, we apply several bias mitigation techniques during model training, and
find that these techniques reduce performance disparities without decreasing
accuracy. With the publication of this work, we release the demographic
information about the participants in the ASL Citizen dataset to encourage
future bias mitigation work in this space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond FVD: Enhanced Evaluation Metrics for Video Generation Quality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Ya,  Luo, Gian Favero, Zhi Hao Luo, Alexia Jolicoeur-Martineau, Christopher Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Fr\'echet Video Distance (FVD) is a widely adopted metric for evaluating
video generation distribution quality. However, its effectiveness relies on
critical assumptions. Our analysis reveals three significant limitations: (1)
the non-Gaussianity of the Inflated 3D Convnet (I3D) feature space; (2) the
insensitivity of I3D features to temporal distortions; (3) the impractical
sample sizes required for reliable estimation. These findings undermine FVD's
reliability and show that FVD falls short as a standalone metric for video
generation evaluation. After extensive analysis of a wide range of metrics and
backbone architectures, we propose JEDi, the JEPA Embedding Distance, based on
features derived from a Joint Embedding Predictive Architecture, measured using
Maximum Mean Discrepancy with polynomial kernel. Our experiments on multiple
open-source datasets show clear evidence that it is a superior alternative to
the widely used FVD metric, requiring only 16% of the samples to reach its
steady value, while increasing alignment with human evaluation by 34%, on
average.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MARs: Multi-view Attention Regularizations for Patch-based Feature
  Recognition of Space Terrain <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothy Chase Jr, Karthik Dantu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The visual detection and tracking of surface terrain is required for
spacecraft to safely land on or navigate within close proximity to celestial
objects. Current approaches rely on template matching with pre-gathered
patch-based features, which are expensive to obtain and a limiting factor in
perceptual capability. While recent literature has focused on in-situ detection
methods to enhance navigation and operational autonomy, robust description is
still needed. In this work, we explore metric learning as the lightweight
feature description mechanism and find that current solutions fail to address
inter-class similarity and multi-view observational geometry. We attribute this
to the view-unaware attention mechanism and introduce Multi-view Attention
Regularizations (MARs) to constrain the channel and spatial attention across
multiple feature views, regularizing the what and where of attention focus. We
thoroughly analyze many modern metric learning losses with and without MARs and
demonstrate improved terrain-feature recognition performance by upwards of 85%.
We additionally introduce the Luna-1 dataset, consisting of Moon crater
landmarks and reference navigation frames from NASA mission data to support
future research in this difficult task. Luna-1 and source code are publicly
available at https://droneslab.github.io/mars/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024. Project page available at
  https://droneslab.github.io/mars/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLM2Vec: Training Vision-Language Models for Massive Multimodal
  Embedding Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding models have been crucial in enabling various downstream tasks such
as semantic similarity, information retrieval, and clustering. Recently, there
has been a surge of interest in developing universal text embedding models that
can generalize across tasks (e.g., MTEB). However, progress in learning
universal multimodal embedding models has been relatively slow despite their
importance. In this work, we aim to explore the potential for building
universal embeddings capable of handling a wide range of downstream tasks. Our
contributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark),
which covers 4 meta-tasks (i.e. classification, visual question answering,
multimodal retrieval, and visual grounding) and 36 datasets, including 20
training and 16 evaluation datasets, and (2) VLM2Vec (Vision-Language Model ->
Vector), a contrastive training framework that converts any state-of-the-art
vision-language model into an embedding model via training on MMEB. Unlike
previous models such as CLIP and BLIP, VLM2Vec can process any combination of
images and text to generate a fixed-dimensional vector based on task
instructions. We build a series of VLM2Vec models on Phi-3.5-V and evaluate
them on MMEB's evaluation split. Our results show that \model achieves an
absolute average improvement of 10% to 20% over existing multimodal embedding
models on both in-distribution and out-of-distribution datasets in MMEB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIBench: A Comprehensive Benchmark for Model Inversion Attack and
  Defense 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiang Qiu, Hongyao Yu, Hao Fang, Wenbo Yu, Bin Chen, Xuan Wang, Shu-Tao Xia, Ke Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model Inversion (MI) attacks aim at leveraging the output information of
target models to reconstruct privacy-sensitive training data, raising
widespread concerns on privacy threats of Deep Neural Networks (DNNs).
Unfortunately, in tandem with the rapid evolution of MI attacks, the lack of a
comprehensive, aligned, and reliable benchmark has emerged as a formidable
challenge. This deficiency leads to inadequate comparisons between different
attack methods and inconsistent experimental setups. In this paper, we
introduce the first practical benchmark for model inversion attacks and
defenses to address this critical gap, which is named \textit{MIBench}. This
benchmark serves as an extensible and reproducible modular-based toolbox and
currently integrates a total of 16 state-of-the-art attack and defense methods.
Moreover, we furnish a suite of assessment tools encompassing 9 commonly used
evaluation protocols to facilitate standardized and fair evaluation and
analysis. Capitalizing on this foundation, we conduct extensive experiments
from multiple perspectives to holistically compare and analyze the performance
of various methods across different scenarios, which overcomes the misalignment
issues and discrepancy prevalent in previous works. Based on the collected
attack methods and defense strategies, we analyze the impact of target
resolution, defense robustness, model predictive power, model architectures,
transferability and loss function. Our hope is that this \textit{MIBench} could
provide a unified, practical and extensible toolbox and is widely utilized by
researchers in the field to rigorously test and compare their novel methods,
ensuring equitable evaluations and thereby propelling further advancements in
the future development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Multimodal Diffusion Models to Accelerate Imaging with Side
  Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timofey Efimov, Harry Dong, Megna Shah, Jeff Simmons, Sean Donegan, Yuejie Chi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have found phenomenal success as expressive priors for
solving inverse problems, but their extension beyond natural images to more
structured scientific domains remains limited. Motivated by applications in
materials science, we aim to reduce the number of measurements required from an
expensive imaging modality of interest, by leveraging side information from an
auxiliary modality that is much cheaper to obtain. To deal with the
non-differentiable and black-box nature of the forward model, we propose a
framework to train a multimodal diffusion model over the joint modalities,
turning inverse problems with black-box forward models into simple linear
inpainting problems. Numerically, we demonstrate the feasibility of training
diffusion models over materials imagery data, and show that our approach
achieves superior image reconstruction by leveraging the available side
information, requiring significantly less amount of data from the expensive
microscopy modality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-Feedback Efficient Reinforcement Learning for Online Diffusion
  Model Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayano Hiranaka, Shang-Fu Chen, Chieh-Hsin Lai, Dongjun Kim, Naoki Murata, Takashi Shibuya, Wei-Hsiang Liao, Shao-Hua Sun, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controllable generation through Stable Diffusion (SD) fine-tuning aims to
improve fidelity, safety, and alignment with human guidance. Existing
reinforcement learning from human feedback methods usually rely on predefined
heuristic reward functions or pretrained reward models built on large-scale
datasets, limiting their applicability to scenarios where collecting such data
is costly or difficult. To effectively and efficiently utilize human feedback,
we develop a framework, HERO, which leverages online human feedback collected
on the fly during model learning. Specifically, HERO features two key
mechanisms: (1) Feedback-Aligned Representation Learning, an online training
method that captures human feedback and provides informative learning signals
for fine-tuning, and (2) Feedback-Guided Image Generation, which involves
generating images from SD's refined initialization samples, enabling faster
convergence towards the evaluator's intent. We demonstrate that HERO is 4x more
efficient in online feedback for body part anomaly correction compared to the
best existing method. Additionally, experiments show that HERO can effectively
handle tasks like reasoning, counting, personalization, and reducing NSFW
content with only 0.5K online feedback.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form
  Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan Reddy Mekala, Frederik Pahde, Simon Baur, Sneha Chandrashekar, Madeline Diep, Markus Wenzel, Eric L. Wisotzky, Galip Ümit Yolcu, Sebastian Lapuschkin, Jackie Ma, Peter Eisert, Mikael Lindvall, Adam Porter, Wojciech Samek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of dermatological diagnoses, where the analysis of dermatoscopic
and microscopic skin lesion images is pivotal for the accurate and early
detection of various medical conditions, the costs associated with creating
diverse and high-quality annotated datasets have hampered the accuracy and
generalizability of machine learning models. We propose an innovative
unsupervised augmentation solution that harnesses Generative Adversarial
Network (GAN) based models and associated techniques over their latent space to
generate controlled semiautomatically-discovered semantic variations in
dermatoscopic images. We created synthetic images to incorporate the semantic
variations and augmented the training data with these images. With this
approach, we were able to increase the performance of machine learning models
and set a new benchmark amongst non-ensemble based models in skin lesion
classification on the HAM10000 dataset; and used the observed analytics and
generated models for detailed studies on model explainability, affirming the
effectiveness of our solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has been submitted to the Workshop on Synthetic Data
  for Computer Vision (SyntheticData4CV 2024 is a side event on 18th European
  Conference on Computer Vision 2024). This preprint has not undergone peer
  review or any post-submission improvements or corrections</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiDAR-GS:Real-time LiDAR Re-Simulation using Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qifeng Chen, Sheng Yang, Sicong Du, Tao Tang, Peng Chen, Yuchi Huo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR simulation plays a crucial role in closed-loop simulation for
autonomous driving. Although recent advancements, such as the use of
reconstructed mesh and Neural Radiance Fields (NeRF), have made progress in
simulating the physical properties of LiDAR, these methods have struggled to
achieve satisfactory frame rates and rendering quality. To address these
limitations, we present LiDAR-GS, the first LiDAR Gaussian Splatting method,
for real-time high-fidelity re-simulation of LiDAR sensor scans in public urban
road scenes. The vanilla Gaussian Splatting, designed for camera models, cannot
be directly applied to LiDAR re-simulation. To bridge the gap between passive
camera and active LiDAR, our LiDAR-GS designs a differentiable laser beam
splatting, grounded in the LiDAR range view model. This innovation allows for
precise surface splatting by projecting lasers onto micro cross-sections,
effectively eliminating artifacts associated with local affine approximations.
Additionally, LiDAR-GS leverages Neural Gaussian Fields, which further
integrate view-dependent clues, to represent key LiDAR properties that are
influenced by the incident angle and external factors. Combining these
practices with some essential adaptations, e.g., dynamic instances
decomposition, our approach succeeds in simultaneously re-simulating depth,
intensity, and ray-drop channels, achieving state-of-the-art results in both
rendering frame rate and quality on publically available large scene datasets.
Our source code will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetaDD: Boosting <span class="highlight-title">Dataset</span> Distillation with Neural Network
  Architecture-Invariant Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunlong Zhao, Xiaoheng Deng, Xiu Su, Hongyan Xu, Xiuxing Li, Yijing Liu, Shan You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset distillation (DD) entails creating a refined, compact distilled
dataset from a large-scale dataset to facilitate efficient training. A
significant challenge in DD is the dependency between the distilled dataset and
the neural network (NN) architecture used. Training a different NN architecture
with a distilled dataset distilled using a specific architecture often results
in diminished trainning performance for other architectures. This paper
introduces MetaDD, designed to enhance the generalizability of DD across
various NN architectures. Specifically, MetaDD partitions distilled data into
meta features (i.e., the data's common characteristics that remain consistent
across different NN architectures) and heterogeneous features (i.e., the data's
unique feature to each NN architecture). Then, MetaDD employs an
architecture-invariant loss function for multi-architecture feature alignment,
which increases meta features and reduces heterogeneous features in distilled
data. As a low-memory consumption component, MetaDD can be seamlessly
integrated into any DD methodology. Experimental results demonstrate that
MetaDD significantly improves performance across various DD methods. On the
Distilled Tiny-Imagenet with Sre2L (50 IPC), MetaDD achieves cross-architecture
NN accuracy of up to 30.1\%, surpassing the second-best method (GLaD) by 1.7\%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IGroupSS-Mamba: Interval Group Spatial-Spectral Mamba for Hyperspectral
  Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan He, Bing Tu, Puzhao Jiang, Bo Liu, Jun Li, Antonio Plaza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral image (HSI) classification has garnered substantial attention
in remote sensing fields. Recent Mamba architectures built upon the Selective
State Space Models (S6) have demonstrated enormous potential in long-range
sequence modeling. However, the high dimensionality of hyperspectral data and
information redundancy pose challenges to the application of Mamba in HSI
classification, suffering from suboptimal performance and computational
efficiency. In light of this, this paper investigates a lightweight Interval
Group Spatial-Spectral Mamba framework (IGroupSS-Mamba) for HSI classification,
which allows for multi-directional and multi-scale global spatial-spectral
information extraction in a grouping and hierarchical manner. Technically, an
Interval Group S6 Mechanism (IGSM) is developed as the core component, which
partitions high-dimensional features into multiple non-overlapping groups at
intervals, and then integrates a unidirectional S6 for each group with a
specific scanning direction to achieve non-redundant sequence modeling.
Compared to conventional applying multi-directional scanning to all bands, this
grouping strategy leverages the complementary strengths of different scanning
directions while decreasing computational costs. To adequately capture the
spatial-spectral contextual information, an Interval Group Spatial-Spectral
Block (IGSSB) is introduced, in which two IGSM-based spatial and spectral
operators are cascaded to characterize the global spatial-spectral relationship
along the spatial and spectral dimensions, respectively. IGroupSS-Mamba is
constructed as a hierarchical structure stacked by multiple IGSSB blocks,
integrating a pixel aggregation-based downsampling strategy for multiscale
spatial-spectral semantic learning from shallow to deep stages. Extensive
experiments demonstrate that IGroupSS-Mamba outperforms the state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DreamSat: Towards a General 3D Model for Novel View Synthesis of Space
  Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nidhi Mathihalli, Audrey Wei, Giovanni Lavezzi, Peng Mun Siew, Victor Rodriguez-Fernandez, Hodei Urrutxua, Richard Linares
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novel view synthesis (NVS) enables to generate new images of a scene or
convert a set of 2D images into a comprehensive 3D model. In the context of
Space Domain Awareness, since space is becoming increasingly congested, NVS can
accurately map space objects and debris, improving the safety and efficiency of
space operations. Similarly, in Rendezvous and Proximity Operations missions,
3D models can provide details about a target object's shape, size, and
orientation, allowing for better planning and prediction of the target's
behavior. In this work, we explore the generalization abilities of these
reconstruction techniques, aiming to avoid the necessity of retraining for each
new scene, by presenting a novel approach to 3D spacecraft reconstruction from
single-view images, DreamSat, by fine-tuning the Zero123 XL, a state-of-the-art
single-view reconstruction model, on a high-quality dataset of 190 high-quality
spacecraft models and integrating it into the DreamGaussian framework. We
demonstrate consistent improvements in reconstruction quality across multiple
metrics, including Contrastive Language-Image Pretraining (CLIP) score
(+0.33%), Peak Signal-to-Noise Ratio (PSNR) (+2.53%), Structural Similarity
Index (SSIM) (+2.38%), and Learned Perceptual Image Patch Similarity (LPIPS)
(+0.16%) on a test set of 30 previously unseen spacecraft images. Our method
addresses the lack of domain-specific 3D reconstruction tools in the space
industry by leveraging state-of-the-art diffusion models and 3D Gaussian
splatting techniques. This approach maintains the efficiency of the
DreamGaussian framework while enhancing the accuracy and detail of spacecraft
reconstructions. The code for this work can be accessed on GitHub
(https://github.com/ARCLab-MIT/space-nvs).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the 75th International Astronautical Congress, October
  2024, Milan, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-in-the-loop Reasoning For Traffic Sign Detection: Collaborative
  Approach Yolo With Video-llava 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Azarafza, Fatima Idrees, Ali Ehteshami Bejnordi, Charles Steinmetz, Stefan Henkler, Achim Rettberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic Sign Recognition (TSR) detection is a crucial component of autonomous
vehicles. While You Only Look Once (YOLO) is a popular real-time object
detection algorithm, factors like training data quality and adverse weather
conditions (e.g., heavy rain) can lead to detection failures. These failures
can be particularly dangerous when visual similarities between objects exist,
such as mistaking a 30 km/h sign for a higher speed limit sign. This paper
proposes a method that combines video analysis and reasoning, prompting with a
human-in-the-loop guide large vision model to improve YOLOs accuracy in
detecting road speed limit signs, especially in semi-real-world conditions. It
is hypothesized that the guided prompting and reasoning abilities of
Video-LLava can enhance YOLOs traffic sign detection capabilities. This
hypothesis is supported by an evaluation based on human-annotated accuracy
metrics within a dataset of recorded videos from the CARLA car simulator. The
results demonstrate that a collaborative approach combining YOLO with
Video-LLava and reasoning can effectively address challenging situations such
as heavy rain and overcast conditions that hinder YOLOs detection capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ xLSTM-FER: Enhancing Student Expression Recognition with Extended Vision
  Long Short-Term Memory Network <span class="chip">APWeb</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qionghao Huang, Jili Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Student expression recognition has become an essential tool for assessing
learning experiences and emotional states. This paper introduces xLSTM-FER, a
novel architecture derived from the Extended Long Short-Term Memory (xLSTM),
designed to enhance the accuracy and efficiency of expression recognition
through advanced sequence processing capabilities for student facial expression
recognition. xLSTM-FER processes input images by segmenting them into a series
of patches and leveraging a stack of xLSTM blocks to handle these patches.
xLSTM-FER can capture subtle changes in real-world students' facial expressions
and improve recognition accuracy by learning spatial-temporal relationships
within the sequence. Experiments on CK+, RAF-DF, and FERplus demonstrate the
potential of xLSTM-FER in expression recognition tasks, showing better
performance compared to state-of-the-art methods on standard datasets. The
linear computational and memory complexity of xLSTM-FER make it particularly
suitable for handling high-resolution images. Moreover, the design of xLSTM-FER
allows for efficient processing of non-sequential inputs such as images without
additional computation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper, consisting of 10 pages and 3 figures, has been accepted by
  the AIEDM Workshop at the 8th APWeb-WAIM Joint International Conference on
  Web and Big Data</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Control-oriented Clustering of Visual Latent Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Qi, Haocheng Yin, Heng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We initiate a study of the geometry of the visual representation space -- the
information channel from the vision encoder to the action decoder -- in an
image-based control pipeline learned from behavior cloning. Inspired by the
phenomenon of neural collapse (NC) in image classification, we investigate
whether a similar law of clustering emerges in the visual representation space.
Since image-based control is a regression task without explicitly defined
classes, the central piece of the puzzle lies in determining according to what
implicit classes the visual features cluster, if such a law exists. Focusing on
image-based planar pushing, we posit the most important role of the visual
representation in a control task is to convey a goal to the action decoder. We
then classify training samples of expert demonstrations into eight
"control-oriented" classes based on (a) the relative pose between the object
and the target in the input or (b) the relative pose of the object induced by
expert actions in the output, where one class corresponds to one relative pose
orthant (REPO). Across four different instantiations of architecture, we report
the prevalent emergence of control-oriented clustering in the visual
representation space according to the eight REPOs. Beyond empirical
observation, we show such a law of clustering can be leveraged as an
algorithmic tool to improve test-time performance when training a policy with
limited expert demonstrations. Particularly, we pretrain the vision encoder
using NC as a regularization to encourage control-oriented clustering of the
visual features. Surprisingly, such an NC-pretrained vision encoder, when
finetuned end-to-end with the action decoder, boosts the test-time performance
by 10% to 35% in the low-data regime. Real-world vision-based planar pushing
experiments confirmed the surprising advantage of control-oriented visual
representation pretraining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Object Detection via Local-global Contrastive Learning <span class="chip">BMVC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danai Triantafyllidou, Sarah Parisot, Ales Leonardis, Steven McDonagh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual domain gaps often impact object detection performance. Image-to-image
translation can mitigate this effect, where contrastive approaches enable
learning of the image-to-image mapping under unsupervised regimes. However,
existing methods often fail to handle content-rich scenes with multiple object
instances, which manifests in unsatisfactory detection performance. Sensitivity
to such instance-level content is typically only gained through object
annotations, which can be expensive to obtain. Towards addressing this issue,
we present a novel image-to-image translation method that specifically targets
cross-domain object detection. We formulate our approach as a contrastive
learning framework with an inductive prior that optimises the appearance of
object instances through spatial attention masks, implicitly delineating the
scene into foreground regions associated with the target object instances and
background non-object regions. Instead of relying on object annotations to
explicitly account for object instances during translation, our approach learns
to represent objects by contrasting local-global information. This affords
investigation of an under-explored challenge: obtaining performant detection,
under domain shifts, without relying on object annotations nor detector model
fine-tuning. We experiment with multiple cross-domain object detection settings
across three challenging benchmarks and report state-of-the-art performance.
Project page: https://local-global-detection.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BMVC 2024 - Project page: https://local-global-detection.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SELECT: A Large-Scale Benchmark of Data Curation Strategies for Image
  Classification <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Feuer, Jiawei Xu, Niv Cohen, Patrick Yubeaton, Govind Mittal, Chinmay Hegde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data curation is the problem of how to collect and organize samples into a
dataset that supports efficient learning. Despite the centrality of the task,
little work has been devoted towards a large-scale, systematic comparison of
various curation methods. In this work, we take steps towards a formal
evaluation of data curation strategies and introduce SELECT, the first
large-scale benchmark of curation strategies for image classification.
  In order to generate baseline methods for the SELECT benchmark, we create a
new dataset, ImageNet++, which constitutes the largest superset of ImageNet-1K
to date. Our dataset extends ImageNet with 5 new training-data shifts, each
approximately the size of ImageNet-1K itself, and each assembled using a
distinct curation strategy. We evaluate our data curation baselines in two
ways: (i) using each training-data shift to train identical image
classification models from scratch (ii) using the data itself to fit a
pretrained self-supervised representation.
  Our findings show interesting trends, particularly pertaining to recent
methods for data curation such as synthetic data generation and lookup based on
CLIP embeddings. We show that although these strategies are highly competitive
for certain tasks, the curation strategy used to assemble the original
ImageNet-1K dataset remains the gold standard. We anticipate that our benchmark
can illuminate the path for new methods to further reduce the gap. We release
our checkpoints, code, documentation, and a link to our dataset at
https://github.com/jimmyxu123/SELECT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024, Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HE-Drive: Human-Like End-to-End Driving with Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junming Wang, Xingyu Zhang, Zebin Xing, Songen Gu, Xiaoyang Guo, Yang Hu, Ziying Song, Qian Zhang, Xiaoxiao Long, Wei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose HE-Drive: the first human-like-centric end-to-end
autonomous driving system to generate trajectories that are both temporally
consistent and comfortable. Recent studies have shown that imitation
learning-based planners and learning-based trajectory scorers can effectively
generate and select accuracy trajectories that closely mimic expert
demonstrations. However, such trajectory planners and scorers face the dilemma
of generating temporally inconsistent and uncomfortable trajectories. To solve
the above problems, Our HE-Drive first extracts key 3D spatial representations
through sparse perception, which then serves as conditional inputs for a
Conditional Denoising Diffusion Probabilistic Models (DDPMs)-based motion
planner to generate temporal consistency multi-modal trajectories. A
Vision-Language Models (VLMs)-guided trajectory scorer subsequently selects the
most comfortable trajectory from these candidates to control the vehicle,
ensuring human-like end-to-end driving. Experiments show that HE-Drive not only
achieves state-of-the-art performance (i.e., reduces the average collision rate
by 71% than VAD) and efficiency (i.e., 1.9X faster than SparseDrive) on the
challenging nuScenes and OpenScene datasets but also provides the most
comfortable driving experience on real-world data.For more information, visit
the project website: https://jmwang0117.github.io/HE-Drive/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PhotoReg: Photometrically Registering 3D Gaussian Splatting Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwen Yuan, Tianyi Zhang, Matthew Johnson-Roberson, Weiming Zhi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building accurate representations of the environment is critical for
intelligent robots to make decisions during deployment. Advances in
photorealistic environment models have enabled robots to develop
hyper-realistic reconstructions, which can be used to generate images that are
intuitive for human inspection. In particular, the recently introduced
\ac{3DGS}, which describes the scene with up to millions of primitive
ellipsoids, can be rendered in real time. \ac{3DGS} has rapidly gained
prominence. However, a critical unsolved problem persists: how can we fuse
multiple \ac{3DGS} into a single coherent model? Solving this problem will
enable robot teams to jointly build \ac{3DGS} models of their surroundings. A
key insight of this work is to leverage the {duality} between photorealistic
reconstructions, which render realistic 2D images from 3D structure, and
\emph{3D foundation models}, which predict 3D structure from image pairs. To
this end, we develop PhotoReg, a framework to register multiple photorealistic
\ac{3DGS} models with 3D foundation models. As \ac{3DGS} models are generally
built from monocular camera images, they have \emph{arbitrary scale}. To
resolve this, PhotoReg actively enforces scale consistency among the different
\ac{3DGS} models by considering depth estimates within these models. Then, the
alignment is iteratively refined with fine-grained photometric losses to
produce high-quality fused \ac{3DGS} models. We rigorously evaluate PhotoReg on
both standard benchmark datasets and our custom-collected datasets, including
with two quadruped robots. The code is released at
\url{ziweny11.github.io/photoreg}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Systematic Literature <span class="highlight-title">Review</span> of Vision-Based Approaches to Outdoor
  Livestock Monitoring with Lessons from Wildlife Studies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stacey D. Scott, Zayn J. Abbas, Feerass Ellid, Eli-Henry Dykhne, Muhammad Muhaiminul Islam, Weam Ayad, Kristina Kacmorova, Dan Tulpan, Minglun Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precision livestock farming (PLF) aims to improve the health and welfare of
livestock animals and farming outcomes through the use of advanced
technologies. Computer vision, combined with recent advances in machine
learning and deep learning artificial intelligence approaches, offers a
possible solution to the PLF ideal of 24/7 livestock monitoring that helps
facilitate early detection of animal health and welfare issues. However, a
significant number of livestock species are raised in large outdoor habitats
that pose technological challenges for computer vision approaches. This review
provides a comprehensive overview of computer vision methods and open
challenges in outdoor animal monitoring. We include research from both the
livestock and wildlife fields in the review because of the similarities in
appearance, behaviour, and habitat for many livestock and wildlife. We focus on
large terrestrial mammals, such as cattle, horses, deer, goats, sheep, koalas,
giraffes, and elephants. We use an image processing pipeline to frame our
discussion and highlight the current capabilities and open technical challenges
at each stage of the pipeline. The review found a clear trend towards the use
of deep learning approaches for animal detection, counting, and multi-species
classification. We discuss in detail the applicability of current vision-based
methods to PLF contexts and promising directions for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conditional Variational Autoencoders for Probabilistic Pose Regression <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fereidoon Zangeneh, Leonard Bruns, Amit Dekel, Alessandro Pieropan, Patric Jensfelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots rely on visual relocalization to estimate their pose from camera
images when they lose track. One of the challenges in visual relocalization is
repetitive structures in the operation environment of the robot. This calls for
probabilistic methods that support multiple hypotheses for robot's pose. We
propose such a probabilistic method to predict the posterior distribution of
camera poses given an observed image. Our proposed training strategy results in
a generative model of camera poses given an image, which can be used to draw
samples from the pose posterior distribution. Our method is streamlined and
well-founded in theory and outperforms existing methods on localization in
presence of ambiguities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoWeeder: Unsupervised Weed Mapping through Crop-Row Detection <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pasquale De Marinis, Rino Vessio, Giovanna Castellano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precision agriculture relies heavily on effective weed management to ensure
robust crop yields. This study presents RoWeeder, an innovative framework for
unsupervised weed mapping that combines crop-row detection with a
noise-resilient deep learning model. By leveraging crop-row information to
create a pseudo-ground truth, our method trains a lightweight deep learning
model capable of distinguishing between crops and weeds, even in the presence
of noisy data. Evaluated on the WeedMap dataset, RoWeeder achieves an F1 score
of 75.3, outperforming several baselines. Comprehensive ablation studies
further validated the model's performance. By integrating RoWeeder with drone
technology, farmers can conduct real-time aerial surveys, enabling precise weed
management across large fields. The code is available at:
\url{https://github.com/pasqualedem/RoWeeder}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Computer Vision for Plant Phenotyping and Agriculture (CVPPA)
  workshop at ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparison of marker-less 2D image-based methods for infant pose
  estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Jahn, Sarah Flügge, Dajie Zhang, Luise Poustka, Sven Bölte, Florentin Wörgötter, Peter B Marschik, Tomas Kulvicius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are increasing efforts to automate clinical methods for early diagnosis
of developmental disorders, among them the General Movement Assessment (GMA), a
video-based tool to classify infant motor functioning. Optimal pose estimation
is a crucial part of the automated GMA. In this study we compare the
performance of available generic- and infant-pose estimators, and the choice of
viewing angle for optimal recordings, i.e., conventional diagonal view used in
GMA vs. top-down view. For this study, we used 4500 annotated video-frames from
75 recordings of infant spontaneous motor functions from 4 to 26 weeks. To
determine which available pose estimation method and camera angle yield the
best pose estimation accuracy on infants in a GMA related setting, the distance
to human annotations as well as the percentage of correct key-points (PCK) were
computed and compared. The results show that the best performing generic model
trained on adults, ViTPose, also performs best on infants. We see no
improvement from using specialized infant-pose estimators over the generic pose
estimators on our own infant dataset. However, when retraining a generic model
on our data, there is a significant improvement in pose estimation accuracy.
The pose estimation accuracy obtained from the top-down view is significantly
better than that obtained from the diagonal view, especially for the detection
of the hip key-points. The results also indicate only limited generalization
capabilities of infant-pose estimators to other infant datasets, which hints
that one should be careful when choosing infant pose estimators and using them
on infant datasets which they were not trained on. While the standard GMA
method uses a diagonal view for assessment, pose estimation accuracy
significantly improves using a top-down view. This suggests that a top-down
view should be included in recording setups for automated GMA research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 6DGS: Enhanced Direction-Aware Gaussian Splatting for Volumetric
  Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongpai Gao, Benjamin Planche, Meng Zheng, Anwesa Choudhuri, Terrence Chen, Ziyan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novel view synthesis has advanced significantly with the development of
neural radiance fields (NeRF) and 3D Gaussian splatting (3DGS). However,
achieving high quality without compromising real-time rendering remains
challenging, particularly for physically-based ray tracing with view-dependent
effects. Recently, N-dimensional Gaussians (N-DG) introduced a 6D
spatial-angular representation to better incorporate view-dependent effects,
but the Gaussian representation and control scheme are sub-optimal. In this
paper, we revisit 6D Gaussians and introduce 6D Gaussian Splatting (6DGS),
which enhances color and opacity representations and leverages the additional
directional information in the 6D space for optimized Gaussian control. Our
approach is fully compatible with the 3DGS framework and significantly improves
real-time radiance field rendering by better modeling view-dependent effects
and fine details. Experiments demonstrate that 6DGS significantly outperforms
3DGS and N-DG, achieving up to a 15.73 dB improvement in PSNR with a reduction
of 66.5% Gaussian points compared to 3DGS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Demo Video: https://www.youtube.com/watch?v=77wN-K6Q9aM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ L-C4: Language-Based Video Colorization for Creative and Consistent
  Color 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Chang, Shuchen Weng, Huan Ouyang, Yu Li, Si Li, Boxin Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic video colorization is inherently an ill-posed problem because each
monochrome frame has multiple optional color candidates. Previous
exemplar-based video colorization methods restrict the user's imagination due
to the elaborate retrieval process. Alternatively, conditional image
colorization methods combined with post-processing algorithms still struggle to
maintain temporal consistency. To address these issues, we present
Language-based video Colorization for Creative and Consistent Colors (L-C4) to
guide the colorization process using user-provided language descriptions. Our
model is built upon a pre-trained cross-modality generative model, leveraging
its comprehensive language understanding and robust color representation
abilities. We introduce the cross-modality pre-fusion module to generate
instance-aware text embeddings, enabling the application of creative colors.
Additionally, we propose temporally deformable attention to prevent flickering
or color shifts, and cross-clip fusion to maintain long-term color consistency.
Extensive experimental results demonstrate that L-C4 outperforms relevant
methods, achieving semantically accurate colors, unrestricted creative
correspondence, and temporally robust consistency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revealing Directions for Text-guided 3D Face Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Chen, Yichao Yan, Sehngqi Liu, Yuhao Cheng, Weiming Zhao, Lincheng Li, Mengxiao Bi, Xiaokang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D face editing is a significant task in multimedia, aimed at the
manipulation of 3D face models across various control signals. The success of
3D-aware GAN provides expressive 3D models learned from 2D single-view images
only, encouraging researchers to discover semantic editing directions in its
latent space. However, previous methods face challenges in balancing quality,
efficiency, and generalization. To solve the problem, we explore the
possibility of introducing the strength of diffusion model into 3D-aware GANs.
In this paper, we present Face Clan, a fast and text-general approach for
generating and manipulating 3D faces based on arbitrary attribute descriptions.
To achieve disentangled editing, we propose to diffuse on the latent space
under a pair of opposite prompts to estimate the mask indicating the region of
interest on latent codes. Based on the mask, we then apply denoising to the
masked latent codes to reveal the editing direction. Our method offers a
precisely controllable manipulation method, allowing users to intuitively
customize regions of interest with the text description. Experiments
demonstrate the effectiveness and generalization of our Face Clan for various
pre-trained GANs. It offers an intuitive and wide application for text-guided
face editing that contributes to the landscape of multimedia content creation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Efficient Variants of Segment Anything Model: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaorui Sun, Jun Liu, Heng Tao Shen, Xiaofeng Zhu, Ping Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Segment Anything Model (SAM) is a foundational model for image
segmentation tasks, known for its strong generalization across diverse
applications. However, its impressive performance comes with significant
computational and resource demands, making it challenging to deploy in
resource-limited environments such as mobile devices. To address this, a
variety of SAM variants have been proposed to enhance efficiency without
sacrificing accuracy. This survey provides the first comprehensive review of
these efficient SAM variants. We begin by exploring the motivations driving
this research. We then present core techniques used in SAM and model
acceleration. This is followed by an in-depth analysis of various acceleration
strategies, categorized by approach. Finally, we offer a unified and extensive
evaluation of these methods, assessing their efficiency and accuracy on
representative benchmarks, and providing a clear comparison of their overall
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Report in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Ship Recognition and Georeferencing for the Improvement of
  Maritime Situational Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Borja Carrillo Perez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an era where maritime infrastructures are crucial, advanced situational
awareness solutions are increasingly important. The use of optical camera
systems can allow real-time usage of maritime footage. This thesis presents an
investigation into leveraging deep learning and computer vision to advance
real-time ship recognition and georeferencing for the improvement of maritime
situational awareness. A novel dataset, ShipSG, is introduced, containing 3,505
images and 11,625 ship masks with corresponding class and geographic position.
After an exploration of state-of-the-art, a custom real-time segmentation
architecture, ScatYOLOv8+CBAM, is designed for the NVIDIA Jetson AGX Xavier
embedded system. This architecture adds the 2D scattering transform and
attention mechanisms to YOLOv8, achieving an mAP of 75.46% and an 25.3 ms per
frame, outperforming state-of-the-art methods by over 5%. To improve small and
distant ship recognition in high-resolution images on embedded systems, an
enhanced slicing mechanism is introduced, improving mAP by 8% to 11%.
Additionally, a georeferencing method is proposed, achieving positioning errors
of 18 m for ships up to 400 m away and 44 m for ships between 400 m and 1200 m.
The findings are also applied in real-world scenarios, such as the detection of
abnormal ship behaviour, camera integrity assessment and 3D reconstruction. The
approach of this thesis outperforms existing methods and provides a framework
for integrating recognized and georeferenced ships into real-time systems,
enhancing operational effectiveness and decision-making for maritime
stakeholders. This thesis contributes to the maritime computer vision field by
establishing a benchmark for ship segmentation and georeferencing research,
demonstrating the viability of deep-learning-based recognition and
georeferencing methods for real-time maritime monitoring.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Next state prediction gives rise to entangled, yet compositional
  representations of objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tankred Saanum, Luca M. Schulze Buschoff, Peter Dayan, Eric Schulz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compositional representations are thought to enable humans to generalize
across combinatorially vast state spaces. Models with learnable object slots,
which encode information about objects in separate latent codes, have shown
promise for this type of generalization but rely on strong architectural
priors. Models with distributed representations, on the other hand, use
overlapping, potentially entangled neural codes, and their ability to support
compositional generalization remains underexplored. In this paper we examine
whether distributed models can develop linearly separable representations of
objects, like slotted models, through unsupervised training on videos of object
interactions. We show that, surprisingly, models with distributed
representations often match or outperform models with object slots in
downstream prediction tasks. Furthermore, we find that linearly separable
object representations can emerge without object-centric priors, with auxiliary
objectives like next-state prediction playing a key role. Finally, we observe
that distributed models' object representations are never fully disentangled,
even if they are linearly separable: Multiple objects can be encoded through
partially overlapping neural populations while still being highly separable
with a linear classifier. We hypothesize that maintaining partially shared
codes enables distributed models to better compress object dynamics,
potentially enhancing generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRFusion: Toward Effective and Robust Multi-Modal Place Recognition with
  Image and Point Cloud Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sijie Wang, Qiyu Kang, Rui She, Kai Zhao, Yang Song, Wee Peng Tay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Place recognition plays a crucial role in the fields of robotics and computer
vision, finding applications in areas such as autonomous driving, mapping, and
localization. Place recognition identifies a place using query sensor data and
a known database. One of the main challenges is to develop a model that can
deliver accurate results while being robust to environmental variations. We
propose two multi-modal place recognition models, namely PRFusion and
PRFusion++. PRFusion utilizes global fusion with manifold metric attention,
enabling effective interaction between features without requiring camera-LiDAR
extrinsic calibrations. In contrast, PRFusion++ assumes the availability of
extrinsic calibrations and leverages pixel-point correspondences to enhance
feature learning on local windows. Additionally, both models incorporate neural
diffusion layers, which enable reliable operation even in challenging
environments. We verify the state-of-the-art performance of both models on
three large-scale benchmarks. Notably, they outperform existing models by a
substantial margin of +3.0 AR@1 on the demanding Boreas dataset. Furthermore,
we conduct ablation studies to validate the effectiveness of our proposed
methods. The codes are available at: https://github.com/sijieaaa/PRFusion
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by IEEE TITS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniBooth: Learning Latent Control for Image Synthesis with Multi-modal
  Instruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leheng Li, Weichao Qiu, Xu Yan, Jing He, Kaiqiang Zhou, Yingjie Cai, Qing Lian, Bingbing Liu, Ying-Cong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present OmniBooth, an image generation framework that enables spatial
control with instance-level multi-modal customization. For all instances, the
multimodal instruction can be described through text prompts or image
references. Given a set of user-defined masks and associated text or image
guidance, our objective is to generate an image, where multiple objects are
positioned at specified coordinates and their attributes are precisely aligned
with the corresponding guidance. This approach significantly expands the scope
of text-to-image generation, and elevates it to a more versatile and practical
dimension in controllability. In this paper, our core contribution lies in the
proposed latent control signals, a high-dimensional spatial feature that
provides a unified representation to integrate the spatial, textual, and image
conditions seamlessly. The text condition extends ControlNet to provide
instance-level open-vocabulary generation. The image condition further enables
fine-grained control with personalized identity. In practice, our method
empowers users with more flexibility in controllable generation, as users can
choose multi-modal conditions from text or images as needed. Furthermore,
thorough experiments demonstrate our enhanced performance in image synthesis
fidelity and alignment across different tasks and datasets. Project page:
https://len-li.github.io/omnibooth-web/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Art2Mus: Bridging Visual Arts and Music through Cross-Modal Generation <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Rinaldi, Nicola Fanelli, Giovanna Castellano, Gennaro Vessio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence and generative models have revolutionized music
creation, with many models leveraging textual or visual prompts for guidance.
However, existing image-to-music models are limited to simple images, lacking
the capability to generate music from complex digitized artworks. To address
this gap, we introduce $\mathcal{A}\textit{rt2}\mathcal{M}\textit{us}$, a novel
model designed to create music from digitized artworks or text inputs.
$\mathcal{A}\textit{rt2}\mathcal{M}\textit{us}$ extends the AudioLDM~2
architecture, a text-to-audio model, and employs our newly curated datasets,
created via ImageBind, which pair digitized artworks with music. Experimental
results demonstrate that $\mathcal{A}\textit{rt2}\mathcal{M}\textit{us}$ can
generate music that resonates with the input stimuli. These findings suggest
promising applications in multimedia art, interactive installations, and
AI-driven creative tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the AI for Visual Arts (AI4VA) workshop at ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ D-PoSE: Depth as an Intermediate Representation for 3D Human Pose and
  Shape Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Vasilikopoulos, Drosakis Drosakis, Antonis Argyros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present D-PoSE (Depth as an Intermediate Representation for 3D Human Pose
and Shape Estimation), a one-stage method that estimates human pose and SMPL-X
shape parameters from a single RGB image. Recent works use larger models with
transformer backbones and decoders to improve the accuracy in human pose and
shape (HPS) benchmarks. D-PoSE proposes a vision based approach that uses the
estimated human depth-maps as an intermediate representation for HPS and
leverages training with synthetic data and the ground-truth depth-maps provided
with them for depth supervision during training. Although trained on synthetic
datasets, D-PoSE achieves state-of-the-art performance on the real-world
benchmark datasets, EMDB and 3DPW. Despite its simple lightweight design and
the CNN backbone, it outperforms ViT-based models that have a number of
parameters that is larger by almost an order of magnitude. D-PoSE code is
available at: https://github.com/nvasilik/D-PoSE
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Patch is Enough: Naturalistic Adversarial Patch against Vision-Language
  <span class="highlight-title">Pre-train</span>ing Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dehong Kong, Siyuan Liang, Xiaopeng Zhu, Yuansheng Zhong, Wenqi Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual language pre-training (VLP) models have demonstrated significant
success across various domains, yet they remain vulnerable to adversarial
attacks. Addressing these adversarial vulnerabilities is crucial for enhancing
security in multimodal learning. Traditionally, adversarial methods targeting
VLP models involve simultaneously perturbing images and text. However, this
approach faces notable challenges: first, adversarial perturbations often fail
to translate effectively into real-world scenarios; second, direct
modifications to the text are conspicuously visible. To overcome these
limitations, we propose a novel strategy that exclusively employs image patches
for attacks, thus preserving the integrity of the original text. Our method
leverages prior knowledge from diffusion models to enhance the authenticity and
naturalness of the perturbations. Moreover, to optimize patch placement and
improve the efficacy of our attacks, we utilize the cross-attention mechanism,
which encapsulates intermodal interactions by generating attention maps to
guide strategic patch placements. Comprehensive experiments conducted in a
white-box setting for image-to-text scenarios reveal that our proposed method
significantly outperforms existing techniques, achieving a 100% attack success
rate. Additionally, it demonstrates commendable performance in transfer tasks
involving text-to-image configurations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by Visual Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved detection of discarded fish species through BoxAL active
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Sokolova, Pieter M. Blok, Angelo Mencarelli, Arjan Vroegop, Aloysius van Helmond, Gert Kootstra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, powerful data-driven deep-learning techniques have been
developed and applied for automated catch registration. However, these methods
are dependent on the labelled data, which is time-consuming, labour-intensive,
expensive to collect and need expert knowledge. In this study, we present an
active learning technique, named BoxAL, which includes estimation of epistemic
certainty of the Faster R-CNN object-detection model. The method allows
selecting the most uncertain training images from an unlabeled pool, which are
then used to train the object-detection model. To evaluate the method, we used
an open-source image dataset obtained with a dedicated image-acquisition system
developed for commercial trawlers targeting demersal species. We demonstrated,
that our approach allows reaching the same object-detection performance as with
the random sampling using 400 fewer labelled images. Besides, mean AP score was
significantly higher at the last training iteration with 1100 training images,
specifically, 39.0&plusmn;1.6 and 34.8&plusmn;1.8 for certainty-based sampling
and random sampling, respectively. Additionally, we showed that epistemic
certainty is a suitable method to sample images that the current iteration of
the model cannot deal with yet. Our study additionally showed that the sampled
new data is more valuable for training than the remaining unlabeled data. Our
software is available on https://github.com/pieterblok/boxal.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TeX-NeRF: Neural Radiance Fields from Pseudo-TeX Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chonghao Zhong, Chao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural radiance fields (NeRF) has gained significant attention for its
exceptional visual effects. However, most existing NeRF methods reconstruct 3D
scenes from RGB images captured by visible light cameras. In practical
scenarios like darkness, low light, or bad weather, visible light cameras
become ineffective. Therefore, we propose TeX-NeRF, a 3D reconstruction method
using only infrared images, which introduces the object material emissivity as
a priori, preprocesses the infrared images using Pseudo-TeX vision, and maps
the temperatures (T), emissivities (e), and textures (X) of the scene into the
saturation (S), hue (H), and value (V) channels of the HSV color space,
respectively. Novel view synthesis using the processed images has yielded
excellent results. Additionally, we introduce 3D-TeX Datasets, the first
dataset comprising infrared images and their corresponding Pseudo-TeX vision
images. Experiments demonstrate that our method not only matches the quality of
scene reconstruction achieved with high-quality RGB images but also provides
accurate temperature estimations for objects in the scene.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Art Forgery Detection using Kolmogorov Arnold and Convolutional Neural
  Networks <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04866v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04866v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandro Boccuzzo, Deborah Desirée Meyer, Ludovica Schaerf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Art authentication has historically established itself as a task requiring
profound connoisseurship of one particular artist. Nevertheless, famous art
forgers such as Wolfgang Beltracchi were able to deceive dozens of art experts.
In recent years Artificial Intelligence algorithms have been successfully
applied to various image processing tasks. In this work, we leverage the
growing improvements in AI to present an art authentication framework for the
identification of the forger Wolfgang Beltracchi. Differently from existing
literature on AI-aided art authentication, we focus on a specialized model of a
forger, rather than an artist, flipping the approach of traditional AI methods.
We use a carefully compiled dataset of known artists forged by Beltracchi and a
set of known works by the forger to train a multiclass image classification
model based on EfficientNet. We compare the results with Kolmogorov Arnold
Networks (KAN) which, to the best of our knowledge, have never been tested in
the art domain. The results show a general agreement between the different
models' predictions on artworks flagged as forgeries, which are then closely
studied using visual analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2024 workshop AI4VA, oral presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Context Adjustment Loss for Learned Image Compression <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghao Han, Shiyin Jiang, Shengxi Li, Xin Deng, Mai Xu, Ce Zhu, Shuhang Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, learned image compression (LIC) technologies have surpassed
conventional methods notably in terms of rate-distortion (RD) performance. Most
present learned techniques are VAE-based with an autoregressive entropy model,
which obviously promotes the RD performance by utilizing the decoded causal
context. However, extant methods are highly dependent on the fixed hand-crafted
causal context. The question of how to guide the auto-encoder to generate a
more effective causal context benefit for the autoregressive entropy models is
worth exploring. In this paper, we make the first attempt in investigating the
way to explicitly adjust the causal context with our proposed Causal Context
Adjustment loss (CCA-loss). By imposing the CCA-loss, we enable the neural
network to spontaneously adjust important information into the early stage of
the autoregressive entropy model. Furthermore, as transformer technology
develops remarkably, variants of which have been adopted by many
state-of-the-art (SOTA) LIC techniques. The existing computing devices have not
adapted the calculation of the attention mechanism well, which leads to a
burden on computation quantity and inference latency. To overcome it, we
establish a convolutional neural network (CNN) image compression model and
adopt the unevenly channel-wise grouped strategy for high efficiency.
Ultimately, the proposed CNN-based LIC network trained with our Causal Context
Adjustment loss attains a great trade-off between inference latency and
rate-distortion performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PostEdit: Posterior Sampling for Efficient Zero-Shot Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Tian, Yixuan Li, Yichao Yan, Shanyan Guan, Yanhao Ge, Xiaokang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of image editing, three core challenges persist:
controllability, background preservation, and efficiency. Inversion-based
methods rely on time-consuming optimization to preserve the features of the
initial images, which results in low efficiency due to the requirement for
extensive network inference. Conversely, inversion-free methods lack
theoretical support for background similarity, as they circumvent the issue of
maintaining initial features to achieve efficiency. As a consequence, none of
these methods can achieve both high efficiency and background consistency. To
tackle the challenges and the aforementioned disadvantages, we introduce
PostEdit, a method that incorporates a posterior scheme to govern the diffusion
sampling process. Specifically, a corresponding measurement term related to
both the initial features and Langevin dynamics is introduced to optimize the
estimated image generated by the given target prompt. Extensive experimental
results indicate that the proposed PostEdit achieves state-of-the-art editing
performance while accurately preserving unedited regions. Furthermore, the
method is both inversion- and training-free, necessitating approximately 1.5
seconds and 18 GB of GPU memory to generate high-quality results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple Image Segmentation Framework via In-Context Examples <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Liu, Chenchen Jing, Hengtao Li, Muzhi Zhu, Hao Chen, Xinlong Wang, Chunhua Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there have been explorations of generalist segmentation models that
can effectively tackle a variety of image segmentation tasks within a unified
in-context learning framework. However, these methods still struggle with task
ambiguity in in-context segmentation, as not all in-context examples can
accurately convey the task information. In order to address this issue, we
present SINE, a simple image Segmentation framework utilizing in-context
examples. Our approach leverages a Transformer encoder-decoder structure, where
the encoder provides high-quality image representations, and the decoder is
designed to yield multiple task-specific output masks to effectively eliminate
task ambiguity. Specifically, we introduce an In-context Interaction module to
complement in-context information and produce correlations between the target
image and the in-context example and a Matching Transformer that uses fixed
matching and a Hungarian algorithm to eliminate differences between different
tasks. In addition, we have further perfected the current evaluation system for
in-context image segmentation, aiming to facilitate a holistic appraisal of
these models. Experiments on various segmentation tasks show the effectiveness
of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Proc. Conference on Neural Information Processing Systems
  (NeurIPS) 2024. Webpage: https://github.com/aim-uofa/SINE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Fusion Strategies for Mapping Biophysical Landscape Features <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucia Gordon, Nico Lang, Catherine Ressijac, Andrew Davies
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal aerial data are used to monitor natural systems, and machine
learning can significantly accelerate the classification of landscape features
within such imagery to benefit ecology and conservation. It remains
under-explored, however, how these multiple modalities ought to be fused in a
deep learning model. As a step towards filling this gap, we study three
strategies (Early fusion, Late fusion, and Mixture of Experts) for fusing
thermal, RGB, and LiDAR imagery using a dataset of spatially-aligned
orthomosaics in these three modalities. In particular, we aim to map three
ecologically-relevant biophysical landscape features in African savanna
ecosystems: rhino middens, termite mounds, and water. The three fusion
strategies differ in whether the modalities are fused early or late, and if
late, whether the model learns fixed weights per modality for each class or
generates weights for each class adaptively, based on the input. Overall, the
three methods have similar macro-averaged performance with Late fusion
achieving an AUC of 0.698, but their per-class performance varies strongly,
with Early fusion achieving the best recall for middens and water and Mixture
of Experts achieving the best recall for mounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, ECCV 2024 Workshop in CV for Ecology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAT: Concept-level backdoor ATtacks for Concept Bottleneck Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songning Lai, Jiayu Yang, Yu Huang, Lijie Hu, Tianlang Xue, Zhangyi Hu, Jiaxu Li, Haicheng Liao, Yutao Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the transformative impact of deep learning across multiple domains,
the inherent opacity of these models has driven the development of Explainable
Artificial Intelligence (XAI). Among these efforts, Concept Bottleneck Models
(CBMs) have emerged as a key approach to improve interpretability by leveraging
high-level semantic information. However, CBMs, like other machine learning
models, are susceptible to security threats, particularly backdoor attacks,
which can covertly manipulate model behaviors. Understanding that the community
has not yet studied the concept level backdoor attack of CBM, because of
"Better the devil you know than the devil you don't know.", we introduce CAT
(Concept-level Backdoor ATtacks), a methodology that leverages the conceptual
representations within CBMs to embed triggers during training, enabling
controlled manipulation of model predictions at inference time. An enhanced
attack pattern, CAT+, incorporates a correlation function to systematically
select the most effective and stealthy concept triggers, thereby optimizing the
attack's impact. Our comprehensive evaluation framework assesses both the
attack success rate and stealthiness, demonstrating that CAT and CAT+ maintain
high performance on clean data while achieving significant targeted effects on
backdoored datasets. This work underscores the potential security risks
associated with CBMs and provides a robust testing methodology for future
security assessments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resource-Efficient Multiview Perception: Integrating Semantic Masking
  with Masked Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kosta Dakic, Kanchana Thilakarathna, Rodrigo N. Calheiros, Teng Joon Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiview systems have become a key technology in modern computer vision,
offering advanced capabilities in scene understanding and analysis. However,
these systems face critical challenges in bandwidth limitations and
computational constraints, particularly for resource-limited camera nodes like
drones. This paper presents a novel approach for communication-efficient
distributed multiview detection and tracking using masked autoencoders (MAEs).
We introduce a semantic-guided masking strategy that leverages pre-trained
segmentation models and a tunable power function to prioritize informative
image regions. This approach, combined with an MAE, reduces communication
overhead while preserving essential visual information. We evaluate our method
on both virtual and real-world multiview datasets, demonstrating comparable
performance in terms of detection and tracking performance metrics compared to
state-of-the-art techniques, even at high masking ratios. Our selective masking
algorithm outperforms random masking, maintaining higher accuracy and precision
as the masking ratio increases. Furthermore, our approach achieves a
significant reduction in transmission data volume compared to baseline methods,
thereby balancing multiview tracking performance with communication efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Efficient and Effective Trajectories for Differential
  Equation-based Image Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyu Zhu, Jinhui Hou, Hui Liu, Huanqiang Zeng, Junhui Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The differential equation-based image restoration approach aims to establish
learnable trajectories connecting high-quality images to a tractable
distribution, e.g., low-quality images or a Gaussian distribution. In this
paper, we reformulate the trajectory optimization of this kind of method,
focusing on enhancing both reconstruction quality and efficiency. Initially, we
navigate effective restoration paths through a reinforcement learning process,
gradually steering potential trajectories toward the most precise options.
Additionally, to mitigate the considerable computational burden associated with
iterative sampling, we propose cost-aware trajectory distillation to streamline
complex paths into several manageable steps with adaptable sizes. Moreover, we
fine-tune a foundational diffusion model (FLUX) with 12B parameters by using
our algorithms, producing a unified framework for handling 7 kinds of image
restoration tasks. Extensive experiments showcase the significant superiority
of the proposed method, achieving a maximum PSNR improvement of 2.1 dB over
state-of-the-art methods, while also greatly enhancing visual perceptual
quality. Project page: \url{https://zhu-zhiyu.github.io/FLUX-IR/}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedBiP: Heterogeneous One-Shot Federated Learning with Personalized
  Latent Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haokun Chen, Hang Li, Yao Zhang, Gengyuan Zhang, Jinhe Bi, Philip Torr, Jindong Gu, Denis Krompass, Volker Tresp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One-Shot Federated Learning (OSFL), a special decentralized machine learning
paradigm, has recently gained significant attention. OSFL requires only a
single round of client data or model upload, which reduces communication costs
and mitigates privacy threats compared to traditional FL. Despite these
promising prospects, existing methods face challenges due to client data
heterogeneity and limited data quantity when applied to real-world OSFL
systems. Recently, Latent Diffusion Models (LDM) have shown remarkable
advancements in synthesizing high-quality images through pretraining on
large-scale datasets, thereby presenting a potential solution to overcome these
issues. However, directly applying pretrained LDM to heterogeneous OSFL results
in significant distribution shifts in synthetic data, leading to performance
degradation in classification models trained on such data. This issue is
particularly pronounced in rare domains, such as medical imaging, which are
underrepresented in LDM's pretraining data. To address this challenge, we
propose Federated Bi-Level Personalization (FedBiP), which personalizes the
pretrained LDM at both instance-level and concept-level. Hereby, FedBiP
synthesizes images following the client's local data distribution without
compromising the privacy regulations. FedBiP is also the first approach to
simultaneously address feature space heterogeneity and client data scarcity in
OSFL. Our method is validated through extensive experiments on three OSFL
benchmarks with feature space heterogeneity, as well as on challenging medical
and satellite image datasets with label heterogeneity. The results demonstrate
the effectiveness of FedBiP, which substantially outperforms other OSFL
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Damage Assessment in Conflict Zones: A Deep Learning Approach
  Using Geospatial Sub-Meter Resolution Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Risso, Alessia Goffi, Beatrice Alessandra Motetti, Alessio Burrello, Jean Baptiste Bove, Enrico Macii, Massimo Poncino, Daniele Jahier Pagliari, Giuseppe Maffeis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Very High Resolution (VHR) geospatial image analysis is crucial for
humanitarian assistance in both natural and anthropogenic crises, as it allows
to rapidly identify the most critical areas that need support. Nonetheless,
manually inspecting large areas is time-consuming and requires domain
expertise. Thanks to their accuracy, generalization capabilities, and highly
parallelizable workload, Deep Neural Networks (DNNs) provide an excellent way
to automate this task. Nevertheless, there is a scarcity of VHR data pertaining
to conflict situations, and consequently, of studies on the effectiveness of
DNNs in those scenarios. Motivated by this, our work extensively studies the
applicability of a collection of state-of-the-art Convolutional Neural Networks
(CNNs) originally developed for natural disasters damage assessment in a war
scenario. To this end, we build an annotated dataset with pre- and
post-conflict images of the Ukrainian city of Mariupol. We then explore the
transferability of the CNN models in both zero-shot and learning scenarios,
demonstrating their potential and limitations. To the best of our knowledge,
this is the first study to use sub-meter resolution imagery to assess building
damage in combat zones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication in the Sixth IEEE
  International Conference on Image Processing Applications and Systems 2024
  copyright IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Image Clustering with Artifacts Attenuation via Inference-Time
  Attention Engineering <span class="chip">ACCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazumoto Nakamura, Yuji Nozawa, Yu-Chieh Lin, Kengo Nakata, Youyang Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of this paper is to improve the performance of pretrained Vision
Transformer (ViT) models, particularly DINOv2, in image clustering task without
requiring re-training or fine-tuning. As model size increases, high-norm
artifacts anomaly appears in the patches of multi-head attention. We observe
that this anomaly leads to reduced accuracy in zero-shot image clustering.
These artifacts are characterized by disproportionately large values in the
attention map compared to other patch tokens. To address these artifacts, we
propose an approach called Inference-Time Attention Engineering (ITAE), which
manipulates attention function during inference. Specifically, we identify the
artifacts by investigating one of the Query-Key-Value (QKV) patches in the
multi-head attention and attenuate their corresponding attention values inside
the pretrained models. ITAE shows improved clustering accuracy on multiple
datasets by exhibiting more expressive features in latent space. Our findings
highlight the potential of ITAE as a practical solution for reducing artifacts
in pretrained ViT models and improving model performance in clustering tasks
without the need for re-training or fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transforming Color: A Novel Image Colorization Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamza Shafiq, Bumshik Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel method for image colorization that utilizes a
color transformer and generative adversarial networks (GANs) to address the
challenge of generating visually appealing colorized images. Conventional
approaches often struggle with capturing long-range dependencies and producing
realistic colorizations. The proposed method integrates a transformer
architecture to capture global information and a GAN framework to improve
visual quality. In this study, a color encoder that utilizes a random normal
distribution to generate color features is applied. These features are then
integrated with grayscale image features to enhance the overall representation
of the images. Our method demonstrates superior performance compared with
existing approaches by utilizing the capacity of the transformer, which can
capture long-range dependencies and generate a realistic colorization of the
GAN. Experimental results show that the proposed network significantly
outperforms other state-of-the-art colorization techniques, highlighting its
potential for image colorization. This research opens new possibilities for
precise and visually compelling image colorization in domains such as digital
restoration and historical image analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis of Hybrid Compositions in Animation Film with Weakly Supervised
  Learning <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mónica Apellaniz Portos, Roberto Labadie-Tamayo, Claudius Stemmler, Erwin Feyersinger, Andreas Babic, Franziska Bruckner, Vrääth Öhner, Matthias Zeppelzauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an approach for the analysis of hybrid visual compositions in
animation in the domain of ephemeral film. We combine ideas from
semi-supervised and weakly supervised learning to train a model that can
segment hybrid compositions without requiring pre-labeled segmentation masks.
We evaluate our approach on a set of ephemeral films from 13 film archives.
Results demonstrate that the proposed learning strategy yields a performance
close to a fully supervised baseline. On a qualitative level the performed
analysis provides interesting insights on hybrid compositions in animation
film.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Vision for Art (VISART VII) Workshop at the European Conference of
  Computer Vision (ECCV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Modality Prior-Induced Hallucinations in Multimodal Large
  Language Models via Deciphering Attention Causality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanyu Zhou, Yibo Yan, Xin Zou, Kun Wang, Aiwei Liu, Xuming Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have emerged as a central focus in
both industry and academia, but often suffer from biases introduced by visual
and language priors, which can lead to multimodal hallucination. These biases
arise from the visual encoder and the Large Language Model (LLM) backbone,
affecting the attention mechanism responsible for aligning multimodal inputs.
Existing decoding-based mitigation methods focus on statistical correlations
and overlook the causal relationships between attention mechanisms and model
output, limiting their effectiveness in addressing these biases. To tackle this
issue, we propose a causal inference framework termed CausalMM that applies
structural causal modeling to MLLMs, treating modality priors as a confounder
between attention mechanisms and output. Specifically, by employing backdoor
adjustment and counterfactual reasoning at both the visual and language
attention levels, our method mitigates the negative effects of modality priors
and enhances the alignment of MLLM's inputs and outputs, with a maximum score
improvement of 65.3% on 6 VLind-Bench indicators and 164 points on MME
Benchmark compared to conventional methods. Extensive experiments validate the
effectiveness of our approach while being a plug-and-play solution. Our code is
available at: https://github.com/The-Martyr/CausalMM
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM-R$^3$: On (In-)Consistency of Multi-modal Large Language Models
  (MLLMs) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shih-Han Chou, Shivam Chandhok, James J. Little, Leonid Sigal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of Large Language Models (LLMs) and Multimodal
(Visio-lingual) LLMs, a flurry of research has emerged, analyzing the
performance of such models across a diverse array of tasks. While most studies
focus on evaluating the capabilities of state-of-the-art (SoTA) MLLM models
through task accuracy (e.g., Visual Question Answering, grounding) across
various datasets, our work explores the related but complementary aspect of
consistency - the ability of an MLLM model to produce semantically similar or
identical responses to semantically similar queries. We note that consistency
is a fundamental prerequisite (necessary but not sufficient condition) for
robustness and trust in MLLMs. Humans, in particular, are known to be highly
consistent (even if not always accurate) in their responses, and consistency is
inherently expected from AI systems. Armed with this perspective, we propose
the MM-R$^3$ benchmark, which analyses the performance in terms of consistency
and accuracy in SoTA MLLMs with three tasks: Question Rephrasing, Image
Restyling, and Context Reasoning. Our analysis reveals that consistency does
not always align with accuracy, indicating that models with higher accuracy are
not necessarily more consistent, and vice versa. Furthermore, we propose a
simple yet effective mitigation strategy in the form of an adapter module
trained to minimize inconsistency across prompts. With our proposed strategy,
we are able to achieve absolute improvements of 5.7% and 12.5%, on average on
widely used MLLMs such as BLIP-2 and LLaVa 1.5M in terms of consistency over
their existing counterparts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WTCL-Dehaze: Rethinking Real-world Image Dehazing via Wavelet Transform
  and Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divine Joseph Appiah, Donghai Guan, Abdul Nasser Kasule, Mingqiang Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Images captured in hazy outdoor conditions often suffer from colour
distortion, low contrast, and loss of detail, which impair high-level vision
tasks. Single image dehazing is essential for applications such as autonomous
driving and surveillance, with the aim of restoring image clarity. In this
work, we propose WTCL-Dehaze an enhanced semi-supervised dehazing network that
integrates Contrastive Loss and Discrete Wavelet Transform (DWT). We
incorporate contrastive regularization to enhance feature representation by
contrasting hazy and clear image pairs. Additionally, we utilize DWT for
multi-scale feature extraction, effectively capturing high-frequency details
and global structures. Our approach leverages both labelled and unlabelled data
to mitigate the domain gap and improve generalization. The model is trained on
a combination of synthetic and real-world datasets, ensuring robust performance
across different scenarios. Extensive experiments demonstrate that our proposed
algorithm achieves superior performance and improved robustness compared to
state-of-the-art single image dehazing methods on both benchmark datasets and
real-world images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages,4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intriguing Properties of Large Language and Vision Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Young-Jun Lee, Byungsoo Ko, Han-Gyu Kim, Yechan Hwang, Ho-Jin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, large language and vision models (LLVMs) have received significant
attention and development efforts due to their remarkable generalization
performance across a wide range of tasks requiring perception and cognitive
abilities. A key factor behind their success is their simple architecture,
which consists of a vision encoder, a projector, and a large language model
(LLM). Despite their achievements in advanced reasoning tasks, their
performance on fundamental perception-related tasks (e.g., MMVP) remains
surprisingly low. This discrepancy raises the question of how LLVMs truly
perceive images and exploit the advantages of the vision encoder. To address
this, we systematically investigate this question regarding several aspects:
permutation invariance, robustness, math reasoning, alignment preserving and
importance, by evaluating the most common LLVM's families (i.e., LLaVA) across
10 evaluation benchmarks. Our extensive experiments reveal several intriguing
properties of current LLVMs: (1) they internally process the image in a global
manner, even when the order of visual patch sequences is randomly permuted; (2)
they are sometimes able to solve math problems without fully perceiving
detailed numerical information; (3) the cross-modal alignment is overfitted to
complex reasoning tasks, thereby, causing them to lose some of the original
perceptual capabilities of their vision encoder; (4) the representation space
in the lower layers (<25%) plays a crucial role in determining performance and
enhancing visual understanding. Lastly, based on the above observations, we
suggest potential future directions for building better LLVMs and constructing
more challenging evaluation benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available in https://github.com/passing2961/IP-LLVM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language
  Generation with Knowledge Graph for Explaining Thoracic Pathologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ameer Hamza,  Abdullah, Yong Hyun Ahn, Sungyoung Lee, Seong Tae Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating Natural Language Explanations (NLEs) for model predictions on
medical images, particularly those depicting thoracic pathologies, remains a
critical and challenging task. Existing methodologies often struggle due to
general models' insufficient domain-specific medical knowledge and privacy
concerns associated with retrieval-based augmentation techniques. To address
these issues, we propose a novel Vision-Language framework augmented with a
Knowledge Graph (KG)-based datastore, which enhances the model's understanding
by incorporating additional domain-specific medical knowledge essential for
generating accurate and informative NLEs. Our framework employs a KG-based
retrieval mechanism that not only improves the precision of the generated
explanations but also preserves data privacy by avoiding direct data retrieval.
The KG datastore is designed as a plug-and-play module, allowing for seamless
integration with various model architectures. We introduce and evaluate three
distinct frameworks within this paradigm: KG-LLaVA, which integrates the
pre-trained LLaVA model with KG-RAG; Med-XPT, a custom framework combining
MedCLIP, a transformer-based projector, and GPT-2; and Bio-LLaVA, which adapts
LLaVA by incorporating the Bio-ViT-L vision model. These frameworks are
validated on the MIMIC-NLE dataset, where they achieve state-of-the-art
results, underscoring the effectiveness of KG augmentation in generating
high-quality NLEs for thoracic pathologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Models in 3D Vision: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Wang, Dongyuan Li, Renhe Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, 3D vision has become a crucial field within computer vision,
powering a wide range of applications such as autonomous driving, robotics,
augmented reality (AR), and medical imaging. This field relies on the accurate
perception, understanding, and reconstruction of 3D scenes from 2D data sources
like images and videos. Diffusion models, originally designed for 2D generative
tasks, offer the potential for more flexible, probabilistic approaches that can
better capture the variability and uncertainty present in real-world 3D data.
However, traditional methods often struggle with efficiency and scalability. In
this paper, we review the state-of-the-art approaches that leverage diffusion
models for 3D visual tasks, including but not limited to 3D object generation,
shape completion, point cloud reconstruction, and scene understanding. We
provide an in-depth discussion of the underlying mathematical principles of
diffusion models, outlining their forward and reverse processes, as well as the
various architectural advancements that enable these models to work with 3D
datasets. We also discuss the key challenges in applying diffusion models to 3D
vision, such as handling occlusions and varying point densities, and the
computational demands of high-dimensional data. Finally, we discuss potential
solutions, including improving computational efficiency, enhancing multimodal
fusion, and exploring the use of large-scale pretraining for better
generalization across 3D tasks. This paper serves as a foundation for future
exploration and development in this rapidly evolving field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TLDR: Token-Level Detective Reward Model for Large Vision Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deqing Fu, Tong Xiao, Rui Wang, Wang Zhu, Pengchuan Zhang, Guan Pang, Robin Jia, Lawrence Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although reward models have been successful in improving multimodal large
language models, the reward models themselves remain brutal and contain minimal
information. Notably, existing reward models only mimic human annotations by
assigning only one binary feedback to any text, no matter how long the text is.
In the realm of multimodal language models, where models are required to
process both images and texts, a naive reward model may learn implicit biases
toward texts and become less grounded in images. In this paper, we propose a
$\textbf{T}$oken-$\textbf{L}$evel $\textbf{D}$etective $\textbf{R}$eward Model
($\textbf{TLDR}$) to provide fine-grained annotations to each text token. We
first introduce a perturbation-based method to generate synthetic hard
negatives and their token-level labels to train TLDR models. Then we show the
rich usefulness of TLDR models both in assisting off-the-shelf models to
self-correct their generations, and in serving as a hallucination evaluation
tool. Finally, we show that TLDR models can significantly speed up human
annotation by 3 times to acquire a broader range of high-quality vision
language data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work done at Meta</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PredFormer: <span class="highlight-title">Transformer</span>s Are Effective Spatial-Temporal Predictive
  Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujin Tang, Lu Qi, Fei Xie, Xiangtai Li, Chao Ma, Ming-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatiotemporal predictive learning methods generally fall into two
categories: recurrent-based approaches, which face challenges in
parallelization and performance, and recurrent-free methods, which employ
convolutional neural networks (CNNs) as encoder-decoder architectures. These
methods benefit from strong inductive biases but often at the expense of
scalability and generalization. This paper proposes PredFormer, a pure
transformer-based framework for spatiotemporal predictive learning. Motivated
by the Vision Transformers (ViT) design, PredFormer leverages carefully
designed Gated Transformer blocks, following a comprehensive analysis of 3D
attention mechanisms, including full-, factorized-, and interleaved-
spatial-temporal attention. With its recurrent-free, transformer-based design,
PredFormer is both simple and efficient, significantly outperforming previous
methods by large margins. Extensive experiments on synthetic and real-world
datasets demonstrate that PredFormer achieves state-of-the-art performance. On
Moving MNIST, PredFormer achieves a 51.3% reduction in MSE relative to SimVP.
For TaxiBJ, the model decreases MSE by 33.1% and boosts FPS from 533 to 2364.
Additionally, on WeatherBench, it reduces MSE by 11.1% while enhancing FPS from
196 to 404. These performance gains in both accuracy and efficiency demonstrate
PredFormer's potential for real-world applications. The source code will be
released at https://github.com/yyyujintang/PredFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ACDC: Autoregressive Coherent Multimodal Generation using Diffusion
  Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyungjin Chung, Dohun Lee, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive models (ARMs) and diffusion models (DMs) represent two leading
paradigms in generative modeling, each excelling in distinct areas: ARMs in
global context modeling and long-sequence generation, and DMs in generating
high-quality local contexts, especially for continuous data such as images and
short videos. However, ARMs often suffer from exponential error accumulation
over long sequences, leading to physically implausible results, while DMs are
limited by their local context generation capabilities. In this work, we
introduce Autoregressive Coherent multimodal generation with Diffusion
Correction (ACDC), a zero-shot approach that combines the strengths of both
ARMs and DMs at the inference stage without the need for additional
fine-tuning. ACDC leverages ARMs for global context generation and
memory-conditioned DMs for local correction, ensuring high-quality outputs by
correcting artifacts in generated multimodal tokens. In particular, we propose
a memory module based on large language models (LLMs) that dynamically adjusts
the conditioning texts for the DMs, preserving crucial global context
information. Our experiments on multimodal tasks, including coherent
multi-frame story generation and autoregressive video generation, demonstrate
that ACDC effectively mitigates the accumulation of errors and significantly
enhances the quality of generated outputs, achieving superior performance while
remaining agnostic to specific ARM and DM architectures. Project page:
https://acdc2025.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 10 figures. Project page: https://acdc2025.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ H-SIREN: Improving implicit neural representations with hyperbolic
  periodic functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Gao, Rajeev K. Jaiman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit neural representations (INR) have been recently adopted in various
applications ranging from computer vision tasks to physics simulations by
solving partial differential equations. Among existing INR-based works,
multi-layer perceptrons with sinusoidal activation functions find widespread
applications and are also frequently treated as a baseline for the development
of better activation functions for INR applications. Recent investigations
claim that the use of sinusoidal activation functions could be sub-optimal due
to their limited supported frequency set as well as their tendency to generate
over-smoothed solutions. We provide a simple solution to mitigate such an issue
by changing the activation function at the first layer from $\sin(x)$ to
$\sin(\sinh(2x))$. We demonstrate H-SIREN in various computer vision and fluid
flow problems, where it surpasses the performance of several state-of-the-art
INRs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Rank Continual Pyramid Vision <span class="highlight-title">Transformer</span>: Incrementally Segment
  Whole-Body Organs in CT with Light-Weighted Adaptation <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vince Zhu, Zhanghexuan Ji, Dazhou Guo, Puyang Wang, Yingda Xia, Le Lu, Xianghua Ye, Wei Zhu, Dakai Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep segmentation networks achieve high performance when trained on specific
datasets. However, in clinical practice, it is often desirable that pretrained
segmentation models can be dynamically extended to enable segmenting new organs
without access to previous training datasets or without training from scratch.
This would ensure a much more efficient model development and deployment
paradigm accounting for the patient privacy and data storage issues. This
clinically preferred process can be viewed as a continual semantic segmentation
(CSS) problem. Previous CSS works would either experience catastrophic
forgetting or lead to unaffordable memory costs as models expand. In this work,
we propose a new continual whole-body organ segmentation model with
light-weighted low-rank adaptation (LoRA). We first train and freeze a pyramid
vision transformer (PVT) base segmentation model on the initial task, then
continually add light-weighted trainable LoRA parameters to the frozen model
for each new learning task. Through a holistically exploration of the
architecture modification, we identify three most important layers (i.e.,
patch-embedding, multi-head attention and feed forward layers) that are
critical in adapting to the new segmentation tasks, while retaining the
majority of the pretrained parameters fixed. Our proposed model continually
segments new organs without catastrophic forgetting and meanwhile maintaining a
low parameter increasing rate. Continually trained and tested on four datasets
covering different body parts of a total of 121 organs, results show that our
model achieves high segmentation accuracy, closely reaching the PVT and nnUNet
upper bounds, and significantly outperforms other regularization-based CSS
methods. When comparing to the leading architecture-based CSS method, our model
has a substantial lower parameter increasing rate while achieving comparable
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Medical Image Computing and Computer Assisted
  Intervention -- MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Adversarial Risk of Test Time Adaptation: An Investigation into
  Realistic Test-Time Data Poisoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongyi Su, Yushu Li, Nanqing Liu, Kui Jia, Xulei Yang, Chuan-Sheng Foo, Xun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time adaptation (TTA) updates the model weights during the inference
stage using testing data to enhance generalization. However, this practice
exposes TTA to adversarial risks. Existing studies have shown that when TTA is
updated with crafted adversarial test samples, also known as test-time poisoned
data, the performance on benign samples can deteriorate. Nonetheless, the
perceived adversarial risk may be overstated if the poisoned data is generated
under overly strong assumptions. In this work, we first review realistic
assumptions for test-time data poisoning, including white-box versus grey-box
attacks, access to benign data, attack budget, and more. We then propose an
effective and realistic attack method that better produces poisoned samples
without access to benign samples, and derive an effective in-distribution
attack objective. We also design two TTA-aware attack objectives. Our
benchmarks of existing attack methods reveal that the TTA methods are more
robust than previously believed. In addition, we analyze effective defense
strategies to help develop adversarially robust TTA methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Next Best Sense: Guiding Vision and Touch with FisherRF for 3D Gaussian
  Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Strong, Boshu Lei, Aiden Swann, Wen Jiang, Kostas Daniilidis, Monroe Kennedy III
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a framework for active next best view and touch selection for
robotic manipulators using 3D Gaussian Splatting (3DGS). 3DGS is emerging as a
useful explicit 3D scene representation for robotics, as it has the ability to
represent scenes in a both photorealistic and geometrically accurate manner.
However, in real-world, online robotic scenes where the number of views is
limited given efficiency requirements, random view selection for 3DGS becomes
impractical as views are often overlapping and redundant. We address this issue
by proposing an end-to-end online training and active view selection pipeline,
which enhances the performance of 3DGS in few-view robotics settings. We first
elevate the performance of few-shot 3DGS with a novel semantic depth alignment
method using Segment Anything Model 2 (SAM2) that we supplement with Pearson
depth and surface normal loss to improve color and depth reconstruction of
real-world scenes. We then extend FisherRF, a next-best-view selection method
for 3DGS, to select views and touch poses based on depth uncertainty. We
perform online view selection on a real robot system during live 3DGS training.
We motivate our improvements to few-shot GS scenes, and extend depth-based
FisherRF to them, where we demonstrate both qualitative and quantitative
improvements on challenging robot scenes. For more information, please see our
project page at https://armlabstanford.github.io/next-best-sense.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAR: Controllable Autoregressive Modeling for Visual Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Yao, Jialin Li, Yifeng Zhou, Yong Liu, Xi Jiang, Chengjie Wang, Feng Zheng, Yuexian Zou, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controllable generation, which enables fine-grained control over generated
outputs, has emerged as a critical focus in visual generative models.
Currently, there are two primary technical approaches in visual generation:
diffusion models and autoregressive models. Diffusion models, as exemplified by
ControlNet and T2I-Adapter, offer advanced control mechanisms, whereas
autoregressive models, despite showcasing impressive generative quality and
scalability, remain underexplored in terms of controllability and flexibility.
In this study, we introduce Controllable AutoRegressive Modeling (CAR), a
novel, plug-and-play framework that integrates conditional control into
multi-scale latent variable modeling, enabling efficient control generation
within a pre-trained visual autoregressive model. CAR progressively refines and
captures control representations, which are injected into each autoregressive
step of the pre-trained model to guide the generation process. Our approach
demonstrates excellent controllability across various types of conditions and
delivers higher image quality compared to previous methods. Additionally, CAR
achieves robust generalization with significantly fewer training resources
compared to those required for pre-training the model. To the best of our
knowledge, we are the first to propose a control framework for pre-trained
autoregressive visual generation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at: https://github.com/MiracleDance/CAR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ActiView: Evaluating Active Perception Ability for Multimodal Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyue Wang, Chi Chen, Fuwen Luo, Yurui Dong, Yuanchi Zhang, Yuzhuang Xu, Xiaolong Wang, Peng Li, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active perception, a crucial human capability, involves setting a goal based
on the current understanding of the environment and performing actions to
achieve that goal. Despite significant efforts in evaluating Multimodal Large
Language Models (MLLMs), active perception has been largely overlooked. To
address this gap, we propose a novel benchmark named ActiView to evaluate
active perception in MLLMs. Since comprehensively assessing active perception
is challenging, we focus on a specialized form of Visual Question Answering
(VQA) that eases the evaluation yet challenging for existing MLLMs. Given an
image, we restrict the perceptual field of a model, requiring it to actively
zoom or shift its perceptual field based on reasoning to answer the question
successfully. We conduct extensive evaluation over 27 models, including
proprietary and open-source models, and observe that the ability to read and
comprehend multiple images simultaneously plays a significant role in enabling
active perception. Results reveal a significant gap in the active perception
capability of MLLMs, indicating that this area deserves more attention. We hope
that our benchmark could help develop methods for MLLMs to understand
multimodal inputs in more natural and holistic ways.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ mDPO: Conditional Preference Optimization for Multimodal Large Language
  Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Wang, Wenxuan Zhou, James Y. Huang, Nan Xu, Sheng Zhang, Hoifung Poon, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct preference optimization (DPO) has shown to be an effective method for
large language model (LLM) alignment. Recent works have attempted to apply DPO
to multimodal scenarios but have found it challenging to achieve consistent
improvement. Through a comparative experiment, we identify the unconditional
preference problem in multimodal preference optimization, where the model
overlooks the image condition. To address this problem, we propose mDPO, a
multimodal DPO objective that prevents the over-prioritization of language-only
preferences by also optimizing image preference. Moreover, we introduce a
reward anchor that forces the reward to be positive for chosen responses,
thereby avoiding the decrease in their likelihood -- an intrinsic problem of
relative preference optimization. Experiments on two multimodal LLMs of
different sizes and three widely used benchmarks demonstrate that mDPO
effectively addresses the unconditional preference problem in multimodal
preference optimization and significantly improves model performance,
particularly in reducing hallucination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference. Project website:
  https://feiwang96.github.io/mDPO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Parameter-Efficient Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00700v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00700v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinmay Savadikar, Xi Song, Tianfu Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Generative Parameter-Efficient Fine-Tuning (GIFT) for adapting
pretrained Transformer backbones on downstream tasks. GIFT learns to generate
the fine-tuned weights for a layer directly from its pretrained weights. The
GIFT network is parameterized in a minimally-simple way by two linear layers
(without bias terms), and is shared by different pretrained layers selected for
fine-tuning (e.g., the Query layers), which result in significantly fewer
trainable parameters compared to the layer-specific methods like Low-Rank
Adapter (LoRA). We also show this formulation bridges parameter-efficient
fine-tuning and representation fine-tuning. We perform comprehensive
experiments on natural language tasks (commonsense and arithmetic reasoning,
instruction tuning, and sequence classification) and computer vision tasks
(fine-grained classification). We obtain the best performance and parameter
efficiency among baselines on commonsense and arithmetic reasoning, and
instruction following using the Llama family of models and on visual
recognition benchmarks using Vision Transformers. Notably, compared to LoRA, we
obtain 5.7% absolute increase in average accuracy with 14 times reduction of
parameters on Commonsense170k using Llama-3 (8B), and 5.4% absolute increase in
the win rate with 4 times reduction of parameters using Llama-2 (7B) during
instruction tuning. Our GIFT also obtains a slightly higher win rate on
instruction tuning than GPT 3.5 (Turbo 1106).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page and code: https://savadikarc.github.io/gift</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D-free meets 3D priors: Novel View Synthesis from a Single Image with
  <span class="highlight-title">Pretrain</span>ed Diffusion Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taewon Kang, Divya Kothandaraman, Dinesh Manocha, Ming C. Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent 3D novel view synthesis (NVS) methods are limited to
single-object-centric scenes and struggle with complex environments. They often
require extensive 3D data for training, lacking generalization beyond the
training distribution. Conversely, 3D-free methods can generate text-controlled
views of complex, in-the-wild scenes using a pretrained stable diffusion model
without the need for a large amount of 3D-based training data, but lack camera
control. In this paper, we introduce a method capable of generating
camera-controlled viewpoints from a single input image, by combining the
benefits of 3D-free and 3D-based approaches. Our method excels in handling
complex and diverse scenes without extensive training or additional 3D and
multiview data. It leverages widely available pretrained NVS models for weak
guidance, integrating this knowledge into a 3D-free view synthesis approach to
achieve the desired results. Experimental results demonstrate that our method
outperforms existing models in both qualitative and quantitative evaluations,
providing high-fidelity and consistent novel view synthesis at desired camera
angles across a wide variety of scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 12 figures, v2: analysis studies and more results added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Finding Visual Task Vectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Hojel, Yutong Bai, Trevor Darrell, Amir Globerson, Amir Bar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Prompting is a technique for teaching models to perform a visual task
via in-context examples, without any additional training. In this work, we
analyze the activations of MAE-VQGAN, a recent Visual Prompting model, and find
task vectors, activations that encode task-specific information. Equipped with
this insight, we demonstrate that it is possible to identify the task vectors
and use them to guide the network towards performing different tasks without
providing any input-output examples. To find task vectors, we compute the
average intermediate activations per task and use the REINFORCE algorithm to
search for the subset of task vectors. The resulting task vectors guide the
model towards performing a task better than the original model without the need
for input-output examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/alhojel/visual_task_vectors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Narrative <span class="highlight-title">Review</span> of Image Processing Techniques Related to Prostate
  Ultrasound 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00678v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00678v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiqiao Wang, Hong Wu, Zhuoyuan Wang, Peiyan Yue, Dong Ni, Pheng-Ann Heng, Yi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prostate cancer (PCa) poses a significant threat to men's health, with early
diagnosis being crucial for improving prognosis and reducing mortality rates.
Transrectal ultrasound (TRUS) plays a vital role in the diagnosis and
image-guided intervention of PCa.To facilitate physicians with more accurate
and efficient computer-assisted diagnosis and interventions, many image
processing algorithms in TRUS have been proposed and achieved state-of-the-art
performance in several tasks, including prostate gland segmentation, prostate
image registration, PCa classification and detection, and interventional needle
detection. The rapid development of these algorithms over the past two decades
necessitates a comprehensive summary. In consequence, this survey provides a
\textcolor{blue}{narrative } analysis of this field, outlining the evolution of
image processing methods in the context of TRUS image analysis and meanwhile
highlighting their relevant contributions. Furthermore, this survey discusses
current challenges and suggests future research directions to possibly advance
this field further.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Ultrasound in Medicine & Biology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaMetrics: Calibrating Metrics For Generation Tasks Using Human
  Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genta Indra Winata, David Anugraha, Lucky Susanto, Garry Kuwanto, Derry Tanti Wijaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the quality of a performance evaluation metric is crucial for
ensuring that model outputs align with human preferences. However, it remains
unclear how well each metric captures the diverse aspects of these preferences,
as metrics often excel in one particular area but not across all dimensions. To
address this, it is essential to systematically calibrate metrics to specific
aspects of human preference, catering to the unique characteristics of each
aspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate
generation tasks across different modalities in a supervised manner.
MetaMetrics optimizes the combination of existing metrics to enhance their
alignment with human preferences. Our metric demonstrates flexibility and
effectiveness in both language and vision downstream tasks, showing significant
benefits across various multilingual and multi-domain scenarios. MetaMetrics
aligns closely with human preferences and is highly extendable and easily
integrable into any application. This makes MetaMetrics a powerful tool for
improving the evaluation of generation tasks, ensuring that metrics are more
representative of human judgment across diverse contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CYCLO: Cyclic Graph <span class="highlight-title">Transformer</span> Approach to Multi-Object Relationship
  Modeling in Aerial Videos <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01029v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01029v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trong-Thuan Nguyen, Pha Nguyen, Xin Li, Jackson Cothren, Alper Yilmaz, Khoa Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video scene graph generation (VidSGG) has emerged as a transformative
approach to capturing and interpreting the intricate relationships among
objects and their temporal dynamics in video sequences. In this paper, we
introduce the new AeroEye dataset that focuses on multi-object relationship
modeling in aerial videos. Our AeroEye dataset features various drone scenes
and includes a visually comprehensive and precise collection of predicates that
capture the intricate relationships and spatial arrangements among objects. To
this end, we propose the novel Cyclic Graph Transformer (CYCLO) approach that
allows the model to capture both direct and long-range temporal dependencies by
continuously updating the history of interactions in a circular manner. The
proposed approach also allows one to handle sequences with inherent cyclical
patterns and process object relationships in the correct sequential order.
Therefore, it can effectively capture periodic and overlapping relationships
while minimizing information loss. The extensive experiments on the AeroEye
dataset demonstrate the effectiveness of the proposed CYCLO model,
demonstrating its potential to perform scene understanding on drone videos.
Finally, the CYCLO method consistently achieves State-of-the-Art (SOTA) results
on two in-the-wild scene graph generation benchmarks, i.e., PVSG and ASPIRe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards
  General Medical AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03361v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03361v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengcheng Chen, Jin Ye, Guoan Wang, Yanjun Li, Zhongying Deng, Wei Li, Tianbin Li, Haodong Duan, Ziyan Huang, Yanzhou Su, Benyou Wang, Shaoting Zhang, Bin Fu, Jianfei Cai, Bohan Zhuang, Eric J Seibel, Junjun He, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) are capable of handling diverse data
types such as imaging, text, and physiological signals, and can be applied in
various fields. In the medical field, LVLMs have a high potential to offer
substantial assistance for diagnosis and treatment. Before that, it is crucial
to develop benchmarks to evaluate LVLMs' effectiveness in various medical
applications. Current benchmarks are often built upon specific academic
literature, mainly focusing on a single domain, and lacking varying perceptual
granularities. Thus, they face specific challenges, including limited clinical
relevance, incomplete evaluations, and insufficient guidance for interactive
LVLMs. To address these limitations, we developed the GMAI-MMBench, the most
comprehensive general medical AI benchmark with well-categorized data structure
and multi-perceptual granularity to date. It is constructed from 284 datasets
across 38 medical image modalities, 18 clinical-related tasks, 18 departments,
and 4 perceptual granularities in a Visual Question Answering (VQA) format.
Additionally, we implemented a lexical tree structure that allows users to
customize evaluation tasks, accommodating various assessment needs and
substantially supporting medical AI research and applications. We evaluated 50
LVLMs, and the results show that even the advanced GPT-4o only achieves an
accuracy of 53.96%, indicating significant room for improvement. Moreover, we
identified five key insufficiencies in current cutting-edge LVLMs that need to
be addressed to advance the development of better medical applications. We
believe that GMAI-MMBench will stimulate the community to build the next
generation of LVLMs toward GMAI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GitHub: https://github.com/uni-medical/GMAI-MMBench; Hugging face:
  https://huggingface.co/datasets/OpenGVLab/GMAI-MMBench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Multimodal Learning with Missing Modalities via
  Parameter-Efficient Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03986v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03986v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Kaykobad Reza, Ashley Prater-Bennette, M. Salman Asif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal learning seeks to utilize data from multiple sources to improve
the overall performance of downstream tasks. It is desirable for redundancies
in the data to make multimodal systems robust to missing or corrupted
observations in some correlated modalities. However, we observe that the
performance of several existing multimodal networks significantly deteriorates
if one or multiple modalities are absent at test time. To enable robustness to
missing modalities, we propose a simple and parameter-efficient adaptation
procedure for pretrained multimodal networks. In particular, we exploit
modulation of intermediate features to compensate for the missing modalities.
We demonstrate that such adaptation can partially bridge performance drop due
to missing modalities and outperform independent, dedicated networks trained
for the available modality combinations in some cases. The proposed adaptation
requires extremely small number of parameters (e.g., fewer than 1% of the total
parameters) and applicable to a wide range of modality combinations and tasks.
We conduct a series of experiments to highlight the missing modality robustness
of our proposed method on five different multimodal tasks across seven
datasets. Our proposed method demonstrates versatility across various tasks and
datasets, and outperforms existing methods for robust multimodal learning with
missing modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI). 28 pages, 6 figures, 17 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NoSENSE: Learned unrolled cardiac MRI reconstruction without explicit
  sensitivity maps <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15608v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15608v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Frederik Zimmermann, Andreas Kofler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel learned image reconstruction method for accelerated
cardiac MRI with multiple receiver coils based on deep convolutional neural
networks (CNNs) and algorithm unrolling. In contrast to many existing learned
MR image reconstruction techniques that necessitate coil-sensitivity map (CSM)
estimation as a distinct network component, our proposed approach avoids
explicit CSM estimation. Instead, it implicitly captures and learns to exploit
the inter-coil relationships of the images. Our method consists of a series of
novel learned image and k-space blocks with shared latent information and
adaptation to the acquisition parameters by feature-wise modulation (FiLM), as
well as coil-wise data-consistency (DC) blocks.
  Our method achieved PSNR values of 34.89 and 35.56 and SSIM values of 0.920
and 0.942 in the cine track and mapping track validation leaderboard of the
MICCAI STACOM CMRxRecon Challenge, respectively, ranking 4th among different
teams at the time of writing.
  Code will be made available at https://github.com/fzimmermann89/CMRxRecon
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MICCAI STACOM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Navigating the Maze of Explainable AI: A Systematic Approach to
  Evaluating Methods and Metrics <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16756v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16756v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Klein, Carsten T. Lüth, Udo Schlegel, Till J. Bungert, Mennatallah El-Assady, Paul F. Jäger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable AI (XAI) is a rapidly growing domain with a myriad of proposed
methods as well as metrics aiming to evaluate their efficacy. However, current
studies are often of limited scope, examining only a handful of XAI methods and
ignoring underlying design parameters for performance, such as the model
architecture or the nature of input data. Moreover, they often rely on one or a
few metrics and neglect thorough validation, increasing the risk of selection
bias and ignoring discrepancies among metrics. These shortcomings leave
practitioners confused about which method to choose for their problem. In
response, we introduce LATEC, a large-scale benchmark that critically evaluates
17 prominent XAI methods using 20 distinct metrics. We systematically
incorporate vital design parameters like varied architectures and diverse input
modalities, resulting in 7,560 examined combinations. Through LATEC, we
showcase the high risk of conflicting metrics leading to unreliable rankings
and consequently propose a more robust evaluation scheme. Further, we
comprehensively evaluate various XAI methods to assist practitioners in
selecting appropriate methods aligning with their needs. Curiously, the
emerging top-performing method, Expected Gradients, is not examined in any
relevant related study. LATEC reinforces its role in future XAI research by
publicly releasing all 326k saliency maps and 378k metric scores as a
(meta-)evaluation dataset. The benchmark is hosted at:
https://github.com/IML-DKFZ/latec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Persistent Test-time Adaptation in Recurring Testing Scenarios <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18193v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18193v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trung-Hieu Hoang, Duc Minh Vo, Minh N. Do
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current test-time adaptation (TTA) approaches aim to adapt to environments
that change continuously. Yet, it is unclear whether TTA methods can maintain
their adaptability over prolonged periods. To answer this question, we
introduce a diagnostic setting - **recurring TTA** where environments not only
change but also recur over time, creating an extensive data stream. This
setting allows us to examine the error accumulation of TTA models, in the most
basic scenario, when they are regularly exposed to previous testing
environments. Furthermore, we simulate a TTA process on a simple yet
representative $\epsilon$-**perturbed Gaussian Mixture Model Classifier**,
deriving theoretical insights into the dataset- and algorithm-dependent factors
contributing to gradual performance degradation. Our investigation leads us to
propose **persistent TTA (PeTTA)**, which senses when the model is diverging
towards collapse and adjusts the adaptation strategy, striking a balance
between the dual objectives of adaptation and model collapse prevention. The
supreme stability of PeTTA over existing approaches, in the face of lifelong
TTA scenarios, has been demonstrated over comprehensive experiments on various
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autoregressive Image Diffusion: Generation of Image Sequence and
  Application in MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14327v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14327v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanxiong Luo, Shoujin Huang, Martin Uecker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic resonance imaging (MRI) is a widely used non-invasive imaging
modality. However, a persistent challenge lies in balancing image quality with
imaging speed. This trade-off is primarily constrained by k-space measurements,
which traverse specific trajectories in the spatial Fourier domain (k-space).
These measurements are often undersampled to shorten acquisition times,
resulting in image artifacts and compromised quality. Generative models learn
image distributions and can be used to reconstruct high-quality images from
undersampled k-space data. In this work, we present the autoregressive image
diffusion (AID) model for image sequences and use it to sample the posterior
for accelerated MRI reconstruction. The algorithm incorporates both
undersampled k-space and pre-existing information. Models trained with fastMRI
dataset are evaluated comprehensively. The results show that the AID model can
robustly generate sequentially coherent image sequences. In MRI applications,
the AID can outperform the standard diffusion model and reduce hallucinations,
due to the learned inter-image dependencies. The project code is available at
https://github.com/mrirecon/aid.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of
  Large Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14768v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14768v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) need knowledge updates to meet the ever-growing
world facts and correct the hallucinated responses, facilitating the methods of
lifelong model editing. Where the updated knowledge resides in memories is a
fundamental question for model editing. In this paper, we find that editing
either long-term memory (direct model parameters) or working memory
(non-parametric knowledge of neural network activations/representations by
retrieval) will result in an impossible triangle -- reliability,
generalization, and locality can not be realized together in the lifelong
editing settings. For long-term memory, directly editing the parameters will
cause conflicts with irrelevant pretrained knowledge or previous edits (poor
reliability and locality). For working memory, retrieval-based activations can
hardly make the model understand the edits and generalize (poor
generalization). Therefore, we propose WISE to bridge the gap between memories.
In WISE, we design a dual parametric memory scheme, which consists of the main
memory for the pretrained knowledge and a side memory for the edited knowledge.
We only edit the knowledge in the side memory and train a router to decide
which memory to go through when given a query. For continual editing, we devise
a knowledge-sharding mechanism where different sets of edits reside in distinct
subspaces of parameters, and are subsequently merged into a shared memory
without conflicts. Extensive experiments show that WISE can outperform previous
model editing methods and overcome the impossible triangle under lifelong model
editing of question answering, hallucination, and out-of-distribution settings
across trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is
available at https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Selective <span class="highlight-title">Transformer</span> for Hyperspectral Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichu Xu, Di Wang, Lefei Zhang, Liangpei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer has achieved satisfactory results in the field of hyperspectral
image (HSI) classification. However, existing Transformer models face two key
challenges when dealing with HSI scenes characterized by diverse land cover
types and rich spectral information: (1) fixed receptive field representation
overlooks effective contextual information; (2) redundant self-attention
feature representation. To address these limitations, we propose a novel
Selective Transformer (SFormer) for HSI classification. The SFormer is designed
to dynamically select receptive fields for capturing both spatial and spectral
contextual information, while mitigating the impact of redundant data by
prioritizing the most relevant features. This enables a highly accurate
classification of the land covers of the HSI. Specifically, a Kernel Selective
Transformer Block (KSTB) is first utilized to dynamically select an appropriate
receptive field range to effectively extract spatial-spectral features.
Furthermore, to capture the most crucial tokens, a Token Selective Transformer
Block (TSTB) is introduced, which selects the most relevant tokens based on the
ranking of attention scores for each query. Extensive experiments on four
benchmark HSI datasets demonstrate that the proposed SFormer outperforms the
state-of-the-art HSI classification models. The codes will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeRAF: 3D Scene Infused Neural Radiance and Acoustic Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amandine Brunetto, Sascha Hornauer, Fabien Moutarde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sound plays a major role in human perception. Along with vision, it provides
essential information for understanding our surroundings. Despite advances in
neural implicit representations, learning acoustics that align with visual
scenes remains a challenge. We propose NeRAF, a method that jointly learns
acoustic and radiance fields. NeRAF synthesizes both novel views and
spatialized room impulse responses (RIR) at new positions by conditioning the
acoustic field on 3D scene geometric and appearance priors from the radiance
field. The generated RIR can be applied to auralize any audio signal. Each
modality can be rendered independently and at spatially distinct positions,
offering greater versatility. We demonstrate that NeRAF generates high-quality
audio on SoundSpaces and RAF datasets, achieving significant performance
improvements over prior methods while being more data-efficient. Additionally,
NeRAF enhances novel view synthesis of complex scenes trained with sparse data
through cross-modal learning. NeRAF is designed as a Nerfstudio module,
providing convenient access to realistic audio-visual generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://amandinebtto.github.io/NeRAF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IRASNet: Improved Feature-Level Clutter Reduction for Domain Generalized
  SAR-ATR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16845v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16845v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oh-Tae Jang, Hae-Kang Song, Min-Jun Kim, Kyung-Hwan Lee, Geon Lee, Sung-Ho Kim, Hee-Sub Shin, Jae-Woo Ok, Min-Young Back, Jae-Hyuk Yoon, Kyung-Tae Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, computer-aided design models and electromagnetic simulations have
been used to augment synthetic aperture radar (SAR) data for deep learning.
However, an automatic target recognition (ATR) model struggles with domain
shift when using synthetic data because the model learns specific clutter
patterns present in such data, which disturbs performance when applied to
measured data with different clutter distributions. This study proposes a
framework particularly designed for domain-generalized SAR-ATR called IRASNet,
enabling effective feature-level clutter reduction and domain-invariant feature
learning. First, we propose a clutter reduction module (CRM) that maximizes the
signal-to-clutter ratio on feature maps. The module reduces the impact of
clutter at the feature level while preserving target and shadow information,
thereby improving ATR performance. Second, we integrate adversarial learning
with CRM to extract clutter-reduced domain-invariant features. The integration
bridges the gap between synthetic and measured datasets without requiring
measured data during training. Third, we improve feature extraction from target
and shadow regions by implementing a positional supervision task using mask
ground truth encoding. The improvement enhances the ability of the model to
discriminate between classes. Our proposed IRASNet presents new
state-of-the-art public SAR datasets utilizing target and shadow information to
achieve superior performance across various test conditions. IRASNet not only
enhances generalization performance but also significantly improves
feature-level clutter reduction, making it a valuable advancement in the field
of radar image pattern recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boost Your NeRF: A Model-Agnostic Mixture of Experts Framework for High
  Quality and Efficient Rendering <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10389v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10389v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Di Sario, Riccardo Renzulli, Enzo Tartaglione, Marco Grangetto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the introduction of NeRFs, considerable attention has been focused on
improving their training and inference times, leading to the development of
Fast-NeRFs models. Despite demonstrating impressive rendering speed and
quality, the rapid convergence of such models poses challenges for further
improving reconstruction quality. Common strategies to improve rendering
quality involves augmenting model parameters or increasing the number of
sampled points. However, these computationally intensive approaches encounter
limitations in achieving significant quality enhancements. This study
introduces a model-agnostic framework inspired by Sparsely-Gated Mixture of
Experts to enhance rendering quality without escalating computational
complexity. Our approach enables specialization in rendering different scene
components by employing a mixture of experts with varying resolutions. We
present a novel gate formulation designed to maximize expert capabilities and
propose a resolution-based routing technique to effectively induce sparsity and
decompose scenes. Our work significantly improves reconstruction quality while
maintaining competitive performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted to the ECCV 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seeking Flat Minima with Mean Teacher on Semi- and Weakly-Supervised
  Domain Generalization for Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19351v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19351v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryosuke Furuta, Yoichi Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detectors do not work well when domains largely differ between
training and testing data. To overcome this domain gap in object detection
without requiring expensive annotations, we consider two problem settings:
semi-supervised domain generalizable object detection (SS-DGOD) and
weakly-supervised DGOD (WS-DGOD). In contrast to the conventional domain
generalization for object detection that requires labeled data from multiple
domains, SS-DGOD and WS-DGOD require labeled data only from one domain and
unlabeled or weakly-labeled data from multiple domains for training. In this
paper, we show that object detectors can be effectively trained on the two
settings with the same Mean Teacher learning framework, where a student network
is trained with pseudo-labels output from a teacher on the unlabeled or
weakly-labeled data. We provide novel interpretations of why the Mean Teacher
learning framework works well on the two settings in terms of the relationships
between the generalization gap and flat minima in parameter space. On the basis
of the interpretations, we also show that incorporating a simple regularization
method into the Mean Teacher learning framework leads to flatter minima. The
experimental results demonstrate that the regularization leads to flatter
minima and boosts the performance of the detectors trained with the Mean
Teacher learning framework on the two settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Breaking the Frame: Visual Place Recognition by Overlap Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16204v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16204v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Wei, Philipp Lindenberger, Jiri Matas, Daniel Barath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual place recognition methods struggle with occlusions and partial visual
overlaps. We propose a novel visual place recognition approach based on overlap
prediction, called VOP, shifting from traditional reliance on global image
similarities and local features to image overlap prediction. VOP proceeds
co-visible image sections by obtaining patch-level embeddings using a Vision
Transformer backbone and establishing patch-to-patch correspondences without
requiring expensive feature detection and matching. Our approach uses a voting
mechanism to assess overlap scores for potential database images. It provides a
nuanced image retrieval metric in challenging scenarios. Experimental results
show that VOP leads to more accurate relative pose estimation and localization
results on the retrieved image pairs than state-of-the-art baselines on a
number of large-scale, real-world indoor and outdoor benchmarks. The code is
available at https://github.com/weitong8591/vop.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectrum Extraction and Clipping for Implicitly Linear Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16017v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16017v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Ebrahimpour Boroojeny, Matus Telgarsky, Hari Sundaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show the effectiveness of automatic differentiation in efficiently and
correctly computing and controlling the spectrum of implicitly linear
operators, a rich family of layer types including all standard convolutional
and dense layers. We provide the first clipping method which is correct for
general convolution layers, and illuminate the representational limitation that
caused correctness issues in prior work. We study the effect of the batch
normalization layers when concatenated with convolutional layers and show how
our clipping method can be applied to their composition. By comparing the
accuracy and performance of our algorithms to the state-of-the-art methods,
using various experiments, we show they are more precise and efficient and lead
to better generalization and adversarial robustness. We provide the code for
using our methods at https://github.com/Ali-E/FastClip.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GS-Hider: Hiding Messages into 3D Gaussian Splatting <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanyu Zhang, Jiarui Meng, Runyi Li, Zhipei Xu, Yongbing Zhang, Jian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has already become the emerging research focus
in the fields of 3D scene reconstruction and novel view synthesis. Given that
training a 3DGS requires a significant amount of time and computational cost,
it is crucial to protect the copyright, integrity, and privacy of such 3D
assets. Steganography, as a crucial technique for encrypted transmission and
copyright protection, has been extensively studied. However, it still lacks
profound exploration targeted at 3DGS. Unlike its predecessor NeRF, 3DGS
possesses two distinct features: 1) explicit 3D representation; and 2)
real-time rendering speeds. These characteristics result in the 3DGS point
cloud files being public and transparent, with each Gaussian point having a
clear physical significance. Therefore, ensuring the security and fidelity of
the original 3D scene while embedding information into the 3DGS point cloud
files is an extremely challenging task. To solve the above-mentioned issue, we
first propose a steganography framework for 3DGS, dubbed GS-Hider, which can
embed 3D scenes and images into original GS point clouds in an invisible manner
and accurately extract the hidden messages. Specifically, we design a coupled
secured feature attribute to replace the original 3DGS's spherical harmonics
coefficients and then use a scene decoder and a message decoder to disentangle
the original RGB scene and the hidden message. Extensive experiments
demonstrated that the proposed GS-Hider can effectively conceal multimodal
messages without compromising rendering quality and possesses exceptional
security, robustness, capacity, and flexibility. Our project is available at:
https://xuanyuzhang21.github.io/project/gshider.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024, 3DGS steganography</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language
  Models for Robotic Garment Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18082v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18082v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Li, Siyuan Huang, Qiaojun Yu, Zhengkai Jiang, Ce Hao, Yimeng Zhu, Hongsheng Li, Peng Gao, Cewu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automating garment manipulation poses a significant challenge for assistive
robotics due to the diverse and deformable nature of garments. Traditional
approaches typically require separate models for each garment type, which
limits scalability and adaptability. In contrast, this paper presents a unified
approach using vision-language models (VLMs) to improve keypoint prediction
across various garment categories. By interpreting both visual and semantic
information, our model enables robots to manage different garment states with a
single model. We created a large-scale synthetic dataset using advanced
simulation techniques, allowing scalable training without extensive real-world
data. Experimental results indicate that the VLM-based method significantly
enhances keypoint detection accuracy and task success rates, providing a more
flexible and general solution for robotic garment manipulation. In addition,
this research also underscores the potential of VLMs to unify various garment
manipulation tasks within a single framework, paving the way for broader
applications in home automation and assistive robotics for future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Question Decomposition on Multimodal Large Language Models <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19339v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19339v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Zhang, Jianzhe Liu, Zhen Han, Shuo Chen, Bailan He, Volker Tresp, Zhiqiang Xu, Jindong Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question decomposition has emerged as an effective strategy for prompting
Large Language Models (LLMs) to answer complex questions. However, while
existing methods primarily focus on unimodal language models, the question
decomposition capability of Multimodal Large Language Models (MLLMs) has yet to
be explored. To this end, this paper explores visual question decomposition on
MLLMs. Specifically, we introduce a systematic evaluation framework including a
dataset and several evaluation criteria to assess the quality of the decomposed
sub-questions, revealing that existing MLLMs struggle to produce high-quality
sub-questions. To address this limitation, we propose a specific finetuning
dataset, DecoVQA+, for enhancing the model's question decomposition capability.
Aiming at enabling models to perform appropriate selective decomposition, we
propose an efficient finetuning pipeline. The finetuning pipeline consists of
our proposed dataset and a training objective for selective decomposition.
Finetuned MLLMs demonstrate significant improvements in the quality of
sub-questions and the policy of selective question decomposition. Additionally,
the models also achieve higher accuracy with selective decomposition on VQA
benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StructuReiser: A Structure-preserving Video Stylization Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15341v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15341v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Radim Spetlik, David Futschik, Daniel Sykora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce StructuReiser, a novel video-to-video translation method that
transforms input videos into stylized sequences using a set of user-provided
keyframes. Unlike existing approaches, StructuReiser maintains strict adherence
to the structural elements of the target video, preserving the original
identity while seamlessly applying the desired stylistic transformations. This
enables a level of control and consistency that was previously unattainable
with traditional text-driven or keyframe-based methods. Furthermore,
StructuReiser supports real-time inference and custom keyframe editing, making
it ideal for interactive applications and expanding the possibilities for
creative expression and video manipulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VILENS: Visual, Inertial, Lidar, and Leg Odometry for All-Terrain Legged
  Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.07243v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.07243v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Wisth, Marco Camurri, Maurice Fallon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present visual inertial lidar legged navigation system (VILENS), an
odometry system for legged robots based on factor graphs. The key novelty is
the tight fusion of four different sensor modalities to achieve reliable
operation when the individual sensors would otherwise produce degenerate
estimation. To minimize leg odometry drift, we extend the robot's state with a
linear velocity bias term, which is estimated online. This bias is observable
because of the tight fusion of this preintegrated velocity factor with vision,
lidar, and inertial measurement unit (IMU) factors. Extensive experimental
validation on different ANYmal quadruped robots is presented, for a total
duration of 2 h and 1.8 km traveled. The experiments involved dynamic
locomotion over loose rocks, slopes, and mud, which caused challenges such as
slippage and terrain deformation. Perceptual challenges included dark and dusty
underground caverns, and open and feature-deprived areas. We show an average
improvement of 62% translational and 51% rotational errors compared to a
state-of-the-art loosely coupled approach. To demonstrate its robustness,
VILENS was also integrated with a perceptive controller and a local path
planner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Video: https://youtu.be/NG4pkjJKhus</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Consistency Trajectory Models for Image Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12510v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12510v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beomsu Kim, Jaemin Kim, Jeongsol Kim, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) excel in unconditional generation, as well as on
applications such as image editing and restoration. The success of DMs lies in
the iterative nature of diffusion: diffusion breaks down the complex process of
mapping noise to data into a sequence of simple denoising tasks. Moreover, we
are able to exert fine-grained control over the generation process by injecting
guidance terms into each denoising step. However, the iterative process is also
computationally intensive, often taking from tens up to thousands of function
evaluations. Although consistency trajectory models (CTMs) enable traversal
between any time points along the probability flow ODE (PFODE) and score
inference with a single function evaluation, CTMs only allow translation from
Gaussian noise to data. This work aims to unlock the full potential of CTMs by
proposing generalized CTMs (GCTMs), which translate between arbitrary
distributions via ODEs. We discuss the design space of GCTMs and demonstrate
their efficacy in various image manipulation tasks such as image-to-image
translation, restoration, and editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Contrastive Feature Representations for Facial Action Unit
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06165v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06165v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiao Shang, Bin Liu, Fengmao Lv, Fei Teng, Tianrui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial action unit (AU) detection has long encountered the challenge of
detecting subtle feature differences when AUs activate. Existing methods often
rely on encoding pixel-level information of AUs, which not only encodes
additional redundant information but also leads to increased model complexity
and limited generalizability. Additionally, the accuracy of AU detection is
negatively impacted by the class imbalance issue of each AU type, and the
presence of noisy and false AU labels. In this paper, we introduce a novel
contrastive learning framework aimed for AU detection that incorporates both
self-supervised and supervised signals, thereby enhancing the learning of
discriminative features for accurate AU detection. To tackle the class
imbalance issue, we employ a negative sample re-weighting strategy that adjusts
the step size of updating parameters for minority and majority class samples.
Moreover, to address the challenges posed by noisy and false AU labels, we
employ a sampling technique that encompasses three distinct types of positive
sample pairs. This enables us to inject self-supervised signals into the
supervised signal, effectively mitigating the adverse effects of noisy labels.
Our experimental assessments, conducted on four widely-utilized benchmark
datasets (BP4D, DISFA, GFT and Aff-Wild2), underscore the superior performance
of our approach compared to state-of-the-art methods of AU detection. Our code
is available at \url{https://github.com/Ziqiao-Shang/AUNCE}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 17 figures, submitted to IEEE Transactions on Circuits and
  Systems for Video Technology (TCSVT)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Classification of All Blood Cell Images using ML and DL Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.06300v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.06300v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rabia Asghar, Sanjay Kumar, Paul Hynds, Abeera Mahfooz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human blood primarily comprises plasma, red blood cells, white blood cells,
and platelets. It plays a vital role in transporting nutrients to different
organs, where it stores essential health-related data about the human body.
Blood cells are utilized to defend the body against diverse infections,
including fungi, viruses, and bacteria. Hence, blood analysis can help
physicians assess an individual's physiological condition. Blood cells have
been sub-classified into eight groups: Neutrophils, eosinophils, basophils,
lymphocytes, monocytes, immature granulocytes (promyelocytes, myelocytes, and
metamyelocytes), erythroblasts, and platelets or thrombocytes on the basis of
their nucleus, shape, and cytoplasm. Traditionally, pathologists and
hematologists in laboratories have examined these blood cells using a
microscope before manually classifying them. The manual approach is slower and
more prone to human error. Therefore, it is essential to automate this process.
In our paper, transfer learning with CNN pre-trained models. VGG16, VGG19,
ResNet-50, ResNet-101, ResNet-152, InceptionV3, MobileNetV2, and DenseNet-20
applied to the PBC dataset's normal DIB. The overall accuracy achieved with
these models lies between 91.375 and 94.72%. Hence, inspired by these
pre-trained architectures, a model has been proposed to automatically classify
the ten types of blood cells with increased accuracy. A novel CNN-based
framework has been presented to improve accuracy. The proposed CNN model has
been tested on the PBC dataset normal DIB. The outcomes of the experiments
demonstrate that our CNN-based framework designed for blood cell classification
attains an accuracy of 99.91% on the PBC dataset. Our proposed convolutional
neural network model performs competitively when compared to earlier results
reported in the literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Framework for Pupil Tracking with Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khadija Iddrisu, Waseem Shariff, Suzanne Little
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Saccades are extremely rapid movements of both eyes that occur
simultaneously, typically observed when an individual shifts their focus from
one object to another. These movements are among the swiftest produced by
humans and possess the potential to achieve velocities greater than that of
blinks. The peak angular speed of the eye during a saccade can reach as high as
700{\deg}/s in humans, especially during larger saccades that cover a visual
angle of 25{\deg}. Previous research has demonstrated encouraging outcomes in
comprehending neurological conditions through the study of saccades. A
necessary step in saccade detection involves accurately identifying the precise
location of the pupil within the eye, from which additional information such as
gaze angles can be inferred. Conventional frame-based cameras often struggle
with the high temporal precision necessary for tracking very fast movements,
resulting in motion blur and latency issues. Event cameras, on the other hand,
offer a promising alternative by recording changes in the visual scene
asynchronously and providing high temporal resolution and low latency. By
bridging the gap between traditional computer vision and event-based vision, we
present events as frames that can be readily utilized by standard deep learning
algorithms. This approach harnesses YOLOv8, a state-of-the-art object detection
technology, to process these frames for pupil tracking using the publicly
accessible Ev-Eye dataset. Experimental results demonstrate the framework's
effectiveness, highlighting its potential applications in neuroscience,
ophthalmology, and human-computer interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is a preprint of a paper submitted to the 26th Irish
  Machine Vision and Image Processing Conference (IMVIP 2024). If accepted, the
  copy of record will be available at IET Digital Library</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Universal Medical Image Representation Learning with Compositional
  Decoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19890v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19890v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaini Wang, Ling Yang, Siping Zhou, Guangquan Zhou, Wentao Zhang, Bin Cui, Shuo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual-language models have advanced the development of universal models, yet
their application in medical imaging remains constrained by specific functional
requirements and the limited data. Current general-purpose models are typically
designed with task-specific branches and heads, which restricts the shared
feature space and the flexibility of model. To address these challenges, we
have developed a decomposed-composed universal medical imaging paradigm
(UniMed) that supports tasks at all levels. To this end, we first propose a
decomposed decoder that can predict two types of outputs -- pixel and semantic,
based on a defined input queue. Additionally, we introduce a composed decoder
that unifies the input and output spaces and standardizes task annotations
across different levels into a discrete token format. The coupled design of
these two components enables the model to flexibly combine tasks and mutual
benefits. Moreover, our joint representation learning strategy skilfully
leverages large amounts of unlabeled data and unsupervised loss, achieving
efficient one-stage pretraining for more robust performance. Experimental
results show that UniMed achieves state-of-the-art performance on eight
datasets across all three tasks and exhibits strong zero-shot and 100-shot
transferability. We will release the code and trained models upon the paper's
acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ColorwAI: Generative Colorways of Textiles through GAN and Diffusion
  Disentanglement <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11514v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11514v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ludovica Schaerf, Andrea Alfarano, Eric Postma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Colorway creation is the task of generating textile samples in alternate
color variations maintaining an underlying pattern. The individuation of a
suitable color palette for a colorway is a complex creative task, responding to
client and market needs, stylistic and cultural specifications, and mood. We
introduce a modification of this task, the "generative colorway" creation, that
includes minimal shape modifications, and propose a framework, "ColorwAI", to
tackle this task using color disentanglement on StyleGAN and Diffusion. We
introduce a variation of the InterfaceGAN method for supervised
disentanglement, ShapleyVec. We use Shapley values to subselect a few
dimensions of the detected latent direction. Moreover, we introduce a general
framework to adopt common disentanglement methods on any architecture with a
semantic latent space and test it on Diffusion and GANs. We interpret the color
representations within the models' latent space. We find StyleGAN's W space to
be the most aligned with human notions of color. Finally, we suggest that
disentanglement can solicit a creative system for colorway creation, and
evaluate it through expert questionnaires and creativity theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2024 VISART workshop, oral presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning an Actionable Discrete Diffusion Policy via Large-Scale
  Actionless Video <span class="highlight-title">Pre-Train</span>ing <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14407v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14407v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran He, Chenjia Bai, Ling Pan, Weinan Zhang, Bin Zhao, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning a generalist embodied agent capable of completing multiple tasks
poses challenges, primarily stemming from the scarcity of action-labeled
robotic datasets. In contrast, a vast amount of human videos exist, capturing
intricate tasks and interactions with the physical world. Promising prospects
arise for utilizing actionless human videos for pre-training and transferring
the knowledge to facilitate robot policy learning through limited robot
demonstrations. However, it remains a challenge due to the domain gap between
humans and robots. Moreover, it is difficult to extract useful information
representing the dynamic world from human videos, because of its noisy and
multimodal data structure. In this paper, we introduce a novel framework to
tackle these challenges, which leverages a unified discrete diffusion to
combine generative pre-training on human videos and policy fine-tuning on a
small number of action-labeled robot videos. We start by compressing both human
and robot videos into unified video tokens. In the pre-training stage, we
employ a discrete diffusion model with a mask-and-replace diffusion strategy to
predict future video tokens in the latent space. In the fine-tuning stage, we
harness the imagined future videos to guide low-level action learning with a
limited set of robot data. Experiments demonstrate that our method generates
high-fidelity future videos for planning and enhances the fine-tuned policies
compared to previous state-of-the-art approaches with superior performance. Our
project website is available at https://video-diff.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. 24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Large Language Models Understand Symbolic Graphics Programs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08313v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08313v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Z. Xiao, Katherine M. Collins, Joshua B. Tenenbaum, Adrian Weller, Michael J. Black, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Against the backdrop of enthusiasm for large language models (LLMs), there is
an urgent need to scientifically assess their capabilities and shortcomings.
This is nontrivial in part because it is difficult to find tasks which the
models have not encountered during training. Utilizing symbolic graphics
programs, we propose a domain well-suited to test multiple spatial-semantic
reasoning skills of LLMs. Popular in computer graphics, these programs
procedurally generate visual data. While LLMs exhibit impressive skills in
general program synthesis and analysis, symbolic graphics programs offer a new
layer of evaluation: they allow us to test an LLM's ability to answer
different-grained semantic-level questions of the images or 3D geometries
without a vision encoder. To semantically understand the symbolic programs,
LLMs would need to possess the ability to "imagine" and reason how the
corresponding graphics content would look with only the symbolic description.
We use this task to evaluate LLMs by creating a large benchmark for the
semantic visual understanding of symbolic graphics programs, built procedurally
with minimal human effort. Particular emphasis is placed on transformations of
images that leave the image level semantics invariant while introducing
significant changes to the underlying program. We evaluate commercial and
open-source LLMs on our benchmark to assess their ability to reason about
visual output of programs, finding that LLMs considered stronger at reasoning
generally perform better. Lastly, we introduce a novel method to improve this
ability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned
with pre-collected instruction data on symbolic graphics programs.
Interestingly, we find that SIT not only improves LLM's understanding on
symbolic programs, but it also improves general reasoning ability on various
other benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report v2 (46 pages, 24 figures, project page:
  https://sgp-bench.github.io/, substantial update from v1)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TD-NeRF: Novel Truncated Depth Prior for Joint Camera Pose and Neural
  Radiance Field Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Tan, Zongtan Zhou, Yangbing Ge, Zi Wang, Xieyuanli Chen, Dewen Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reliance on accurate camera poses is a significant barrier to the
widespread deployment of Neural Radiance Fields (NeRF) models for 3D
reconstruction and SLAM tasks. The existing method introduces monocular depth
priors to jointly optimize the camera poses and NeRF, which fails to fully
exploit the depth priors and neglects the impact of their inherent noise. In
this paper, we propose Truncated Depth NeRF (TD-NeRF), a novel approach that
enables training NeRF from unknown camera poses - by jointly optimizing
learnable parameters of the radiance field and camera poses. Our approach
explicitly utilizes monocular depth priors through three key advancements: 1)
we propose a novel depth-based ray sampling strategy based on the truncated
normal distribution, which improves the convergence speed and accuracy of pose
estimation; 2) to circumvent local minima and refine depth geometry, we
introduce a coarse-to-fine training strategy that progressively improves the
depth precision; 3) we propose a more robust inter-frame point constraint that
enhances robustness against depth noise during training. The experimental
results on three datasets demonstrate that TD-NeRF achieves superior
performance in the joint optimization of camera pose and NeRF, surpassing prior
works, and generates more accurate depth geometry. The implementation of our
method has been released at https://github.com/nubot-nudt/TD-NeRF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Distortion Prior with Latent Diffusion Models for Remote
  Sensing Image Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03961v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03961v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhui Li, Jutao Li, Xingsong Hou, Huake Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based image compression algorithms typically focus on designing
encoding and decoding networks and improving the accuracy of entropy model
estimation to enhance the rate-distortion (RD) performance. However, few
algorithms leverage the compression distortion prior from existing compression
algorithms to improve RD performance. In this paper, we propose a latent
diffusion model-based remote sensing image compression (LDM-RSIC) method, which
aims to enhance the final decoding quality of RS images by utilizing the
generated distortion prior from a LDM. Our approach consists of two stages. In
the first stage, a self-encoder learns prior from the high-quality input image.
In the second stage, the prior is generated through an LDM, conditioned on the
decoded image of an existing learning-based image compression algorithm, to be
used as auxiliary information for generating the texture-rich enhanced image.
To better utilize the prior, a channel attention and gate-based dynamic feature
attention module (DFAM) is embedded into a Transformer-based multi-scale
enhancement network (MEN) for image enhancement. Extensive experiments
demonstrate the proposed LDM-RSIC significantly outperforms existing
state-of-the-art traditional and learning-based image compression algorithms in
terms of both subjective perception and objective metrics. Additionally, we use
the LDM-based scheme to improve the traditional image compression algorithm
JPEG2000 and obtain 32.00% bit savings on the DOTA testing set. The code will
be available at https://github.com/mlkk518/LDM-RSIC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Greit-HRNet: Grouped Lightweight High-Resolution Network for Human Pose
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07389v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07389v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjia Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As multi-scale features are necessary for human pose estimation tasks,
high-resolution networks are widely applied.
  To improve efficiency, lightweight modules are proposed to replace costly
point-wise convolutions in high-resolution networks, including channel
weighting and spatial weighting methods.
  However, they fail to maintain the consistency of weights and capture global
spatial information.
  To address these problems, we present a Grouped lightweight High-Resolution
Network (Greit-HRNet), in which we propose a Greit block including a group
method Grouped Channel Weighting (GCW) and a spatial weighting method Global
Spatial Weighting (GSW).
  GCW modules group conditional channel weighting to make weights stable and
maintain the high-resolution features with the deepening of the network, while
GSW modules effectively extract global spatial information and exchange
information across channels.
  In addition, we apply the Large Kernel Attention (LKA) method to improve the
whole efficiency of our Greit-HRNet.
  Our experiments on both MS-COCO and MPII human pose estimation datasets
demonstrate the superior performance of our Greit-HRNet, outperforming other
state-of-the-art lightweight networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedThink: Explaining Medical Visual Question Answering via Multimodal
  Decision-Making Rationale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12372v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12372v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaotang Gai, Chenyi Zhou, Jiaxiang Liu, Yang Feng, Jian Wu, Zuozhu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical Visual Question Answering (MedVQA), which offers language responses
to image-based medical inquiries, represents a challenging task and significant
advancement in healthcare. It assists medical experts to swiftly interpret
medical images, thereby enabling faster and more accurate diagnoses. However,
the model interpretability and transparency of existing MedVQA solutions are
often limited, posing challenges in understanding their decision-making
processes. To address this issue, we devise a semi-automated annotation process
to streamline data preparation and build new benchmark MedVQA datasets R-RAD,
R-SLAKE and R-Path. These datasets provide intermediate medical decision-making
rationales generated by multimodal large language models and human annotations
for question-answering pairs in existing MedVQA datasets, i.e., VQA-RAD, SLAKE
and PathVQA. Moreover, we design a novel framework, MedThink, which finetunes
lightweight pretrained generative models by incorporating medical
decision-making rationales. MedThink includes three distinct strategies to
generate decision outcomes and corresponding rationales, thereby clearly
showcasing the medical decision-making process during reasoning. Our
comprehensive experiments show that our method achieves an accuracy of 83.5% on
R-RAD, 86.3% on R-SLAKE and 87.2% on R-Path. These results significantly exceed
those of existing state-of-the-art models with comparable parameters. Datasets
and code will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 1st Place Solution to the 8th HANDS Workshop Challenge -- ARCTIC Track:
  3DGS-based Bimanual Category-agnostic Interaction Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongwan On, Kyeonghwan Gwak, Gunyoung Kang, Hyein Hwang, Soohyun Hwang, Junuk Cha, Jaewook Han, Seungryul Baek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report describes our 1st place solution to the 8th HANDS workshop
challenge (ARCTIC track) in conjunction with ECCV 2024. In this challenge, we
address the task of bimanual category-agnostic hand-object interaction
reconstruction, which aims to generate 3D reconstructions of both hands and the
object from a monocular video, without relying on predefined templates. This
task is particularly challenging due to the significant occlusion and dynamic
contact between the hands and the object during bimanual manipulation. We
worked to resolve these issues by introducing a mask loss and a 3D contact
loss, respectively. Moreover, we applied 3D Gaussian Splatting (3DGS) to this
task. As a result, our method achieved a value of 38.69 in the main metric,
CD$_h$, on the ARCTIC test set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ColPali: Efficient Document Retrieval with Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01449v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01449v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, Pierre Colombo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Documents are visually rich structures that convey information through text,
as well as tables, figures, page layouts, or fonts. While modern document
retrieval systems exhibit strong performance on query-to-text matching, they
struggle to exploit visual cues efficiently, hindering their performance on
practical document retrieval applications such as Retrieval Augmented
Generation. To benchmark current systems on visually rich document retrieval,
we introduce the Visual Document Retrieval Benchmark ViDoRe, composed of
various page-level retrieving tasks spanning multiple domains, languages, and
settings. The inherent shortcomings of modern systems motivate the introduction
of a new retrieval model architecture, ColPali, which leverages the document
understanding capabilities of recent Vision Language Models to produce
high-quality contextualized embeddings solely from images of document pages.
Combined with a late interaction matching mechanism, ColPali largely
outperforms modern document retrieval pipelines while being drastically faster
and end-to-end trainable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SARatrX: Towards Building A Foundation Model for SAR Target Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijie Li, Wei Yang, Yuenan Hou, Li Liu, Yongxiang Liu, Xiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable progress in synthetic aperture radar automatic target
recognition (SAR ATR), recent efforts have concentrated on the detection or
classification of a specific and coarse category, e.g., vehicles, ships,
airplanes, or buildings. One of the fundamental limitations of the
top-performing SAR ATR methods is that the learning paradigm is supervised,
task-specific, limited-category, closed-world learning, which depends on
massive amounts of accurately annotated samples that are expensively labeled by
expert SAR analysts and has limited generalization capability and scalability.
In this work, we make the first attempt towards building a foundation model for
SAR ATR, termed SARatrX. SARatrX learns generalizable representations via
self-supervised learning (SSL) and provides a basis for label-efficient model
adaptation to generic SAR target detection and classification tasks.
Specifically, SARatrX is trained on 0.18 M unlabelled SAR target samples, which
are curated by combining contemporary benchmarks and constitute the largest
publicly available dataset till now. Considering the characteristics of SAR
images, a backbone tailored for SAR ATR is carefully designed, and a two-step
SSL method endowed with multi-scale gradient features was applied to ensure the
feature diversity and model scalability of SARatrX. The capabilities of SARatrX
are evaluated on classification under few-shot and robustness settings and
detection across various categories and scenes, and impressive performance is
achieved, often competitive with or even superior to prior fully supervised,
semi-supervised, or self-supervised algorithms. Our SARatrX and the curated
dataset are released at https://github.com/waterdisappear/SARatrX to foster
research into foundation models for SAR ATR and SAR image interpretation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Expanding Scene Graph Boundaries: Fully Open-vocabulary Scene Graph
  Generation via Visual-Concept Alignment and Retention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10988v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10988v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuyao Chen, Jinlin Wu, Zhen Lei, Zhaoxiang Zhang, Changwen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene Graph Generation (SGG) offers a structured representation critical in
many computer vision applications. Traditional SGG approaches, however, are
limited by a closed-set assumption, restricting their ability to recognize only
predefined object and relation categories. To overcome this, we categorize SGG
scenarios into four distinct settings based on the node and edge: Closed-set
SGG, Open Vocabulary (object) Detection-based SGG (OvD-SGG), Open Vocabulary
Relation-based SGG (OvR-SGG), and Open Vocabulary Detection + Relationbased SGG
(OvD+R-SGG). While object-centric open vocabulary SGG has been studied
recently, the more challenging problem of relation-involved open-vocabulary SGG
remains relatively unexplored. To fill this gap, we propose a unified framework
named OvSGTR towards fully open vocabulary SGG from a holistic view. The
proposed framework is an end-to-end transformer architecture, which learns a
visual-concept alignment for both nodes and edges, enabling the model to
recognize unseen categories. For the more challenging settings of
relation-involved open vocabulary SGG, the proposed approach integrates
relation-aware pretraining utilizing image-caption data and retains
visual-concept alignment through knowledge distillation. Comprehensive
experimental results on the Visual Genome benchmark demonstrate the
effectiveness and superiority of the proposed framework. Our code is available
at https://github.com/gpt4vision/OvSGTR/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discrete Distribution Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00036v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00036v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel generative model, the Discrete Distribution Networks
(DDN), that approximates data distribution using hierarchical discrete
distributions. We posit that since the features within a network inherently
capture distributional information, enabling the network to generate multiple
samples simultaneously, rather than a single output, may offer an effective way
to represent distributions. Therefore, DDN fits the target distribution,
including continuous ones, by generating multiple discrete sample points. To
capture finer details of the target data, DDN selects the output that is
closest to the Ground Truth (GT) from the coarse results generated in the first
layer. This selected output is then fed back into the network as a condition
for the second layer, thereby generating new outputs more similar to the GT. As
the number of DDN layers increases, the representational space of the outputs
expands exponentially, and the generated samples become increasingly similar to
the GT. This hierarchical output pattern of discrete distributions endows DDN
with unique property: more general zero-shot conditional generation. We
demonstrate the efficacy of DDN and its intriguing properties through
experiments on CIFAR-10 and FFHQ. The code is available at
https://discrete-distribution-networks.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TL;DR: A Novel Generative Model with Simple Principles and Unique
  Properties</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lighthouse: A User-Friendly Library for Reproducible Video Moment
  Retrieval and Highlight Detection <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02901v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02901v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taichi Nishimura, Shota Nakada, Hokuto Munakata, Tatsuya Komatsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Lighthouse, a user-friendly library for reproducible video moment
retrieval and highlight detection (MR-HD). Although researchers proposed
various MR-HD approaches, the research community holds two main issues. The
first is a lack of comprehensive and reproducible experiments across various
methods, datasets, and video-text features. This is because no unified training
and evaluation codebase covers multiple settings. The second is user-unfriendly
design. Because previous works use different libraries, researchers set up
individual environments. In addition, most works release only the training
codes, requiring users to implement the whole inference process of MR-HD.
Lighthouse addresses these issues by implementing a unified reproducible
codebase that includes six models, three features, and five datasets. In
addition, it provides an inference API and web demo to make these methods
easily accessible for researchers and developers. Our experiments demonstrate
that Lighthouse generally reproduces the reported scores in the reference
papers. The code is available at https://github.com/line/lighthouse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at EMNLP2024 - system demonstration track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PhoCoLens: Photorealistic and Consistent Reconstruction in Lensless
  Imaging <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Cai, Zhiyuan You, Hailong Zhang, Wentao Liu, Jinwei Gu, Tianfan Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lensless cameras offer significant advantages in size, weight, and cost
compared to traditional lens-based systems. Without a focusing lens, lensless
cameras rely on computational algorithms to recover the scenes from multiplexed
measurements. However, current algorithms struggle with inaccurate forward
imaging models and insufficient priors to reconstruct high-quality images. To
overcome these limitations, we introduce a novel two-stage approach for
consistent and photorealistic lensless image reconstruction. The first stage of
our approach ensures data consistency by focusing on accurately reconstructing
the low-frequency content with a spatially varying deconvolution method that
adjusts to changes in the Point Spread Function (PSF) across the camera's field
of view. The second stage enhances photorealism by incorporating a generative
prior from pre-trained diffusion models. By conditioning on the low-frequency
content retrieved in the first stage, the diffusion model effectively
reconstructs the high-frequency details that are typically lost in the lensless
imaging process, while also maintaining image fidelity. Our method achieves a
superior balance between data fidelity and visual quality compared to existing
methods, as demonstrated with two popular lensless systems, PhlatCam and
DiffuserCam. Project website: https://phocolens.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PhysDreamer: Physics-Based Interaction with 3D Objects via Video
  Generation <span class="chip">ECCV
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13026v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13026v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Y. Feng, Changxi Zheng, Noah Snavely, Jiajun Wu, William T. Freeman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Realistic object interactions are crucial for creating immersive virtual
experiences, yet synthesizing realistic 3D object dynamics in response to novel
interactions remains a significant challenge. Unlike unconditional or
text-conditioned dynamics generation, action-conditioned dynamics requires
perceiving the physical material properties of objects and grounding the 3D
motion prediction on these properties, such as object stiffness. However,
estimating physical material properties is an open problem due to the lack of
material ground-truth data, as measuring these properties for real objects is
highly difficult. We present PhysDreamer, a physics-based approach that endows
static 3D objects with interactive dynamics by leveraging the object dynamics
priors learned by video generation models. By distilling these priors,
PhysDreamer enables the synthesis of realistic object responses to novel
interactions, such as external forces or agent manipulations. We demonstrate
our approach on diverse examples of elastic objects and evaluate the realism of
the synthesized interactions through a user study. PhysDreamer takes a step
towards more engaging and realistic virtual experiences by enabling static 3D
objects to dynamically respond to interactive stimuli in a physically plausible
manner. See our project page at https://physdreamer.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website at: https://physdreamer.github.io/ Appear on ECCV
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Height Decoupling for Precise Vision-based 3D Occupancy Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07972v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07972v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Wu, Zhiqiang Yan, Zhengxue Wang, Xiang Li, Le Hui, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of vision-based 3D occupancy prediction aims to reconstruct 3D
geometry and estimate its semantic classes from 2D color images, where the
2D-to-3D view transformation is an indispensable step. Most previous methods
conduct forward projection, such as BEVPooling and VoxelPooling, both of which
map the 2D image features into 3D grids. However, the current grid representing
features within a certain height range usually introduces many confusing
features that belong to other height ranges. To address this challenge, we
present Deep Height Decoupling (DHD), a novel framework that incorporates
explicit height prior to filter out the confusing features. Specifically, DHD
first predicts height maps via explicit supervision. Based on the height
distribution statistics, DHD designs Mask Guided Height Sampling (MGHS) to
adaptively decouple the height map into multiple binary masks. MGHS projects
the 2D image features into multiple subspaces, where each grid contains
features within reasonable height ranges. Finally, a Synergistic Feature
Aggregation (SFA) module is deployed to enhance the feature representation
through channel and spatial affinities, enabling further occupancy refinement.
On the popular Occ3D-nuScenes benchmark, our method achieves state-of-the-art
performance even with minimal input frames. Code is available at
https://github.com/yanzq95/DHD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Correcting Diffusion Generation through Resampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06038v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06038v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujian Liu, Yang Zhang, Tommi Jaakkola, Shiyu Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite diffusion models' superior capabilities in modeling complex
distributions, there are still non-trivial distributional discrepancies between
generated and ground-truth images, which has resulted in several notable
problems in image generation, including missing object errors in text-to-image
generation and low image quality. Existing methods that attempt to address
these problems mostly do not tend to address the fundamental cause behind these
problems, which is the distributional discrepancies, and hence achieve
sub-optimal results. In this paper, we propose a particle filtering framework
that can effectively address both problems by explicitly reducing the
distributional discrepancies. Specifically, our method relies on a set of
external guidance, including a small set of real images and a pre-trained
object detector, to gauge the distribution gap, and then design the resampling
weight accordingly to correct the gap. Experiments show that our methods can
effectively correct missing object errors and improve image quality in various
image generation tasks. Notably, our method outperforms the existing strongest
baseline by 5% in object occurrence and 1.0 in FID on MS-COCO. Our code is
publicly available at
https://github.com/UCSB-NLP-Chang/diffusion_resampling.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning for Asymptomatic Ratoon Stunting Disease Detection With
  Freely Available Satellite Based Multispectral Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03141v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03141v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Kane Waters, Carla Chia-ming Chen, Mostafa Rahimi Azghadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Disease detection in sugarcane, particularly the identification of
asymptomatic infectious diseases such as Ratoon Stunting Disease (RSD), is
critical for effective crop management. This study employed various machine
learning techniques to detect the presence of RSD in different sugarcane
varieties, using vegetation indices derived from freely available
satellite-based spectral data. Our results show that the Support Vector Machine
with a Radial Basis Function Kernel (SVM-RBF) was the most effective algorithm,
achieving classification accuracy between 85.64% and 96.55%, depending on the
variety. Gradient Boosting and Random Forest also demonstrated high performance
achieving accuracy between 83.33% to 96.55%, while Logistic Regression and
Quadratic Discriminant Analysis showed variable results across different
varieties. The inclusion of sugarcane variety and vegetation indices was
important in the detection of RSD. This agreed with what was identified in the
current literature. Our study highlights the potential of satellite-based
remote sensing as a cost-effective and efficient method for large-scale
sugarcane disease detection alternative to traditional manual laboratory
testing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 1 figure and 3 tables (main text), 1 figure and 2 tables
  (appendices). Submitted to "Computers and Electronics in Agriculture"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SplatSim: Zero-Shot Sim2Real Transfer of RGB Manipulation Policies Using
  Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10161v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10161v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Nomaan Qureshi, Sparsh Garg, Francisco Yandun, David Held, George Kantor, Abhisesh Silwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sim2Real transfer, particularly for manipulation policies relying on RGB
images, remains a critical challenge in robotics due to the significant domain
shift between synthetic and real-world visual data. In this paper, we propose
SplatSim, a novel framework that leverages Gaussian Splatting as the primary
rendering primitive to reduce the Sim2Real gap for RGB-based manipulation
policies. By replacing traditional mesh representations with Gaussian Splats in
simulators, SplatSim produces highly photorealistic synthetic data while
maintaining the scalability and cost-efficiency of simulation. We demonstrate
the effectiveness of our framework by training manipulation policies within
SplatSim and deploying them in the real world in a zero-shot manner, achieving
an average success rate of 86.25%, compared to 97.5% for policies trained on
real-world data. Videos can be found on our project page:
https://splatsim.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ iSeg: An Iterative Refinement-based Framework for Training-free
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03209v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03209v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Sun, Jiale Cao, Jin Xie, Fahad Shahbaz Khan, Yanwei Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stable diffusion has demonstrated strong image synthesis ability to given
text descriptions, suggesting it to contain strong semantic clue for grouping
objects. The researchers have explored employing stable diffusion for
training-free segmentation. Most existing approaches refine cross-attention map
by self-attention map once, demonstrating that self-attention map contains
useful semantic information to improve segmentation. To fully utilize
self-attention map, we present a deep experimental analysis on iteratively
refining cross-attention map with self-attention map, and propose an effective
iterative refinement framework for training-free segmentation, named iSeg. The
proposed iSeg introduces an entropy-reduced self-attention module that utilizes
a gradient descent scheme to reduce the entropy of self-attention map, thereby
suppressing the weak responses corresponding to irrelevant global information.
Leveraging the entropy-reduced self-attention module, our iSeg stably improves
refined cross-attention map with iterative refinement. Further, we design a
category-enhanced cross-attention module to generate accurate cross-attention
map, providing a better initial input for iterative refinement. Extensive
experiments across different datasets and diverse segmentation tasks reveal the
merits of proposed contributions, leading to promising performance on diverse
segmentation tasks. For unsupervised semantic segmentation on Cityscapes, our
iSeg achieves an absolute gain of 3.8% in terms of mIoU compared to the best
existing training-free approach in literature. Moreover, our proposed iSeg can
support segmentation with different kinds of images and interactions. The
project is available at https://linsun449.github.io/iSeg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://linsun449.github.io/iSeg/ Code:
  https://github.com/linsun449/iseg.code</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frame-Voyager: Learning to Query Frames for Video Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03226v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03226v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicheng Yu, Chengkai Jin, Huanyu Wang, Zhenghao Chen, Sheng Jin, Zhongrong Zuo, Xiaolei Xu, Zhenbang Sun, Bingni Zhang, Jiawei Wu, Hao Zhang, Qianru Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Large Language Models (Video-LLMs) have made remarkable progress in
video understanding tasks. However, they are constrained by the maximum length
of input tokens, making it impractical to input entire videos. Existing frame
selection approaches, such as uniform frame sampling and text-frame retrieval,
fail to account for the information density variations in the videos or the
complex instructions in the tasks, leading to sub-optimal performance. In this
paper, we propose Frame-Voyager that learns to query informative frame
combinations, based on the given textual queries in the task. To train
Frame-Voyager, we introduce a new data collection and labeling pipeline, by
ranking frame combinations using a pre-trained Video-LLM. Given a video of M
frames, we traverse its T-frame combinations, feed them into a Video-LLM, and
rank them based on Video-LLM's prediction losses. Using this ranking as
supervision, we train Frame-Voyager to query the frame combinations with lower
losses. In experiments, we evaluate Frame-Voyager on four Video Question
Answering benchmarks by plugging it into two different Video-LLMs. The
experimental results demonstrate that Frame-Voyager achieves impressive results
in all settings, highlighting its potential as a plug-and-play solution for
Video-LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perceptual Piercing: Human Visual Cue-based Object Detection in Low
  Visibility Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01225v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01225v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashutosh Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study proposes a novel deep learning framework inspired by atmospheric
scattering and human visual cortex mechanisms to enhance object detection under
poor visibility scenarios such as fog, smoke, and haze. These conditions pose
significant challenges for object recognition, impacting various sectors,
including autonomous driving, aviation management, and security systems. The
objective is to enhance the precision and reliability of detection systems
under adverse environmental conditions. The research investigates the
integration of human-like visual cues, particularly focusing on selective
attention and environmental adaptability, to ascertain their impact on object
detection's computational efficiency and accuracy. This paper proposes a
multi-tiered strategy that integrates an initial quick detection process,
followed by targeted region-specific dehazing, and concludes with an in-depth
detection phase. The approach is validated using the Foggy Cityscapes,
RESIDE-beta (OTS and RTTS) datasets and is anticipated to set new performance
standards in detection accuracy while significantly optimizing computational
efficiency. The findings offer a viable solution for enhancing object detection
in poor visibility and contribute to the broader understanding of integrating
human visual principles into deep learning algorithms for intricate visual
recognition challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CPFD: Confidence-aware Privileged Feature Distillation for Short Video
  Classification <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03038v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03038v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinghao Shi, Xiang Shen, Kaili Zhao, Xuedong Wang, Vera Wen, Zixuan Wang, Yifan Wu, Zhixin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense features, customized for different business scenarios, are essential in
short video classification. However, their complexity, specific adaptation
requirements, and high computational costs make them resource-intensive and
less accessible during online inference. Consequently, these dense features are
categorized as `Privileged Dense Features'.Meanwhile, end-to-end multi-modal
models have shown promising results in numerous computer vision tasks. In
industrial applications, prioritizing end-to-end multi-modal features, can
enhance efficiency but often leads to the loss of valuable information from
historical privileged dense features. To integrate both features while
maintaining efficiency and manageable resource costs, we present
Confidence-aware Privileged Feature Distillation (CPFD), which empowers
features of an end-to-end multi-modal model by adaptively distilling privileged
features during training. Unlike existing privileged feature distillation (PFD)
methods, which apply uniform weights to all instances during distillation,
potentially causing unstable performance across different business scenarios
and a notable performance gap between teacher model (Dense Feature enhanced
multimodal-model DF-X-VLM) and student model (multimodal-model only X-VLM), our
CPFD leverages confidence scores derived from the teacher model to adaptively
mitigate the performance variance with the student model. We conducted
extensive offline experiments on five diverse tasks demonstrating that CPFD
improves the video classification F1 score by 6.76% compared with end-to-end
multimodal-model (X-VLM) and by 2.31% with vanilla PFD on-average. And it
reduces the performance gap by 84.6% and achieves results comparable to teacher
model DF-X-VLM. The effectiveness of CPFD is further substantiated by online
experiments, and our framework has been deployed in production systems for over
a dozen models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera ready for CIKM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Trustworthiness in Foundation Models for Medical Image
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15851v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15851v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Congzhen Shi, Ryan Rezai, Jiaxi Yang, Qi Dou, Xiaoxiao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of foundation models in medical imaging represents a
significant leap toward enhancing diagnostic accuracy and personalized
treatment. However, the deployment of foundation models in healthcare
necessitates a rigorous examination of their trustworthiness, encompassing
privacy, robustness, reliability, explainability, and fairness. The current
body of survey literature on foundation models in medical imaging reveals
considerable gaps, particularly in the area of trustworthiness. Additionally,
existing surveys on the trustworthiness of foundation models do not adequately
address their specific variations and applications within the medical imaging
domain. This survey aims to fill that gap by presenting a novel taxonomy of
foundation models used in medical imaging and analyzing the key motivations for
ensuring their trustworthiness. We review current research on foundation models
in major medical imaging applications, focusing on segmentation, medical report
generation, medical question and answering (Q\&A), and disease diagnosis. These
areas are highlighted because they have seen a relatively mature and
substantial number of foundation models compared to other applications. We
focus on literature that discusses trustworthiness in medical image analysis
manuscripts. We explore the complex challenges of building trustworthy
foundation models for each application, summarizing current concerns and
strategies for enhancing trustworthiness. Furthermore, we examine the potential
of these models to revolutionize patient care. Our analysis underscores the
imperative for advancing towards trustworthy AI in medical image analysis,
advocating for a balanced approach that fosters innovation while ensuring
ethical and equitable healthcare delivery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BDetCLIP: Multimodal <span class="highlight-title">Prompt</span>ing Contrastive Test-Time Backdoor Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15269v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15269v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Niu, Shuo He, Qi Wei, Zongyu Wu, Feng Liu, Lei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal contrastive learning methods (e.g., CLIP) have shown impressive
zero-shot classification performance due to their strong ability to joint
representation learning for visual and textual modalities. However, recent
research revealed that multimodal contrastive learning on poisoned pre-training
data with a small proportion of maliciously backdoored data can induce
backdoored CLIP that could be attacked by inserted triggers in downstream tasks
with a high success rate. To defend against backdoor attacks on CLIP, existing
defense methods focus on either the pre-training stage or the fine-tuning
stage, which would unfortunately cause high computational costs due to numerous
parameter updates. In this paper, we provide the first attempt at a
computationally efficient backdoor detection method to defend against
backdoored CLIP in the inference stage. We empirically find that the visual
representations of backdoored images are insensitive to both benign and
malignant changes in class description texts. Motivated by this observation, we
propose BDetCLIP, a novel test-time backdoor detection method based on
contrastive prompting. Specifically, we first prompt the language model (e.g.,
GPT-4) to produce class-related description texts (benign) and class-perturbed
random texts (malignant) by specially designed instructions. Then, the
distribution difference in cosine similarity between images and the two types
of class description texts can be used as the criterion to detect backdoor
samples. Extensive experiments validate that our proposed BDetCLIP is superior
to state-of-the-art backdoor detection methods, in terms of both effectiveness
and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InFusionSurf: Refining Neural RGB-D Surface Reconstruction Using
  Per-Frame Intrinsic Refinement and TSDF Fusion Prior Learning <span class="chip">ICME'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04508v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04508v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seunghwan Lee, Gwanmo Park, Hyewon Son, Jiwon Ryu, Han Joo Chae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce InFusionSurf, an innovative enhancement for neural radiance
field (NeRF) frameworks in 3D surface reconstruction using RGB-D video frames.
Building upon previous methods that have employed feature encoding to improve
optimization speed, we further improve the reconstruction quality with minimal
impact on optimization time by refining depth information. InFusionSurf
addresses camera motion-induced blurs in each depth frame through a per-frame
intrinsic refinement scheme. It incorporates the truncated signed distance
field (TSDF) Fusion, a classical real-time 3D surface reconstruction method, as
a pretraining tool for the feature grid, enhancing reconstruction details and
training speed. Comparative quantitative and qualitative analyses show that
InFusionSurf reconstructs scenes with high accuracy while maintaining
optimization efficiency. The effectiveness of our intrinsic refinement and TSDF
Fusion-based pretraining is further validated through an ablation study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICME'24 (Oral), Project page:
  https://rokit-healthcare.github.io/InFusionSurf/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpinQuant: LLM quantization with learned rotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16406v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16406v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, Tijmen Blankevoort
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization (PTQ) techniques applied to weights, activations,
and the KV cache greatly reduce memory usage, latency, and power consumption of
Large Language Models (LLMs), but may lead to large quantization errors when
outliers are present. Rotating activation or weight matrices helps remove
outliers and benefits quantization. In this work, we identify a collection of
applicable rotation parameterizations that lead to identical outputs in
full-precision Transformer architectures while enhancing quantization accuracy.
In addition, we find that some random rotations lead to much better
quantization than others, with an up to 13 points difference in downstream
zero-shot reasoning performance. As a result, we propose SpinQuant, a novel
approach that incorporates learned rotation matrices for optimal quantized
network accuracy. With 4-bit quantization of weight, activation, and KV-cache,
SpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full
precision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by
19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also
outperforms concurrent work QuaRot, which applies random rotations to remove
outliers. In particular, for LLaMA-3 8B models that are hard to quantize,
SpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PACE: marrying generalization in PArameter-efficient fine-tuning with
  Consistency rEgularization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17137v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17137v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Ni, Shan Zhang, Piotr Koniusz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained vision
transformers to downstream tasks. However, the optimization for tasks
performance often comes at the cost of generalizability in fine-tuned models.
To address this issue, we theoretically connect smaller weight gradient norms
during training and larger datasets to the improved model generalization.
Motivated by this connection, we propose reducing gradient norms for enhanced
generalization and aligning fine-tuned model with the pre-trained counterpart
to retain knowledge from large-scale pre-training data. Yet, naive alignment
does not guarantee gradient reduction and can potentially cause gradient
explosion, complicating efforts to manage gradients. To address such issues, we
propose PACE, marrying generalization of PArameter-efficient fine-tuning with
Consistency rEgularization. We perturb features learned from the adapter with
the multiplicative noise and ensure the fine-tuned model remains consistent for
same sample under different perturbations. Theoretical analysis shows that PACE
not only implicitly regularizes gradients for enhanced generalization, but also
implicitly aligns the fine-tuned and pre-trained models to retain knowledge.
Experimental evidence supports our theories. PACE outperforms existing PEFT
methods in four visual adaptation tasks: VTAB-1k, FGVC, few-shot learning and
domain adaptation. Code will be available at
https://github.com/MaxwellYaoNi/PACE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 as a spotlight. This preliminary version
  will soon be extended with the experiments and analyses from the rebuttal</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">19</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Job Interview Preparation Through Immersive Experiences Using
  Photorealistic, AI-powered Metahuman Avatars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navid Ashrafi, Francesco Vona, Carina Ringsdorf, Christian Hertel, Luca Toni, Sarina Kailer, Alice Bartels, Tanja Kojic, Jan-Niklas Voigt-Antons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study will investigate the user experience while interacting with highly
photorealistic virtual job interviewer avatars in Virtual Reality (VR),
Augmented Reality (AR), and on a 2D screen. Having a precise speech recognition
mechanism, our virtual character performs a mock-up software engineering job
interview to adequately immerse the user in a life-like scenario. To evaluate
the efficiency of our system, we measure factors such as the provoked level of
anxiety, social presence, self-esteem, and intrinsic motivation. This research
is a work in progress with a prospective within-subject user study including
approximately 40 participants. All users will engage with three job interview
conditions (VR, AR, and desktop) and provide their feedback. Additionally,
users' bio-physical responses will be collected using a biosensor to measure
the level of anxiety during the job interview.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2-page ISMAR poster paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-Feedback Efficient Reinforcement Learning for Online Diffusion
  Model Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayano Hiranaka, Shang-Fu Chen, Chieh-Hsin Lai, Dongjun Kim, Naoki Murata, Takashi Shibuya, Wei-Hsiang Liao, Shao-Hua Sun, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controllable generation through Stable Diffusion (SD) fine-tuning aims to
improve fidelity, safety, and alignment with human guidance. Existing
reinforcement learning from human feedback methods usually rely on predefined
heuristic reward functions or pretrained reward models built on large-scale
datasets, limiting their applicability to scenarios where collecting such data
is costly or difficult. To effectively and efficiently utilize human feedback,
we develop a framework, HERO, which leverages online human feedback collected
on the fly during model learning. Specifically, HERO features two key
mechanisms: (1) Feedback-Aligned Representation Learning, an online training
method that captures human feedback and provides informative learning signals
for fine-tuning, and (2) Feedback-Guided Image Generation, which involves
generating images from SD's refined initialization samples, enabling faster
convergence towards the evaluator's intent. We demonstrate that HERO is 4x more
efficient in online feedback for body part anomaly correction compared to the
best existing method. Additionally, experiments show that HERO can effectively
handle tasks like reasoning, counting, personalization, and reducing NSFW
content with only 0.5K online feedback.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Role of Governments in Increasing Interconnected Post-Deployment
  Monitoring of AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Merlin Stein, Jamie Bernardi, Connor Dunlop
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language-based AI systems are diffusing into society, bringing positive and
negative impacts. Mitigating negative impacts depends on accurate impact
assessments, drawn from an empirical evidence base that makes causal
connections between AI usage and impacts. Interconnected post-deployment
monitoring combines information about model integration and use, application
use, and incidents and impacts. For example, inference time monitoring of
chain-of-thought reasoning can be combined with long-term monitoring of
sectoral AI diffusion, impacts and incidents. Drawing on information sharing
mechanisms in other industries, we highlight example data sources and specific
data points that governments could collect to inform AI risk management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Music-triggered fashion design: from songs to the metaverse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martina Delgado, Marta Llopart, Eva Sarabia, Sandra Taboada, Pol Vierge, Fernando Vilariño, Joan Moya Kohler, Julieta Grimberg Golijov, Matías Bilkis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of increasingly-growing virtual realities poses unprecedented
opportunities and challenges to different societies. Artistic collectives are
not an exception, and we here aim to put special attention into musicians.
Compositions, lyrics and even show-advertisements are constituents of a message
that artists transmit about their reality. As such, artistic creations are
ultimately linked to feelings and emotions, with aesthetics playing a crucial
role when it comes to transmit artist's intentions. In this context, we here
analyze how virtual realities can help to broaden the opportunities for
musicians to bridge with their audiences, by devising a dynamical
fashion-design recommendation system inspired by sound stimulus. We present our
first steps towards re-defining musical experiences in the metaverse, opening
up alternative opportunities for artists to connect both with real and virtual
(\textit{e.g.} machine-learning agents operating in the metaverse) in
potentially broader ways.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why am I seeing this: Democratizing End User Auditing for Online Content
  Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04917v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04917v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoran Chen, Leyang Li, Luke Cao, Yanfang Ye, Tianshi Li, Yaxing Yao, Toby Jia-jun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized recommendation systems tailor content based on user attributes,
which are either provided or inferred from private data. Research suggests that
users often hypothesize about reasons behind contents they encounter (e.g., "I
see this jewelry ad because I am a woman"), but they lack the means to confirm
these hypotheses due to the opaqueness of these systems. This hinders informed
decision-making about privacy and system use and contributes to the lack of
algorithmic accountability. To address these challenges, we introduce a new
interactive sandbox approach. This approach creates sets of synthetic user
personas and corresponding personal data that embody realistic variations in
personal attributes, allowing users to test their hypotheses by observing how a
website's algorithms respond to these personas. We tested the sandbox in the
context of targeted advertisement. Our user study demonstrates its usability,
usefulness, and effectiveness in empowering end-user auditing in a case study
of targeting ads.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Working with Mixed Reality in Public: Effects of Virtual Display Layouts
  on Productivity, Feeling of Safety, and Social Acceptability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Janne Kaeder, Maurizio Vergari, Verena Biener, Tanja Kojić, Jens Grubert, Sebastian Möller, Jan-Niklas Voigt-Antons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, Mixed Reality (MR) headsets are a game-changer for knowledge work.
Unlike stationary monitors, MR headsets allow users to work with large virtual
displays anywhere they wear the headset, whether in a professional office, a
public setting like a cafe, or a quiet space like a library. This study
compares four different layouts (eye level-close, eye level-far, below eye
level-close, below eye level-far) of virtual displays regarding feelings of
safety, perceived productivity, and social acceptability when working with MR
in public. We test which layout is most preferred by users and seek to
understand which factors affect users' layout preferences. The aim is to derive
useful insights for designing better MR layouts. A field study in a public
library was conducted using a within-subject design. While the participants
interact with a layout, they are asked to work on a planning task. The results
from a repeated measure ANOVA show a statistically significant effect on
productivity but not on safety and social acceptability. Additionally, we
report preferences expressed by the users regarding the layouts and using MR in
public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Part of the 23rd annual IEEE International Symposium on Mixed and
  Augmented Reality (ISMAR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single Vs Dual: Influence of the Number of Displays on User Experience
  within Virtually Embodied Conversational Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navid Ashrafi, Francesco Vona, Philipp Graf, Philipp Harnisch, Sina Hinzmann, Jan-Niklas Voigt-Antons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current research evaluates user experience and preference when
interacting with a patient-reported outcome measure (PROM) healthcare
application displayed on a single tablet in comparison to interaction with the
same application distributed across two tablets. We conducted a within-subject
user study with 43 participants who engaged with and rated the usability of our
system and participated in a post-experiment interview to collect subjective
data. Our findings showed significantly higher usability and higher pragmatic
quality ratings for the single tablet condition. However, some users attribute
a higher level of presence to the avatar and prefer it to be placed on a second
tablet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In 30th ACM Symposium on Virtual Reality Software and Technology
  (VRST 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Stretchable Electrostatic Tactile Surface 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoto Takayanagi, Naoji Matsuhisa, Yuki Hashimoto, Yuta Sugiura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tactile sensation is essential for humans to recognize objects. Various
devices have been developed in the past for tactile presentation by
electrostatic force, which are easy to configure devices, but there is
currently no such device that features stretchability. Considering that the
device is worn over the joints of a human body or robot, it is extremely
important that the device itself be stretchable. In this study, we propose a
stretchable electrostatic tactile surface comprising a stretchable transparent
electrode and a stretchable insulating film that can be stretched to a maximum
of 50%. This means that when attached to the human body, this surface can
respond to the expansion and contraction that occur due to joint movements.
This surface can also provide tactile information in response to deformation
such as pushing and pulling. As a basic investigation, we measured the lower
limit of voltage that can be perceived by changing the configuration of the
surface and evaluated the states of stretching and contraction. We also
investigated and modeled the relationship between the voltage and the perceived
intensity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guidance of the Center of Pressure Using Haptic Presentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yohei Kawasaki, Yuta Sugiura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately instructing posture and the position of the body's center of
gravity is challenging. In this study, we propose a system that utilizes haptic
feedback to induce the Center of Pressure (CoP) movement. The Wii Balance Board
is employed to sense the CoP, and vibration motors are used for haptic
feedback. To provide a comparison, inductions were also performed using visual
and auditory feedback, and the time required for induction was measured.
Additionally, after the experiments, a questionnaire survey was conducted.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Gestural Interaction with a Cushion Interface for Smart Home
  Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuri Suzuki, Kaho Kato, Naomi Furui, Daisuke Sakamoto, Yuta Sugiura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this research, we aim to realize cushion interface for operating smart
home. We designed user-defined gestures using cushion and developed gesture
recognition system. We asked some users to make gestures using cushions for
operating home appliances and determined user-defined gesture sets. We
developed two methods for gesture identification. The First, We inserted sensor
modules consisting of photo reflective sensors and acceleration sensor inside a
cushion. The second, we embedded the acceleration sensor arrays in the cushion
cover. Gesture recognizer was implemented using Convolutional Neural Networks
(CNN). To evaluate our method, We conducted an experiment to measure
recognition accuracy. Results showed that an average accuracy was 94.8% when
training for each user, and an average accuracy of 91.3% when testing with a
user that did not exist in the training data set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The LLM Effect: Are Humans Truly Using LLMs, or Are They Being
  Influenced By Them Instead? <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander S. Choi, Syeda Sabrina Akter, JP Singh, Antonios Anastasopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown capabilities close to human
performance in various analytical tasks, leading researchers to use them for
time and labor-intensive analyses. However, their capability to handle highly
specialized and open-ended tasks in domains like policy studies remains in
question. This paper investigates the efficiency and accuracy of LLMs in
specialized tasks through a structured user study focusing on Human-LLM
partnership. The study, conducted in two stages-Topic Discovery and Topic
Assignment-integrates LLMs with expert annotators to observe the impact of LLM
suggestions on what is usually human-only analysis. Results indicate that
LLM-generated topic lists have significant overlap with human generated topic
lists, with minor hiccups in missing document-specific topics. However, LLM
suggestions may significantly improve task completion speed, but at the same
time introduce anchoring bias, potentially affecting the depth and nuance of
the analysis, raising a critical question about the trade-off between increased
efficiency and the risk of biased analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP Main 2024. First two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does the Infamous Pie Chart Really Hurt Decision-Making in the Real
  World? Assessing the Role of Visualization in High-Level Academic Decisions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Li, Emery D. Berger, Minsuk Kahng, Cindy Xiong Bearfield
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visualization design influences how people perceive data patterns, yet most
research focuses on low-level analytic tasks, such as finding correlations.
Existing work has criticized pie charts for their perceptual limitations.
However, simpler visualizations like pie and bar charts are widely used for
real-world decision-making, such as choosing schools or advisors. As a case
study, we examine whether pie charts hurt high-level decisions compared to bar
charts, using the website that presents academic data, CSRankings.org. By
comparing the impact of pie charts versus bar charts on users' impressions of
faculty productivity and projected workload, we found no significant
differences in decisions among over 300 participants. Our findings challenge
traditional views on visualization design, emphasizing the need for real-world
use cases in evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Creative Beam Search: LLM-as-a-Judge For Improving Response Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00099v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00099v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Franceschelli, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are revolutionizing several areas, including artificial
creativity. However, the process of generation in machines profoundly diverges
from that observed in humans. In particular, machine generation is
characterized by a lack of intentionality and an underlying creative process.
We propose a method called Creative Beam Search that uses Diverse Beam Search
and LLM-as-a-Judge to perform response generation and response validation. The
results of a qualitative experiment show how our approach can provide better
output than standard sampling techniques. We also show that the response
validation step is a necessary complement to the response generation step.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented as a short paper at the 15th International Conference on
  Computational Creativity (ICCC'24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When "A Helpful Assistant" Is Not Really Helpful: Personas in System
  <span class="highlight-title">Prompt</span>s Do Not Improve Performances of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10054v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10054v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingqian Zheng, Jiaxin Pei, Lajanugen Logeswaran, Moontae Lee, David Jurgens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompting serves as the major way humans interact with Large Language Models
(LLM). Commercial AI systems commonly define the role of the LLM in system
prompts. For example, ChatGPT uses "You are a helpful assistant" as part of its
default system prompt. Despite current practices of adding personas to system
prompts, it remains unclear how different personas affect a model's performance
on objective tasks. In this study, we present a systematic evaluation of
personas in system prompts. We curate a list of 162 roles covering 6 types of
interpersonal relationships and 8 domains of expertise. Through extensive
analysis of 4 popular families of LLMs and 2,410 factual questions, we
demonstrate that adding personas in system prompts does not improve model
performance across a range of questions compared to the control setting where
no persona is added. Nevertheless, further analysis suggests that the gender,
type, and domain of the persona can all influence the resulting prediction
accuracies. We further experimented with a list of persona search strategies
and found that, while aggregating results from the best persona for each
question significantly improves prediction accuracy, automatically identifying
the best persona is challenging, with predictions often performing no better
than random selection. Overall, our findings suggest that while adding a
persona may lead to performance gains in certain settings, the effect of each
persona can be largely random. Code and data are available at
https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Dynamic Model of Performative Human-ML Collaboration: Theory and
  Empirical Evidence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13753v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13753v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Sühr, Samira Samadi, Chiara Farronato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) models are increasingly used in various applications,
from recommendation systems in e-commerce to diagnosis prediction in
healthcare. In this paper, we present a novel dynamic framework for thinking
about the deployment of ML models in a performative, human-ML collaborative
system. In our framework, the introduction of ML recommendations changes the
data-generating process of human decisions, which are only a proxy to the
ground truth and which are then used to train future versions of the model. We
show that this dynamic process in principle can converge to different stable
points, i.e. where the ML model and the Human+ML system have the same
performance. Some of these stable points are suboptimal with respect to the
actual ground truth. As a proof of concept, we conduct an empirical user study
with 1,408 participants. In the study, humans solve instances of the knapsack
problem with the help of machine learning predictions of varying performance.
This is an ideal setting because we can identify the actual ground truth, and
evaluate the performance of human decisions supported by ML recommendations. We
find that for many levels of ML performance, humans can improve upon the ML
predictions. We also find that the improvement could be even higher if humans
rationally followed the ML recommendations. Finally, we test whether monetary
incentives can increase the quality of human decisions, but we fail to find any
positive effect. Using our empirical data to approximate our collaborative
system suggests that the learning process would dynamically reach an
equilibrium performance that is around 92% of the maximum knapsack value. Our
results have practical implications for the deployment of ML models in contexts
where human decisions may deviate from the indisputable ground truth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 Pages and appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Auxilio and Beyond: Comparative Evaluation, Usability, and Design
  Guidelines for Head Movement-based Assistive Mouse Controllers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.04483v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.04483v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Ridwan Kabir, Mohammad Ishrak Abedin, Rizvi Ahmed, Saad Bin Ashraf, Hasan Mahmud, Md. Kamrul Hasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Upper limb disability due to neurological disorders or other factors
restricts computer interaction for affected individuals using a generic optical
mouse. This work reports the findings of a comparative evaluation of Auxilio, a
sensor-based wireless head-mounted Assistive Mouse Controller (AMC), that
facilitates computer interaction for such individuals. Combining commercially
available, low-cost motion and infrared sensors, Auxilio utilizes head
movements and cheek muscle twitches for mouse control. Its performance in
pointing tasks with subjects without motor impairments has been juxtaposed
against a commercially available and patented vision-based head-tracking AMC
developed for similar stakeholders. Furthermore, our study evaluates the
usability of Auxilio using the System Usability Scale, supplemented by a
qualitative analysis of participant interview transcripts to identify the
strengths and weaknesses of both AMCs. Experimental results demonstrate the
feasibility and effectiveness of Auxilio, and we summarize our key findings
into design guidelines for the development of similar future AMCs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 6 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "I Like Sunnie More Than I Expected!": Exploring User Expectation and
  Perception of an Anthropomorphic LLM-based Conversational Agent for
  Well-Being Support 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13803v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13803v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyi Wu, Julie Y. A. Cachia, Feixue Han, Bingsheng Yao, Tianyi Xie, Xuan Zhao, Dakuo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The human-computer interaction (HCI) research community has a longstanding
interest in exploring the mismatch between users' actual experiences and
expectation toward new technologies, for instance, large language models
(LLMs). In this study, we compared users' (N = 38) initial expectations against
their post-interaction perceptions of two LLM-powered mental well-being
intervention activity recommendation systems. Both systems have a built-in LLM
to recommend a personalized well-being intervention activity, but one system
(Sunnie) has an anthropomorphic conversational interaction design via elements
such as appearance, persona, and natural conversation. Results showed that user
engagement was high with both systems, and both systems exceeded users'
expectations along the utility dimension, highlighting AI's potential to offer
useful intervention activity recommendations. In addition, Sunnie further
outperformed the non-anthropomorphic baseline system in relational warmth.
These findings suggest that anthropomorphic conversational interaction design
may be particularly effective in fostering warmth in mental health support
contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VizGroup: An AI-Assisted Event-Driven System for Real-Time Collaborative
  Programming Learning Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08743v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08743v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohang Tang, Sam Wong, Kevin Pu, Xi Chen, Yalong Yang, Yan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Programming instructors often conduct collaborative learning activities, like
Peer Instruction, to foster a deeper understanding in students and enhance
their engagement with learning. These activities, however, may not always yield
productive outcomes due to the diversity of student mental models and their
ineffective collaboration. In this work, we introduce VizGroup, an AI-assisted
system that enables programming instructors to easily oversee students'
real-time collaborative learning behaviors during large programming courses.
VizGroup leverages Large Language Models (LLMs) to recommend event
specifications for instructors so that they can simultaneously track and
receive alerts about key correlation patterns between various collaboration
metrics and ongoing coding tasks. We evaluated VizGroup with 12 instructors in
a comparison study using a dataset collected from a Peer Instruction activity
that was conducted in a large programming lecture. The results showed that
VizGroup helped instructors effectively overview, narrow down, and track
nuances throughout students' behaviors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to UIST 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Trustworthiness in Foundation Models for Medical Image
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15851v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15851v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Congzhen Shi, Ryan Rezai, Jiaxi Yang, Qi Dou, Xiaoxiao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of foundation models in medical imaging represents a
significant leap toward enhancing diagnostic accuracy and personalized
treatment. However, the deployment of foundation models in healthcare
necessitates a rigorous examination of their trustworthiness, encompassing
privacy, robustness, reliability, explainability, and fairness. The current
body of survey literature on foundation models in medical imaging reveals
considerable gaps, particularly in the area of trustworthiness. Additionally,
existing surveys on the trustworthiness of foundation models do not adequately
address their specific variations and applications within the medical imaging
domain. This survey aims to fill that gap by presenting a novel taxonomy of
foundation models used in medical imaging and analyzing the key motivations for
ensuring their trustworthiness. We review current research on foundation models
in major medical imaging applications, focusing on segmentation, medical report
generation, medical question and answering (Q\&A), and disease diagnosis. These
areas are highlighted because they have seen a relatively mature and
substantial number of foundation models compared to other applications. We
focus on literature that discusses trustworthiness in medical image analysis
manuscripts. We explore the complex challenges of building trustworthy
foundation models for each application, summarizing current concerns and
strategies for enhancing trustworthiness. Furthermore, we examine the potential
of these models to revolutionize patient care. Our analysis underscores the
imperative for advancing towards trustworthy AI in medical image analysis,
advocating for a balanced approach that fosters innovation while ensuring
ethical and equitable healthcare delivery.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Advisor: Dynamic Data Curation for Safety Alignment of Large
  Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Wang, Ninareh Mehrabi, Palash Goyal, Rahul Gupta, Kai-Wei Chang, Aram Galstyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data is a crucial element in large language model (LLM) alignment. Recent
studies have explored using LLMs for efficient data collection. However,
LLM-generated data often suffers from quality issues, with underrepresented or
absent aspects and low-quality datapoints. To address these problems, we
propose Data Advisor, an enhanced LLM-based method for generating data that
takes into account the characteristics of the desired dataset. Starting from a
set of pre-defined principles in hand, Data Advisor monitors the status of the
generated data, identifies weaknesses in the current dataset, and advises the
next iteration of data generation accordingly. Data Advisor can be easily
integrated into existing data generation methods to enhance data quality and
coverage. Experiments on safety alignment of three representative LLMs (i.e.,
Mistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in
enhancing model safety against various fine-grained safety issues without
sacrificing model utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference. Project website:
  https://feiwang96.github.io/DataAdvisor/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers
  in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengzhao Chen, Yi Liu, Jiahao Wang, Yi Bin, Wenqi Shao, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization is essential for deploying Large Language Models (LLMs) by
enhancing memory efficiency and inference speed. Existing methods for
activation quantization mainly address channel-wise outliers, often neglecting
token-wise outliers, leading to reliance on costly per-token dynamic
quantization. To address this, we introduce PrefixQuant, a novel technique that
isolates outlier tokens offline without re-training. Specifically, PrefixQuant
identifies high-frequency outlier tokens and prefixes them in the KV cache,
preventing the generation of outlier tokens during inference and simplifying
quantization. To our knowledge, PrefixQuant is the first to enable efficient
per-tensor static quantization to outperform expensive per-token dynamic
quantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and
4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization
achieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5
common-sense reasoning tasks, outperforming previous per-token dynamic
quantization methods like QuaRot with 0.98 perplexity improvement and +5.98
points accuracy. Additionally, the inference speed of W4A4 quantized models
using PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot
models by 1.2x to 1.3x. Our code is available at
\url{https://github.com/ChenMnZ/PrefixQuant}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A PTQ method to significantly boost the performance of static
  activation quantization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Regression Conformal Prediction under Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matt Y. Cheung, Tucker J. Netherton, Laurence E. Court, Ashok Veeraraghavan, Guha Balakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty quantification is crucial to account for the imperfect
predictions of machine learning algorithms for high-impact applications.
Conformal prediction (CP) is a powerful framework for uncertainty
quantification that generates calibrated prediction intervals with valid
coverage. In this work, we study how CP intervals are affected by bias - the
systematic deviation of a prediction from ground truth values - a phenomenon
prevalent in many real-world applications. We investigate the influence of bias
on interval lengths of two different types of adjustments -- symmetric
adjustments, the conventional method where both sides of the interval are
adjusted equally, and asymmetric adjustments, a more flexible method where the
interval can be adjusted unequally in positive or negative directions. We
present theoretical and empirical analyses characterizing how symmetric and
asymmetric adjustments impact the "tightness" of CP intervals for regression
tasks. Specifically for absolute residual and quantile-based non-conformity
scores, we prove: 1) the upper bound of symmetrically adjusted interval lengths
increases by $2|b|$ where $b$ is a globally applied scalar value representing
bias, 2) asymmetrically adjusted interval lengths are not affected by bias, and
3) conditions when asymmetrically adjusted interval lengths are guaranteed to
be smaller than symmetric ones. Our analyses suggest that even if predictions
exhibit significant drift from ground truth values, asymmetrically adjusted
intervals are still able to maintain the same tightness and validity of
intervals as if the drift had never happened, while symmetric ones
significantly inflate the lengths. We demonstrate our theoretical results with
two real-world prediction tasks: sparse-view computed tomography (CT)
reconstruction and time-series weather forecasting. Our work paves the way for
more bias-robust machine learning systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures, code available at:
  https://github.com/matthewyccheung/conformal-metric</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differential <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer tends to overallocate attention to irrelevant context. In this
work, we introduce Diff Transformer, which amplifies attention to the relevant
context while canceling noise. Specifically, the differential attention
mechanism calculates attention scores as the difference between two separate
softmax attention maps. The subtraction cancels noise, promoting the emergence
of sparse attention patterns. Experimental results on language modeling show
that Diff Transformer outperforms Transformer in various settings of scaling up
model size and training tokens. More intriguingly, it offers notable advantages
in practical applications, such as long-context modeling, key information
retrieval, hallucination mitigation, in-context learning, and reduction of
activation outliers. By being less distracted by irrelevant context, Diff
Transformer can mitigate hallucination in question answering and text
summarization. For in-context learning, Diff Transformer not only enhances
accuracy but is also more robust to order permutation, which was considered as
a chronic robustness issue. The results position Diff Transformer as a highly
effective and promising architecture to advance large language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SePPO: Semi-Policy Preference Optimization for Diffusion Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daoan Zhang, Guangchen Lan, Dong-Jun Han, Wenlin Yao, Xiaoman Pan, Hongming Zhang, Mingxiao Li, Pengcheng Chen, Yu Dong, Christopher Brinton, Jiebo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning from human feedback (RLHF) methods are emerging as a
way to fine-tune diffusion models (DMs) for visual generation. However,
commonly used on-policy strategies are limited by the generalization capability
of the reward model, while off-policy approaches require large amounts of
difficult-to-obtain paired human-annotated data, particularly in visual
generation tasks. To address the limitations of both on- and off-policy RLHF,
we propose a preference optimization method that aligns DMs with preferences
without relying on reward models or paired human-annotated data. Specifically,
we introduce a Semi-Policy Preference Optimization (SePPO) method. SePPO
leverages previous checkpoints as reference models while using them to generate
on-policy reference samples, which replace "losing images" in preference pairs.
This approach allows us to optimize using only off-policy "winning images."
Furthermore, we design a strategy for reference model selection that expands
the exploration in the policy space. Notably, we do not simply treat reference
samples as negative examples for learning. Instead, we design an anchor-based
criterion to assess whether the reference samples are likely to be winning or
losing images, allowing the model to selectively learn from the generated
reference samples. This approach mitigates performance degradation caused by
the uncertainty in reference sample quality. We validate SePPO across both
text-to-image and text-to-video benchmarks. SePPO surpasses all previous
approaches on the text-to-image benchmarks and also demonstrates outstanding
performance on the text-to-video benchmarks. Code will be released in
https://github.com/DwanZhang-AI/SePPO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLEE: A Unified Framework and Benchmark for Language-based Economic
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eilam Shapira, Omer Madmon, Itamar Reinman, Samuel Joseph Amouyal, Roi Reichart, Moshe Tennenholtz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) show significant potential in economic and
strategic interactions, where communication via natural language is often
prevalent. This raises key questions: Do LLMs behave rationally? Can they mimic
human behavior? Do they tend to reach an efficient and fair outcome? What is
the role of natural language in the strategic interaction? How do
characteristics of the economic environment influence these dynamics? These
questions become crucial concerning the economic and societal implications of
integrating LLM-based agents into real-world data-driven systems, such as
online retail platforms and recommender systems. While the ML community has
been exploring the potential of LLMs in such multi-agent setups, varying
assumptions, design choices and evaluation criteria across studies make it
difficult to draw robust and meaningful conclusions. To address this, we
introduce a benchmark for standardizing research on two-player, sequential,
language-based games. Inspired by the economic literature, we define three base
families of games with consistent parameterization, degrees of freedom and
economic measures to evaluate agents' performance (self-gain), as well as the
game outcome (efficiency and fairness). We develop an open-source framework for
interaction simulation and analysis, and utilize it to collect a dataset of LLM
vs. LLM interactions across numerous game configurations and an additional
dataset of human vs. LLM interactions. Through extensive experimentation, we
demonstrate how our framework and dataset can be used to: (i) compare the
behavior of LLM-based agents to human players in various economic contexts;
(ii) evaluate agents in both individual and collective performance measures;
and (iii) quantify the effect of the economic characteristics of the
environments on the behavior of agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Micro-Narratives <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mourad Heddaya, Qingcheng Zeng, Chenhao Tan, Rob Voigt, Alexander Zentefis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to classify causal micro-narratives from text.
These narratives are sentence-level explanations of the cause(s) and/or
effect(s) of a target subject. The approach requires only a subject-specific
ontology of causes and effects, and we demonstrate it with an application to
inflation narratives. Using a human-annotated dataset spanning historical and
contemporary US news articles for training, we evaluate several large language
models (LLMs) on this multi-label classification task. The best-performing
model--a fine-tuned Llama 3.1 8B--achieves F1 scores of 0.87 on narrative
detection and 0.71 on narrative classification. Comprehensive error analysis
reveals challenges arising from linguistic ambiguity and highlights how model
errors often mirror human annotator disagreements. This research establishes a
framework for extracting causal micro-narratives from real-world data, with
wide-ranging applications to social science research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Workshop on Narrative Understanding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Xiao, Shujian Zhang, Wenxuan Zhou, Marzyeh Ghassemi, Sanqiang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To induce desired behaviors in large language models (LLMs) for
interaction-driven tasks, the instruction-tuning stage typically trains LLMs on
instruction-response pairs using the next-token prediction (NTP) loss. Previous
work aiming to improve instruction-tuning performance often emphasizes the need
for higher-quality supervised fine-tuning (SFT) datasets, which typically
involves expensive data filtering with proprietary LLMs or labor-intensive data
generation by human annotators. However, these approaches do not fully leverage
the datasets' intrinsic properties, resulting in high computational and labor
costs, thereby limiting scalability and performance gains. In this paper, we
propose SFTMix, a novel recipe that elevates instruction-tuning performance
beyond the conventional NTP paradigm, without the need for well-curated
datasets. Observing that LLMs exhibit uneven confidence across the semantic
representation space, we argue that examples with different confidence levels
should play distinct roles during the instruction-tuning process. Based on this
insight, SFTMix leverages training dynamics to identify examples with varying
confidence levels, then applies a Mixup-based regularization to mitigate
overfitting on confident examples while propagating supervision signals to
improve learning on relatively unconfident ones. This approach enables SFTMix
to significantly outperform NTP across a wide range of instruction-following
and healthcare domain-specific SFT tasks, demonstrating its adaptability to
diverse LLM families and scalability to datasets of any size. Comprehensive
ablation studies further verify the robustness of SFTMix's design choices,
underscoring its versatility in consistently enhancing performance across
different LLMs and datasets in broader natural language processing
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimO Loss: Anchor-Free Contrastive Loss for Fine-Grained Supervised
  Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha Bouhsine, Imad El Aaroussi, Atik Faysal, Wang Huaxia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel anchor-free contrastive learning (AFCL) method
leveraging our proposed Similarity-Orthogonality (SimO) loss. Our approach
minimizes a semi-metric discriminative loss function that simultaneously
optimizes two key objectives: reducing the distance and orthogonality between
embeddings of similar inputs while maximizing these metrics for dissimilar
inputs, facilitating more fine-grained contrastive learning. The AFCL method,
powered by SimO loss, creates a fiber bundle topological structure in the
embedding space, forming class-specific, internally cohesive yet orthogonal
neighborhoods. We validate the efficacy of our method on the CIFAR-10 dataset,
providing visualizations that demonstrate the impact of SimO loss on the
embedding space. Our results illustrate the formation of distinct, orthogonal
class neighborhoods, showcasing the method's ability to create well-structured
embeddings that balance class separation with intra-class variability. This
work opens new avenues for understanding and leveraging the geometric
properties of learned representations in various machine learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SymmetryLens: A new candidate paradigm for unsupervised symmetry
  learning via locality and equivariance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Onur Efe, Arkadas Ozakin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a new, unsupervised symmetry learning method that starts with raw
data, and gives the minimal (discrete) generator of an underlying Lie group of
symmetries, together with a symmetry equivariant representation of the data.
The method is able to learn the pixel translation operator from a dataset with
only an approximate translation symmetry, and can learn quite different types
of symmetries which are not apparent to the naked eye, equally well. The method
is based on the formulation of an information-theoretic loss function that
measures both the degree to which the dataset is symmetric under a given
candidate symmetry, and also, the degree of locality of the samples in the
dataset with respect to this symmetry. We demonstrate that this coupling
between symmetry and locality, together with a special optimization technique
developed for entropy estimation, results in a highly stable system that gives
reproducible results. The symmetry actions we consider are group
representations, however, we believe the approach has the potential to be
generalized to more general, nonlinear actions of non-commutative Lie groups.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have sparked interest in
their formal reasoning capabilities, particularly in mathematics. The GSM8K
benchmark is widely used to assess the mathematical reasoning of models on
grade-school-level questions. While the performance of LLMs on GSM8K has
significantly improved in recent years, it remains unclear whether their
mathematical reasoning capabilities have genuinely advanced, raising questions
about the reliability of the reported metrics. To address these concerns, we
conduct a large-scale study on several SOTA open and closed models. To overcome
the limitations of existing evaluations, we introduce GSM-Symbolic, an improved
benchmark created from symbolic templates that allow for the generation of a
diverse set of questions. GSM-Symbolic enables more controllable evaluations,
providing key insights and more reliable metrics for measuring the reasoning
capabilities of models.Our findings reveal that LLMs exhibit noticeable
variance when responding to different instantiations of the same question.
Specifically, the performance of all models declines when only the numerical
values in the question are altered in the GSM-Symbolic benchmark. Furthermore,
we investigate the fragility of mathematical reasoning in these models and show
that their performance significantly deteriorates as the number of clauses in a
question increases. We hypothesize that this decline is because current LLMs
cannot perform genuine logical reasoning; they replicate reasoning steps from
their training data. Adding a single clause that seems relevant to the question
causes significant performance drops (up to 65%) across all state-of-the-art
models, even though the clause doesn't contribute to the reasoning chain needed
for the final answer. Overall, our work offers a more nuanced understanding of
LLMs' capabilities and limitations in mathematical reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ETGL-DDPG: A Deep Deterministic Policy Gradient Algorithm for Sparse
  Reward Continuous Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Futuhi, Shayan Karimi, Chao Gao, Martin Müller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider deep deterministic policy gradient (DDPG) in the context of
reinforcement learning with sparse rewards. To enhance exploration, we
introduce a search procedure, \emph{${\epsilon}{t}$-greedy}, which generates
exploratory options for exploring less-visited states. We prove that search
using $\epsilon t$-greedy has polynomial sample complexity under mild MDP
assumptions. To more efficiently use the information provided by rewarded
transitions, we develop a new dual experience replay buffer framework,
\emph{GDRB}, and implement \emph{longest n-step returns}. The resulting
algorithm, \emph{ETGL-DDPG}, integrates all three techniques: \bm{$\epsilon
t$}-greedy, \textbf{G}DRB, and \textbf{L}ongest $n$-step, into DDPG. We
evaluate ETGL-DDPG on standard benchmarks and demonstrate that it outperforms
DDPG, as well as other state-of-the-art methods, across all tested
sparse-reward continuous environments. Ablation studies further highlight how
each strategy individually enhances the performance of DDPG in this setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cookbook: A framework for improving LLM generative abilities via
  programmatic data generating templates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avanika Narayan, Mayee F. Chen, Kush Bhatia, Christopher Ré
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) on instruction datasets is a common
way to improve their generative capabilities. However, instruction datasets can
be expensive and time-consuming to manually curate, and while LLM-generated
data is less labor-intensive, it may violate user privacy agreements or terms
of service of LLM providers. Therefore, we seek a way of constructing
instruction datasets with samples that are not generated by humans or LLMs but
still improve LLM generative capabilities. In this work, we introduce Cookbook,
a framework that programmatically generates training data consisting of simple
patterns over random tokens, resulting in a scalable, cost-effective approach
that avoids legal and privacy issues. First, Cookbook uses a template -- a data
generating Python function -- to produce training data that encourages the
model to learn an explicit pattern-based rule that corresponds to a desired
task. We find that fine-tuning on Cookbook-generated data is able to improve
performance on its corresponding task by up to 52.7 accuracy points. Second,
since instruction datasets improve performance on multiple downstream tasks
simultaneously, Cookbook algorithmically learns how to mix data from various
templates to optimize performance on multiple tasks. On the standard multi-task
GPT4ALL evaluation suite, Mistral-7B fine-tuned using a Cookbook-generated
dataset attains the best accuracy on average compared to other 7B parameter
instruction-tuned models and is the best performing model on 3 out of 8 tasks.
Finally, we analyze when and why Cookbook improves performance and present a
metric that allows us to verify that the improvement is largely explained by
the model's generations adhering better to template rules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Precise Model Benchmarking with Only a Few Observations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Fogliato, Pratik Patil, Nil-Jana Akpinar, Mathew Monfort
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we precisely estimate a large language model's (LLM) accuracy on
questions belonging to a specific topic within a larger question-answering
dataset? The standard direct estimator, which averages the model's accuracy on
the questions in each subgroup, may exhibit high variance for subgroups
(topics) with small sample sizes. Synthetic regression modeling, which
leverages the model's accuracy on questions about other topics, may yield
biased estimates that are too unreliable for large subgroups. We prescribe a
simple yet effective solution: an empirical Bayes (EB) estimator that balances
direct and regression estimates for each subgroup separately, improving the
precision of subgroup-level estimates of model performance. Our experiments on
multiple datasets show that this approach consistently provides more precise
estimates of the LLM performance compared to the direct and regression
approaches, achieving substantial reductions in the mean squared error.
Confidence intervals for EB estimates also have near-nominal coverage and are
narrower compared to those for the direct estimator. Additional experiments on
tabular and vision data validate the benefits of this EB approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Density estimation with LLMs: a geometric investigation of in-context
  learning trajectories <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toni J. B. Liu, Nicolas Boullé, Raphaël Sarfati, Christopher J. Earls
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate remarkable emergent abilities to
perform in-context learning across various tasks, including time series
forecasting. This work investigates LLMs' ability to estimate probability
density functions (PDFs) from data observed in-context; such density estimation
(DE) is a fundamental task underlying many probabilistic modeling problems. We
leverage the Intensive Principal Component Analysis (InPCA) to visualize and
analyze the in-context learning dynamics of LLaMA-2 models. Our main finding is
that these LLMs all follow similar learning trajectories in a low-dimensional
InPCA space, which are distinct from those of traditional density estimation
methods like histograms and Gaussian kernel density estimation (KDE). We
interpret the LLaMA in-context DE process as a KDE with an adaptive kernel
width and shape. This custom kernel model captures a significant portion of
LLaMA's behavior despite having only two parameters. We further speculate on
why LLaMA's kernel width and shape differs from classical algorithms, providing
insights into the mechanism of in-context probabilistic reasoning in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond FVD: Enhanced Evaluation Metrics for Video Generation Quality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Ya,  Luo, Gian Favero, Zhi Hao Luo, Alexia Jolicoeur-Martineau, Christopher Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Fr\'echet Video Distance (FVD) is a widely adopted metric for evaluating
video generation distribution quality. However, its effectiveness relies on
critical assumptions. Our analysis reveals three significant limitations: (1)
the non-Gaussianity of the Inflated 3D Convnet (I3D) feature space; (2) the
insensitivity of I3D features to temporal distortions; (3) the impractical
sample sizes required for reliable estimation. These findings undermine FVD's
reliability and show that FVD falls short as a standalone metric for video
generation evaluation. After extensive analysis of a wide range of metrics and
backbone architectures, we propose JEDi, the JEPA Embedding Distance, based on
features derived from a Joint Embedding Predictive Architecture, measured using
Maximum Mean Discrepancy with polynomial kernel. Our experiments on multiple
open-source datasets show clear evidence that it is a superior alternative to
the widely used FVD metric, requiring only 16% of the samples to reach its
steady value, while increasing alignment with human evaluation by 34%, on
average.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss
  Landscape Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyue Wen, Zhiyuan Li, Jason Wang, David Hall, Percy Liang, Tengyu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training language models currently requires pre-determining a fixed compute
budget because the typical cosine learning rate schedule depends on the total
number of steps. In contrast, the Warmup-Stable-Decay (WSD) schedule uses a
constant learning rate to produce a main branch of iterates that can in
principle continue indefinitely without a pre-specified compute budget. Then,
given any compute budget, one can branch out from the main branch at a proper
at any time with a rapidly decaying learning rate to produce a strong model.
Empirically, WSD generates a non-traditional loss curve: the loss remains
elevated during the stable phase but sharply declines during the decay phase.
Towards explaining this phenomenon, we conjecture that pretraining loss
exhibits a river valley landscape, which resembles a deep valley with a river
at its bottom. Under this assumption, we show that during the stable phase, the
iterate undergoes large oscillations due to the high learning rate, yet it
progresses swiftly along the river. During the decay phase, the rapidly
dropping learning rate minimizes the iterate's oscillations, moving it closer
to the river and revealing true optimization progress. Therefore, the sustained
high learning rate phase and fast decaying phase are responsible for progress
in the river and the mountain directions respectively, and are both critical.
Our analysis predicts phenomenons consistent with empirical observations and
shows that this landscape can emerge from pretraining on a simple bi-gram
dataset. Inspired by the theory, we introduce WSD-S, a variant of WSD that
reuses previous checkpoints' decay phases and keeps only one main branch, where
we resume from a decayed checkpoint. WSD-S empirically outperforms WSD and
Cyclic-Cosine in obtaining multiple language model checkpoints across various
compute budgets in a single run for parameters scaling from 0.1B to 1.2B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages,13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Matrix-weighted networks for modeling multidimensional dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Tian, Sadamori Kojaku, Hiroki Sayama, Renaud Lambiotte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Networks are powerful tools for modeling interactions in complex systems.
While traditional networks use scalar edge weights, many real-world systems
involve multidimensional interactions. For example, in social networks,
individuals often have multiple interconnected opinions that can affect
different opinions of other individuals, which can be better characterized by
matrices. We propose a novel, general framework for modeling such
multidimensional interacting dynamics: matrix-weighted networks (MWNs). We
present the mathematical foundations of MWNs and examine consensus dynamics and
random walks within this context. Our results reveal that the coherence of MWNs
gives rise to non-trivial steady states that generalize the notions of
communities and structural balance in traditional networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MARs: Multi-view Attention Regularizations for Patch-based Feature
  Recognition of Space Terrain <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothy Chase Jr, Karthik Dantu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The visual detection and tracking of surface terrain is required for
spacecraft to safely land on or navigate within close proximity to celestial
objects. Current approaches rely on template matching with pre-gathered
patch-based features, which are expensive to obtain and a limiting factor in
perceptual capability. While recent literature has focused on in-situ detection
methods to enhance navigation and operational autonomy, robust description is
still needed. In this work, we explore metric learning as the lightweight
feature description mechanism and find that current solutions fail to address
inter-class similarity and multi-view observational geometry. We attribute this
to the view-unaware attention mechanism and introduce Multi-view Attention
Regularizations (MARs) to constrain the channel and spatial attention across
multiple feature views, regularizing the what and where of attention focus. We
thoroughly analyze many modern metric learning losses with and without MARs and
demonstrate improved terrain-feature recognition performance by upwards of 85%.
We additionally introduce the Luna-1 dataset, consisting of Moon crater
landmarks and reference navigation frames from NASA mission data to support
future research in this difficult task. Luna-1 and source code are publicly
available at https://droneslab.github.io/mars/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024. Project page available at
  https://droneslab.github.io/mars/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are causal effect estimations enough for optimal recommendations under
  multitreatment scenarios? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sherly Alfonso-Sánchez, Kristina P. Sendova, Cristián Bravo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When making treatment selection decisions, it is essential to include a
causal effect estimation analysis to compare potential outcomes under different
treatments or controls, assisting in optimal selection. However, merely
estimating individual treatment effects may not suffice for truly optimal
decisions. Our study addressed this issue by incorporating additional criteria,
such as the estimations' uncertainty, measured by the conditional
value-at-risk, commonly used in portfolio and insurance management. For
continuous outcomes observable before and after treatment, we incorporated a
specific prediction condition. We prioritized treatments that could yield
optimal treatment effect results and lead to post-treatment outcomes more
desirable than pretreatment levels, with the latter condition being called the
prediction criterion. With these considerations, we propose a comprehensive
methodology for multitreatment selection. Our approach ensures satisfaction of
the overlap assumption, crucial for comparing outcomes for treated and control
groups, by training propensity score models as a preliminary step before
employing traditional causal models. To illustrate a practical application of
our methodology, we applied it to the credit card limit adjustment problem.
Analyzing a fintech company's historical data, we found that relying solely on
counterfactual predictions was inadequate for appropriate credit line
modifications. Incorporating our proposed additional criteria significantly
enhanced policy performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Presto! Distilling Steps and Layers for Accelerating Music Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary Novack, Ge Zhu, Jonah Casebeer, Julian McAuley, Taylor Berg-Kirkpatrick, Nicholas J. Bryan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advances in diffusion-based text-to-music (TTM) methods, efficient,
high-quality generation remains a challenge. We introduce Presto!, an approach
to inference acceleration for score-based diffusion transformers via reducing
both sampling steps and cost per step. To reduce steps, we develop a new
score-based distribution matching distillation (DMD) method for the EDM-family
of diffusion models, the first GAN-based distillation method for TTM. To reduce
the cost per step, we develop a simple, but powerful improvement to a recent
layer distillation method that improves learning via better preserving hidden
state variance. Finally, we combine our step and layer distillation methods
together for a dual-faceted approach. We evaluate our step and layer
distillation methods independently and show each yield best-in-class
performance. Our combined distillation method can generate high-quality outputs
with improved diversity, accelerating our base model by 10-18x (230/435ms
latency for 32 second mono/stereo 44.1kHz, 15x faster than comparable SOTA) --
the fastest high-quality TTM to our knowledge. Sound examples can be found at
https://presto-music.github.io/web/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simulation-Free Deep Learning Approach to Stochastic Optimal Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengjian Hua, Matthieu Laurière, Eric Vanden-Eijnden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a simulation-free algorithm for the solution of generic problems
in stochastic optimal control (SOC). Unlike existing methods, our approach does
not require the solution of an adjoint problem, but rather leverages Girsanov
theorem to directly calculate the gradient of the SOC objective on-policy. This
allows us to speed up the optimization of control policies parameterized by
neural networks since it completely avoids the expensive back-propagation step
through stochastic differential equations (SDEs) used in the Neural SDE
framework. In particular, it enables us to solve SOC problems in high dimension
and on long time horizons. We demonstrate the efficiency of our approach in
various domains of applications, including standard stochastic optimal control
problems, sampling from unnormalized distributions via construction of a
Schr\"odinger-F\"ollmer process, and fine-tuning of pre-trained diffusion
models. In all cases our method is shown to outperform the existing methods in
both the computing time and memory efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PAMLR: A Passive-Active Multi-Armed Bandit-Based Solution for LoRa
  Channel Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihoon Yun, Chengzhang Li, Anish Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving low duty cycle operation in low-power wireless networks in urban
environments is complicated by the complex and variable dynamics of external
interference and fading. We explore the use of reinforcement learning for
achieving low power consumption for the task of optimal selection of channels.
The learning relies on a hybrid of passive channel sampling for dealing with
external interference and active channel sampling for dealing with fading. Our
solution, Passive-Active Multi-armed bandit for LoRa (PAMLR, pronounced
"Pamela"), balances the two types of samples to achieve energy-efficient
channel selection: active channel measurements are tuned to an appropriately
low level to update noise thresholds, and to compensate passive channel
measurements are tuned to an appropriately high level for selecting the
top-most channels from channel exploration using the noise thresholds. The
rates of both types of samples are adapted in response to channel dynamics.
Based on extensive testing in multiple environments in different cities, we
validate that PAMLR can maintain excellent communication quality, as
demonstrated by a low SNR regret compared to the optimal channel allocation
policy, while substantially minimizing the energy cost associated with channel
measurements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tuning-Free Bilevel Optimization: New Algorithms and Convergence
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Yang, Hao Ban, Minhui Huang, Shiqian Ma, Kaiyi Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilevel optimization has recently attracted considerable attention due to its
abundant applications in machine learning problems. However, existing methods
rely on prior knowledge of problem parameters to determine stepsizes, resulting
in significant effort in tuning stepsizes when these parameters are unknown. In
this paper, we propose two novel tuning-free algorithms, D-TFBO and S-TFBO.
D-TFBO employs a double-loop structure with stepsizes adaptively adjusted by
the "inverse of cumulative gradient norms" strategy. S-TFBO features a simpler
fully single-loop structure that updates three variables simultaneously with a
theory-motivated joint design of adaptive stepsizes for all variables. We
provide a comprehensive convergence analysis for both algorithms and show that
D-TFBO and S-TFBO respectively require $O(\frac{1}{\epsilon})$ and
$O(\frac{1}{\epsilon}\log^4(\frac{1}{\epsilon}))$ iterations to find an
$\epsilon$-accurate stationary point, (nearly) matching their well-tuned
counterparts using the information of problem parameters. Experiments on
various problems show that our methods achieve performance comparable to
existing well-tuned approaches, while being more robust to the selection of
initial stepsizes. To the best of our knowledge, our methods are the first to
completely eliminate the need for stepsize tuning, while achieving theoretical
guarantees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LOTOS: Layer-wise Orthogonalization for Training Robust Ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Ebrahimpour-Boroojeny, Hari Sundaram, Varun Chandrasekaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transferability of adversarial examples is a well-known property that
endangers all classification models, even those that are only accessible
through black-box queries. Prior work has shown that an ensemble of models is
more resilient to transferability: the probability that an adversarial example
is effective against most models of the ensemble is low. Thus, most ongoing
research focuses on improving ensemble diversity. Another line of prior work
has shown that Lipschitz continuity of the models can make models more robust
since it limits how a model's output changes with small input perturbations. In
this paper, we study the effect of Lipschitz continuity on transferability
rates. We show that although a lower Lipschitz constant increases the
robustness of a single model, it is not as beneficial in training robust
ensembles as it increases the transferability rate of adversarial examples
across models in the ensemble. Therefore, we introduce LOTOS, a new training
paradigm for ensembles, which counteracts this adverse effect. It does so by
promoting orthogonality among the top-$k$ sub-spaces of the transformations of
the corresponding affine layers of any pair of models in the ensemble. We
theoretically show that $k$ does not need to be large for convolutional layers,
which makes the computational overhead negligible. Through various experiments,
we show LOTOS increases the robust accuracy of ensembles of ResNet-18 models by
$6$ percentage points (p.p) against black-box attacks on CIFAR-10. It is also
capable of combining with the robustness of prior state-of-the-art methods for
training robust ensembles to enhance their robust accuracy by $10.7$ p.p.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Digital Twin Framework for Liquid-cooled Supercomputers as
  Demonstrated at Exascale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wesley Brewer, Matthias Maiterth, Vineet Kumar, Rafal Wojda, Sedrick Bouknight, Jesse Hines, Woong Shin, Scott Greenwood, David Grant, Wesley Williams, Feiyi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ExaDigiT, an open-source framework for developing comprehensive
digital twins of liquid-cooled supercomputers. It integrates three main
modules: (1) a resource allocator and power simulator, (2) a transient
thermo-fluidic cooling model, and (3) an augmented reality model of the
supercomputer and central energy plant. The framework enables the study of
"what-if" scenarios, system optimizations, and virtual prototyping of future
systems. Using Frontier as a case study, we demonstrate the framework's
capabilities by replaying six months of system telemetry for systematic
verification and validation. Such a comprehensive analysis of a liquid-cooled
exascale supercomputer is the first of its kind. ExaDigiT elucidates complex
transient cooling system dynamics, runs synthetic or real workloads, and
predicts energy losses due to rectification and voltage conversion. Throughout
our paper, we present lessons learned to benefit HPC practitioners developing
similar digital twins. We envision the digital twin will be a key enabler for
sustainable, energy-efficient supercomputing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures, To be published in the Proceedings of the
  International Conference for High Performance Computing, Networking, Storage
  and Analysis. 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agnostic Smoothed Online Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moïse Blanchard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical results in statistical learning typically consider two extreme
data-generating models: i.i.d. instances from an unknown distribution, or fully
adversarial instances, often much more challenging statistically. To bridge the
gap between these models, recent work introduced the smoothed framework, in
which at each iteration an adversary generates instances from a distribution
constrained to have density bounded by $\sigma^{-1}$ compared to some fixed
base measure $\mu$. This framework interpolates between the i.i.d. and
adversarial cases, depending on the value of $\sigma$. For the classical online
prediction problem, most prior results in smoothed online learning rely on the
arguably strong assumption that the base measure $\mu$ is known to the learner,
contrasting with standard settings in the PAC learning or consistency
literature. We consider the general agnostic problem in which the base measure
is unknown and values are arbitrary. Along this direction, Block et al. showed
that empirical risk minimization has sublinear regret under the well-specified
assumption. We propose an algorithm R-Cover based on recursive coverings which
is the first to guarantee sublinear regret for agnostic smoothed online
learning without prior knowledge of $\mu$. For classification, we prove that
R-Cover has adaptive regret $\tilde O(\sqrt{dT/\sigma})$ for function classes
with VC dimension $d$, which is optimal up to logarithmic factors. For
regression, we establish that R-Cover has sublinear oblivious regret for
function classes with polynomial fat-shattering dimension growth.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assouad, Fano, and Le Cam with Interaction: A Unifying Lower Bound
  Framework and Characterization for Bandit Learnability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Chen, Dylan J. Foster, Yanjun Han, Jian Qian, Alexander Rakhlin, Yunbei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we develop a unified framework for lower bound methods in
statistical estimation and interactive decision making. Classical lower bound
techniques -- such as Fano's inequality, Le Cam's method, and Assouad's lemma
-- have been central to the study of minimax risk in statistical estimation,
yet they are insufficient for the analysis of methods that collect data in an
interactive manner. The recent minimax lower bounds for interactive decision
making via the Decision-Estimation Coefficient (DEC) appear to be genuinely
different from the classical methods. We propose a unified view of these
distinct methodologies through a general algorithmic lower bound method. We
further introduce a novel complexity measure, decision dimension, which
facilitates the derivation of new lower bounds for interactive decision making.
In particular, decision dimension provides a characterization of bandit
learnability for any structured bandit model class. Further, we characterize
the sample complexity of learning convex model class up to a polynomial gap
with the decision dimension, addressing the remaining gap between upper and
lower bounds in Foster et al. (2021, 2023).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-Feedback Efficient Reinforcement Learning for Online Diffusion
  Model Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayano Hiranaka, Shang-Fu Chen, Chieh-Hsin Lai, Dongjun Kim, Naoki Murata, Takashi Shibuya, Wei-Hsiang Liao, Shao-Hua Sun, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controllable generation through Stable Diffusion (SD) fine-tuning aims to
improve fidelity, safety, and alignment with human guidance. Existing
reinforcement learning from human feedback methods usually rely on predefined
heuristic reward functions or pretrained reward models built on large-scale
datasets, limiting their applicability to scenarios where collecting such data
is costly or difficult. To effectively and efficiently utilize human feedback,
we develop a framework, HERO, which leverages online human feedback collected
on the fly during model learning. Specifically, HERO features two key
mechanisms: (1) Feedback-Aligned Representation Learning, an online training
method that captures human feedback and provides informative learning signals
for fine-tuning, and (2) Feedback-Guided Image Generation, which involves
generating images from SD's refined initialization samples, enabling faster
convergence towards the evaluator's intent. We demonstrate that HERO is 4x more
efficient in online feedback for body part anomaly correction compared to the
best existing method. Additionally, experiments show that HERO can effectively
handle tasks like reasoning, counting, personalization, and reducing NSFW
content with only 0.5K online feedback.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyper-Representations: Learning from Populations of Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantin Schürholt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This thesis addresses the challenge of understanding Neural Networks through
the lens of their most fundamental component: the weights, which encapsulate
the learned information and determine the model behavior. At the core of this
thesis is a fundamental question: Can we learn general, task-agnostic
representations from populations of Neural Network models? The key contribution
of this thesis to answer that question are hyper-representations, a
self-supervised method to learn representations of NN weights. Work in this
thesis finds that trained NN models indeed occupy meaningful structures in the
weight space, that can be learned and used. Through extensive experiments, this
thesis demonstrates that hyper-representations uncover model properties, such
as their performance, state of training, or hyperparameters. Moreover, the
identification of regions with specific properties in hyper-representation
space allows to sample and generate model weights with targeted properties.
This thesis demonstrates applications for fine-tuning, and transfer learning to
great success. Lastly, it presents methods that allow hyper-representations to
generalize beyond model sizes, architectures, and tasks. The practical
implications of that are profound, as it opens the door to foundation models of
Neural Networks, which aggregate and instantiate their knowledge across models
and architectures. Ultimately, this thesis contributes to the deeper
understanding of Neural Networks by investigating structures in their weights
which leads to more interpretable, efficient, and adaptable models. By laying
the groundwork for representation learning of NN weights, this research
demonstrates the potential to change the way Neural Networks are developed,
analyzed, and used.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD Dissertation accepted at University of St. Gallen</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nonasymptotic Analysis of Stochastic Gradient Descent with the
  Richardson-Romberg Extrapolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marina Sheshukova, Denis Belomestny, Alain Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of solving strongly convex and smooth minimization
problems using stochastic gradient descent (SGD) algorithm with a constant step
size. Previous works suggested to combine the Polyak-Ruppert averaging
procedure with the Richardson-Romberg extrapolation technique to reduce the
asymptotic bias of SGD at the expense of a mild increase of the variance. We
significantly extend previous results by providing an expansion of the
mean-squared error of the resulting estimator with respect to the number of
iterations $n$. More precisely, we show that the mean-squared error can be
decomposed into the sum of two terms: a leading one of order
$\mathcal{O}(n^{-1/2})$ with explicit dependence on a minimax-optimal
asymptotic covariance matrix, and a second-order term of order
$\mathcal{O}(n^{-3/4})$ where the power $3/4$ can not be improved in general.
We also extend this result to the $p$-th moment bound keeping optimal scaling
of the remainders with respect to $n$. Our analysis relies on the properties of
the SGD iterates viewed as a time-homogeneous Markov chain. In particular, we
establish that this chain is geometrically ergodic with respect to a suitably
defined weighted Wasserstein semimetric.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SparsePO: Controlling Preference Alignment of LLMs via Sparse Token
  Masks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fenia Christopoulou, Ronald Cardenas, Gerasimos Lampouras, Haitham Bou-Ammar, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference Optimization (PO) has proven an effective step for aligning
language models to human-desired behaviors. Current variants, following the
offline Direct Preference Optimization objective, have focused on a strict
setting where all tokens are contributing signals of KL divergence and rewards
to the loss function. However, human preference is not affected by each word in
a sequence equally but is often dependent on specific words or phrases, e.g.
existence of toxic terms leads to non-preferred responses. Based on this
observation, we argue that not all tokens should be weighted equally during PO
and propose a flexible objective termed SparsePO, that aims to automatically
learn to weight the KL divergence and reward corresponding to each token during
PO training. We propose two different variants of weight-masks that can either
be derived from the reference model itself or learned on the fly. Notably, our
method induces sparsity in the learned masks, allowing the model to learn how
to best weight reward and KL divergence contributions at the token level,
learning an optimal level of mask sparsity. Extensive experiments on multiple
domains, including sentiment control, dialogue, text summarization and
text-to-code generation, illustrate that our approach assigns meaningful
weights to tokens according to the target task, generates more responses with
the desired preference and improves reasoning tasks by up to 2 percentage
points compared to other token- and response-level PO methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 papges, 9 figures, 5 tables. Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CR-CTC: Consistency regularization on CTC for improved speech
  recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zengwei Yao, Wei Kang, Xiaoyu Yang, Fangjun Kuang, Liyong Guo, Han Zhu, Zengrui Jin, Zhaoqing Li, Long Lin, Daniel Povey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connectionist Temporal Classification (CTC) is a widely used method for
automatic speech recognition (ASR), renowned for its simplicity and
computational efficiency. However, it often falls short in recognition
performance compared to transducer or systems combining CTC and attention-based
encoder-decoder (CTC/AED). In this work, we propose the Consistency-Regularized
CTC (CR-CTC), which enforces consistency between two CTC distributions obtained
from different augmented views of the input speech mel-spectrogram. We provide
in-depth insights into its essential behaviors from three perspectives: 1) it
conducts self-distillation between random pairs of sub-models that process
different augmented views; 2) it learns contextual representation through
masked prediction for positions within time-masked regions, especially when we
increase the amount of time masking; 3) it suppresses the extremely peaky CTC
distributions, thereby reducing overfitting and improving the generalization
ability. Extensive experiments on LibriSpeech, Aishell-1, and GigaSpeech
datasets demonstrate the effectiveness of our CR-CTC, which achieves
performance comparable to, or even slightly better than, that of transducer and
CTC/AED.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DreamSat: Towards a General 3D Model for Novel View Synthesis of Space
  Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nidhi Mathihalli, Audrey Wei, Giovanni Lavezzi, Peng Mun Siew, Victor Rodriguez-Fernandez, Hodei Urrutxua, Richard Linares
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novel view synthesis (NVS) enables to generate new images of a scene or
convert a set of 2D images into a comprehensive 3D model. In the context of
Space Domain Awareness, since space is becoming increasingly congested, NVS can
accurately map space objects and debris, improving the safety and efficiency of
space operations. Similarly, in Rendezvous and Proximity Operations missions,
3D models can provide details about a target object's shape, size, and
orientation, allowing for better planning and prediction of the target's
behavior. In this work, we explore the generalization abilities of these
reconstruction techniques, aiming to avoid the necessity of retraining for each
new scene, by presenting a novel approach to 3D spacecraft reconstruction from
single-view images, DreamSat, by fine-tuning the Zero123 XL, a state-of-the-art
single-view reconstruction model, on a high-quality dataset of 190 high-quality
spacecraft models and integrating it into the DreamGaussian framework. We
demonstrate consistent improvements in reconstruction quality across multiple
metrics, including Contrastive Language-Image Pretraining (CLIP) score
(+0.33%), Peak Signal-to-Noise Ratio (PSNR) (+2.53%), Structural Similarity
Index (SSIM) (+2.38%), and Learned Perceptual Image Patch Similarity (LPIPS)
(+0.16%) on a test set of 30 previously unseen spacecraft images. Our method
addresses the lack of domain-specific 3D reconstruction tools in the space
industry by leveraging state-of-the-art diffusion models and 3D Gaussian
splatting techniques. This approach maintains the efficiency of the
DreamGaussian framework while enhancing the accuracy and detail of spacecraft
reconstructions. The code for this work can be accessed on GitHub
(https://github.com/ARCLab-MIT/space-nvs).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the 75th International Astronautical Congress, October
  2024, Milan, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyperINF: Unleashing the HyperPower of the Schulz's Method for Data
  Influence Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Zhou, Simin Fan, Martin Jaggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Influence functions provide a principled method to assess the contribution of
individual training samples to a specific target. Yet, their high computational
costs limit their applications on large-scale models and datasets. Existing
methods proposed for influence function approximation have significantly
reduced the computational overheads. However, they mostly suffer from
inaccurate estimation due to the lack of strong convergence guarantees from the
algorithm. The family of hyperpower methods are well-known for their rigorous
convergence guarantees on matrix inverse approximation, while the matrix
multiplication operation can involve intractable memory and computation costs
on large-scale models. We propose HyperINF, an efficient and accurate influence
function approximation method which leverages the hyperpower method,
specifically Schulz's iterative algorithm.
  To deal with the computation-intensive matrix multiplication, we incorporate
the generalized fisher information (GFIM) as a low-rank approximation of the
Hessian matrix, which reduces the memory and computation overheads to constant
costs independent of ranks on LoRA-tuned models.
  We first demonstrate the superior accuracy and stability of \method compared
to other baselines through a synthetic convergence simulation for matrix
inversion. We further validate the efficacy of \method through extensive
real-world data attribution tasks, including mislabeled data detection and data
selection for LLM and VLM fine-tuning.
  On LoRA-tuned models, HyperINF achieves superior downstream performance with
minimal memory and computational overhead, while other baselines suffer from
significant degradation. Our codebase is available at
https://github.com/Blackzxy/HyperINF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ScienceAgentBench: Toward Rigorous Assessment of Language Agents for
  Data-Driven Scientific Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N. Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancements of language language models (LLMs) have piqued growing
interest in developing LLM-based language agents to automate scientific
discovery end-to-end, which has sparked both excitement and skepticism about
the true capabilities of such agents. In this work, we argue that for an agent
to fully automate scientific discovery, it must be able to complete all
essential tasks in the workflow. Thus, we call for rigorous assessment of
agents on individual tasks in a scientific workflow before making bold claims
on end-to-end automation. To this end, we present ScienceAgentBench, a new
benchmark for evaluating language agents for data-driven scientific discovery.
To ensure the scientific authenticity and real-world relevance of our
benchmark, we extract 102 tasks from 44 peer-reviewed publications in four
disciplines and engage nine subject matter experts to validate them. We unify
the target output for every task to a self-contained Python program file and
employ an array of evaluation metrics to examine the generated programs,
execution results, and costs. Each task goes through multiple rounds of manual
validation by annotators and subject matter experts to ensure its annotation
quality and scientific plausibility. We also propose two effective strategies
to mitigate data contamination concerns. Using our benchmark, we evaluate five
open-weight and proprietary LLMs, each with three frameworks: direct prompting,
OpenHands, and self-debug. Given three attempts for each task, the
best-performing agent can only solve 32.4% of the tasks independently and 34.3%
with expert-provided knowledge. These results underscore the limited capacities
of current language agents in generating code for data-driven discovery, let
alone end-to-end automation for scientific research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compression via <span class="highlight-title">Pre-train</span>ed <span class="highlight-title">Transformer</span>s: A Study on Byte-Level
  Multimodal Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Heurtel-Depeiges, Anian Ruoss, Joel Veness, Tim Genewein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models have recently been shown to be strong data compressors.
However, when accounting for their excessive parameter count, their compression
ratios are actually inferior to standard compression algorithms. Moreover,
naively reducing the number of parameters may not necessarily help as it leads
to worse predictions and thus weaker compression. In this paper, we conduct a
large-scale empirical study to investigate whether there is a sweet spot where
competitive compression ratios with pre-trained vanilla transformers are
possible. To this end, we train families of models on 165GB of raw byte
sequences of either text, image, or audio data (and all possible combinations
of the three) and then compress 1GB of out-of-distribution (OOD) data from each
modality. We find that relatively small models (i.e., millions of parameters)
can outperform standard general-purpose compression algorithms (gzip, LZMA2)
and even domain-specific compressors (PNG, JPEG 2000, FLAC) - even when
factoring in parameter count. We achieve, e.g., the lowest compression ratio of
0.49 on OOD audio data (vs. 0.54 for FLAC). To study the impact of model- and
dataset scale, we conduct extensive ablations and hyperparameter sweeps, and we
investigate the effect of unimodal versus multimodal training. We find that
even small models can be trained to perform well on multiple modalities, but,
in contrast to previously reported results with large-scale foundation models,
transfer to unseen modalities is generally weak.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent
  Sparse Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijie Yang, Zhihao Zhang, Zhuofu Chen, Zikun Li, Zhihao Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have driven significant advancements across
diverse NLP tasks, with long-context models gaining prominence for handling
extended inputs. However, the expanding key-value (KV) cache size required by
Transformer architectures intensifies the memory constraints, particularly
during the decoding phase, creating a significant bottleneck. Existing sparse
attention mechanisms designed to address this bottleneck have two limitations:
(1) they often fail to reliably identify the most relevant tokens for
attention, and (2) they overlook the spatial coherence of token selection
across consecutive Transformer layers, which can lead to performance
degradation and substantial overhead in token selection. This paper introduces
TidalDecode, a simple yet effective algorithm and system for fast and accurate
LLM decoding through position persistent sparse attention. TidalDecode
leverages the spatial coherence of tokens selected by existing sparse attention
methods and introduces a few token selection layers that perform full attention
to identify the tokens with the highest attention scores, while all other
layers perform sparse attention with the pre-selected tokens. This design
enables TidalDecode to substantially reduce the overhead of token selection for
sparse attention without sacrificing the quality of the generated results.
Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely
matches the generative performance of full attention methods while reducing the
LLM decoding latency by up to 2.1x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Function Gradient Approximation with Random Shallow ReLU Networks with
  Control Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Lamperski, Siddharth Salapaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks are widely used to approximate unknown functions in control.
A common neural network architecture uses a single hidden layer (i.e. a shallow
network), in which the input parameters are fixed in advance and only the
output parameters are trained. The typical formal analysis asserts that if
output parameters exist to approximate the unknown function with sufficient
accuracy, then desired control performance can be achieved. A long-standing
theoretical gap was that no conditions existed to guarantee that, for the fixed
input parameters, required accuracy could be obtained by training the output
parameters. Our recent work has partially closed this gap by demonstrating that
if input parameters are chosen randomly, then for any sufficiently smooth
function, with high-probability there are output parameters resulting in
$O((1/m)^{1/2})$ approximation errors, where $m$ is the number of neurons.
However, some applications, notably continuous-time value function
approximation, require that the network approximates the both the unknown
function and its gradient with sufficient accuracy. In this paper, we show that
randomly generated input parameters and trained output parameters result in
gradient errors of $O((\log(m)/m)^{1/2})$, and additionally, improve the
constants from our prior work. We show how to apply the result to policy
evaluation problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review for American Control Conference, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Control-oriented Clustering of Visual Latent Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Qi, Haocheng Yin, Heng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We initiate a study of the geometry of the visual representation space -- the
information channel from the vision encoder to the action decoder -- in an
image-based control pipeline learned from behavior cloning. Inspired by the
phenomenon of neural collapse (NC) in image classification, we investigate
whether a similar law of clustering emerges in the visual representation space.
Since image-based control is a regression task without explicitly defined
classes, the central piece of the puzzle lies in determining according to what
implicit classes the visual features cluster, if such a law exists. Focusing on
image-based planar pushing, we posit the most important role of the visual
representation in a control task is to convey a goal to the action decoder. We
then classify training samples of expert demonstrations into eight
"control-oriented" classes based on (a) the relative pose between the object
and the target in the input or (b) the relative pose of the object induced by
expert actions in the output, where one class corresponds to one relative pose
orthant (REPO). Across four different instantiations of architecture, we report
the prevalent emergence of control-oriented clustering in the visual
representation space according to the eight REPOs. Beyond empirical
observation, we show such a law of clustering can be leveraged as an
algorithmic tool to improve test-time performance when training a policy with
limited expert demonstrations. Particularly, we pretrain the vision encoder
using NC as a regularization to encourage control-oriented clustering of the
visual features. Surprisingly, such an NC-pretrained vision encoder, when
finetuned end-to-end with the action decoder, boosts the test-time performance
by 10% to 35% in the low-data regime. Real-world vision-based planar pushing
experiments confirmed the surprising advantage of control-oriented visual
representation pretraining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SELECT: A Large-Scale Benchmark of Data Curation Strategies for Image
  Classification <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Feuer, Jiawei Xu, Niv Cohen, Patrick Yubeaton, Govind Mittal, Chinmay Hegde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data curation is the problem of how to collect and organize samples into a
dataset that supports efficient learning. Despite the centrality of the task,
little work has been devoted towards a large-scale, systematic comparison of
various curation methods. In this work, we take steps towards a formal
evaluation of data curation strategies and introduce SELECT, the first
large-scale benchmark of curation strategies for image classification.
  In order to generate baseline methods for the SELECT benchmark, we create a
new dataset, ImageNet++, which constitutes the largest superset of ImageNet-1K
to date. Our dataset extends ImageNet with 5 new training-data shifts, each
approximately the size of ImageNet-1K itself, and each assembled using a
distinct curation strategy. We evaluate our data curation baselines in two
ways: (i) using each training-data shift to train identical image
classification models from scratch (ii) using the data itself to fit a
pretrained self-supervised representation.
  Our findings show interesting trends, particularly pertaining to recent
methods for data curation such as synthetic data generation and lookup based on
CLIP embeddings. We show that although these strategies are highly competitive
for certain tasks, the curation strategy used to assemble the original
ImageNet-1K dataset remains the gold standard. We anticipate that our benchmark
can illuminate the path for new methods to further reduce the gap. We release
our checkpoints, code, documentation, and a link to our dataset at
https://github.com/jimmyxu123/SELECT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024, Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FreSh: Frequency Shifting for Accelerated Neural Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Kania, Marko Mihajlovic, Sergey Prokudin, Jacek Tabor, Przemysław Spurek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit Neural Representations (INRs) have recently gained attention as a
powerful approach for continuously representing signals such as images, videos,
and 3D shapes using multilayer perceptrons (MLPs). However, MLPs are known to
exhibit a low-frequency bias, limiting their ability to capture high-frequency
details accurately. This limitation is typically addressed by incorporating
high-frequency input embeddings or specialized activation layers. In this work,
we demonstrate that these embeddings and activations are often configured with
hyperparameters that perform well on average but are suboptimal for specific
input signals under consideration, necessitating a costly grid search to
identify optimal settings. Our key observation is that the initial frequency
spectrum of an untrained model's output correlates strongly with the model's
eventual performance on a given target signal. Leveraging this insight, we
propose frequency shifting (or FreSh), a method that selects embedding
hyperparameters to align the frequency spectrum of the model's initial output
with that of the target signal. We show that this simple initialization
technique improves performance across various neural representation methods and
tasks, achieving results comparable to extensive hyperparameter sweeps but with
only marginal computational overhead compared to training a single model with
default hyperparameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PhotoReg: Photometrically Registering 3D Gaussian Splatting Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwen Yuan, Tianyi Zhang, Matthew Johnson-Roberson, Weiming Zhi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building accurate representations of the environment is critical for
intelligent robots to make decisions during deployment. Advances in
photorealistic environment models have enabled robots to develop
hyper-realistic reconstructions, which can be used to generate images that are
intuitive for human inspection. In particular, the recently introduced
\ac{3DGS}, which describes the scene with up to millions of primitive
ellipsoids, can be rendered in real time. \ac{3DGS} has rapidly gained
prominence. However, a critical unsolved problem persists: how can we fuse
multiple \ac{3DGS} into a single coherent model? Solving this problem will
enable robot teams to jointly build \ac{3DGS} models of their surroundings. A
key insight of this work is to leverage the {duality} between photorealistic
reconstructions, which render realistic 2D images from 3D structure, and
\emph{3D foundation models}, which predict 3D structure from image pairs. To
this end, we develop PhotoReg, a framework to register multiple photorealistic
\ac{3DGS} models with 3D foundation models. As \ac{3DGS} models are generally
built from monocular camera images, they have \emph{arbitrary scale}. To
resolve this, PhotoReg actively enforces scale consistency among the different
\ac{3DGS} models by considering depth estimates within these models. Then, the
alignment is iteratively refined with fine-grained photometric losses to
produce high-quality fused \ac{3DGS} models. We rigorously evaluate PhotoReg on
both standard benchmark datasets and our custom-collected datasets, including
with two quadruped robots. The code is released at
\url{ziweny11.github.io/photoreg}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Systematic Literature <span class="highlight-title">Review</span> of Vision-Based Approaches to Outdoor
  Livestock Monitoring with Lessons from Wildlife Studies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stacey D. Scott, Zayn J. Abbas, Feerass Ellid, Eli-Henry Dykhne, Muhammad Muhaiminul Islam, Weam Ayad, Kristina Kacmorova, Dan Tulpan, Minglun Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precision livestock farming (PLF) aims to improve the health and welfare of
livestock animals and farming outcomes through the use of advanced
technologies. Computer vision, combined with recent advances in machine
learning and deep learning artificial intelligence approaches, offers a
possible solution to the PLF ideal of 24/7 livestock monitoring that helps
facilitate early detection of animal health and welfare issues. However, a
significant number of livestock species are raised in large outdoor habitats
that pose technological challenges for computer vision approaches. This review
provides a comprehensive overview of computer vision methods and open
challenges in outdoor animal monitoring. We include research from both the
livestock and wildlife fields in the review because of the similarities in
appearance, behaviour, and habitat for many livestock and wildlife. We focus on
large terrestrial mammals, such as cattle, horses, deer, goats, sheep, koalas,
giraffes, and elephants. We use an image processing pipeline to frame our
discussion and highlight the current capabilities and open technical challenges
at each stage of the pipeline. The review found a clear trend towards the use
of deep learning approaches for animal detection, counting, and multi-species
classification. We discuss in detail the applicability of current vision-based
methods to PLF contexts and promising directions for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Fine-Tuning of Generalist Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Bagatella, Jonas Hübotter, Georg Martius, Andreas Krause
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained generalist policies are rapidly gaining relevance in robot
learning due to their promise of fast adaptation to novel, in-domain tasks.
This adaptation often relies on collecting new demonstrations for a specific
task of interest and applying imitation learning algorithms, such as behavioral
cloning. However, as soon as several tasks need to be learned, we must decide
which tasks should be demonstrated and how often? We study this multi-task
problem and explore an interactive framework in which the agent adaptively
selects the tasks to be demonstrated. We propose AMF (Active Multi-task
Fine-tuning), an algorithm to maximize multi-task policy performance under a
limited demonstration budget by collecting demonstrations yielding the largest
information gain on the expert policy. We derive performance guarantees for AMF
under regularity assumptions and demonstrate its empirical effectiveness to
efficiently fine-tune neural policies in complex and high-dimensional
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DEPT: Decoupled Embeddings for <span class="highlight-title">Pre-train</span>ing Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Iacob, Lorenzo Sani, Meghdad Kurmanji, William F. Shen, Xinchi Qiu, Dongqi Cai, Yan Gao, Nicholas D. Lane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Model pre-training benefits from a broader data mixture to enhance
performance across domains and languages. However, training on such
heterogeneous text corpora is complex, requiring extensive and cost-intensive
efforts. Since these data sources vary in lexical, syntactic, and semantic
aspects, they cause negative interference or the "curse of multilinguality". We
propose a novel pre-training framework to alleviate this curse. Our method,
DEPT, decouples the embedding layers from the transformer body while
simultaneously training the latter in multiple contexts. DEPT enables the model
to train without being bound to a shared global vocabulary. DEPT: (1) can train
robustly and effectively under significant data heterogeneity, (2) reduces the
parameter count of the token embeddings by up to 80% and the communication
costs by 675x for billion-scale models (3) enhances model generalization and
plasticity in adapting to new languages and domains, and (4) allows training
with custom optimized vocabulary per data source. We prove DEPT's potential by
performing the first vocabulary-agnostic federated multilingual pre-training of
a 1.3 billion-parameter model across high and low-resource languages, reducing
its parameter count by 409 million.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FRIDA: Free-Rider Detection using Privacy Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pol G. Recasens, Ádám Horváth, Alberto Gutierrez-Torre, Jordi Torres, Josep Ll. Berral, Balázs Pejó
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is increasingly popular as it enables multiple parties
with limited datasets and resources to train a high-performing machine learning
model collaboratively. However, similarly to other collaborative systems,
federated learning is vulnerable to free-riders -- participants who do not
contribute to the training but still benefit from the shared model. Free-riders
not only compromise the integrity of the learning process but also slow down
the convergence of the global model, resulting in increased costs for the
honest participants.
  To address this challenge, we propose FRIDA: free-rider detection using
privacy attacks, a framework that leverages inference attacks to detect
free-riders. Unlike traditional methods that only capture the implicit effects
of free-riding, FRIDA directly infers details of the underlying training
datasets, revealing characteristics that indicate free-rider behaviour. Through
extensive experiments, we demonstrate that membership and property inference
attacks are effective for this purpose. Our evaluation shows that FRIDA
outperforms state-of-the-art methods, especially in non-IID settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RelUNet: Relative Channel Fusion U-Net for Multichannel Speech
  Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ibrahim Aldarmaki, Thamar Solorio, Bhiksha Raj, Hanan Aldarmaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural multi-channel speech enhancement models, in particular those based on
the U-Net architecture, demonstrate promising performance and generalization
potential. These models typically encode input channels independently, and
integrate the channels during later stages of the network. In this paper, we
propose a novel modification of these models by incorporating relative
information from the outset, where each channel is processed in conjunction
with a reference channel through stacking. This input strategy exploits
comparative differences to adaptively fuse information between channels,
thereby capturing crucial spatial information and enhancing the overall
performance. The experiments conducted on the CHiME-3 dataset demonstrate
improvements in speech enhancement metrics across various architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ T-JEPA: Augmentation-Free <span class="highlight-title">Self-Supervised</span> Learning for Tabular Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Thimonier, José Lucas De Melo Costa, Fabrice Popineau, Arpad Rimmel, Bich-Liên Doan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervision is often used for pre-training to foster performance on a
downstream task by constructing meaningful representations of samples.
Self-supervised learning (SSL) generally involves generating different views of
the same sample and thus requires data augmentations that are challenging to
construct for tabular data. This constitutes one of the main challenges of
self-supervision for structured data. In the present work, we propose a novel
augmentation-free SSL method for tabular data. Our approach, T-JEPA, relies on
a Joint Embedding Predictive Architecture (JEPA) and is akin to mask
reconstruction in the latent space. It involves predicting the latent
representation of one subset of features from the latent representation of a
different subset within the same sample, thereby learning rich representations
without augmentations. We use our method as a pre-training technique and train
several deep classifiers on the obtained representation. Our experimental
results demonstrate a substantial improvement in both classification and
regression tasks, outperforming models trained directly on samples in their
original data space. Moreover, T-JEPA enables some methods to consistently
outperform or match the performance of traditional methods likes Gradient
Boosted Decision Trees. To understand why, we extensively characterize the
obtained representations and show that T-JEPA effectively identifies relevant
features for downstream tasks without access to the labels. Additionally, we
introduce regularization tokens, a novel regularization method critical for
training of JEPA-based models on structured data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assumption-Lean Post-Integrated Inference with Negative Control Outcomes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin-Hong Du, Kathryn Roeder, Larry Wasserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data integration has become increasingly common in aligning multiple
heterogeneous datasets. With high-dimensional outcomes, data integration
methods aim to extract low-dimensional embeddings of observations to remove
unwanted variations, such as batch effects and unmeasured covariates, inherent
in data collected from different sources. However, multiple hypothesis testing
after data integration can be substantially biased due to the data-dependent
integration processes. To address this challenge, we introduce a robust
post-integrated inference (PII) method that adjusts for latent heterogeneity
using negative control outcomes. By leveraging causal interpretations, we
derive nonparametric identification conditions that form the basis of our PII
approach.
  Our assumption-lean semiparametric inference method extends robustness and
generality to projected direct effect estimands that account for mediators,
confounders, and moderators. These estimands remain statistically meaningful
under model misspecifications and with error-prone embeddings. We provide
deterministic quantifications of the bias of target estimands induced by
estimated embeddings and finite-sample linear expansions of the estimators with
uniform concentration bounds on the residuals for all outcomes.
  The proposed doubly robust estimators are consistent and efficient under
minimal assumptions, facilitating data-adaptive estimation with machine
learning algorithms. Using random forests, we evaluate empirical statistical
errors in simulations and analyze single-cell CRISPR perturbed datasets with
potential unmeasured confounders.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages for main text, and 18 pages for appendix, 9 figures for main
  text, 4 figures for appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MC-QDSNN: Quantized Deep evolutionary SNN with Multi-Dendritic
  Compartment Neurons for Stress Detection using Physiological Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajay B. S., Phani Pavan K, Madhav Rao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long short-term memory (LSTM) has emerged as a definitive network for
analyzing and inferring time series data. LSTM has the capability to extract
spectral features and a mixture of temporal features. Due to this benefit, a
similar feature extraction method is explored for the spiking counterparts
targeting time-series data. Though LSTMs perform well in their spiking form,
they tend to be compute and power intensive. Addressing this issue, this work
proposes Multi-Compartment Leaky (MCLeaky) neuron as a viable alternative for
efficient processing of time series data. The MCLeaky neuron, derived from the
Leaky Integrate and Fire (LIF) neuron model, contains multiple memristive
synapses interlinked to form a memory component, which emulates the human
brain's Hippocampus region. The proposed MCLeaky neuron based Spiking Neural
Network model and its quantized variant were benchmarked against
state-of-the-art (SOTA) Spiking LSTMs to perform human stress detection, by
comparing compute requirements, latency and real-world performances on unseen
data with models derived through Neural Architecture Search (NAS). Results show
that networks with MCLeaky activation neuron managed a superior accuracy of
98.8% to detect stress based on Electrodermal Activity (EDA) signals, better
than any other investigated models, while using 20% less parameters on average.
MCLeaky neuron was also tested for various signals including EDA Wrist and
Chest, Temperature, ECG, and combinations of them. Quantized MCLeaky model was
also derived and validated to forecast their performance on hardware
architectures, which resulted in 91.84% accuracy. The neurons were evaluated
for multiple modalities of data towards stress detection, which resulted in
energy savings of 25.12x to 39.20x and EDP gains of 52.37x to 81.9x over ANNs,
while offering a best accuracy of 98.8% when compared with the rest of the SOTA
implementations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 15 figures. Applied to IEEE Transactions on Computer Aided
  Design Journal. Awaiting a verdict</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Model-Based Reinforcement Learning Through Optimistic Thompson
  Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jasmine Bayrooti, Carl Henrik Ek, Amanda Prorok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning complex robot behavior through interactions with the environment
necessitates principled exploration. Effective strategies should prioritize
exploring regions of the state-action space that maximize rewards, with
optimistic exploration emerging as a promising direction aligned with this idea
and enabling sample-efficient reinforcement learning. However, existing methods
overlook a crucial aspect: the need for optimism to be informed by a belief
connecting the reward and state. To address this, we propose a practical,
theoretically grounded approach to optimistic exploration based on Thompson
sampling. Our model structure is the first that allows for reasoning about
joint uncertainty over transitions and rewards. We apply our method on a set of
MuJoCo and VMAS continuous control tasks. Our experiments demonstrate that
optimistic exploration significantly accelerates learning in environments with
sparse rewards, action penalties, and difficult-to-explore regions.
Furthermore, we provide insights into when optimism is beneficial and emphasize
the critical role of model uncertainty in guiding exploration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe Learning-Based Optimization of Model Predictive Control:
  Application to Battery Fast-Charging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Hirt, Andreas Höhl, Johannes Pohlodek, Joachim Schaeffer, Maik Pfefferkorn, Richard D. Braatz, Rolf Findeisen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model predictive control (MPC) is a powerful tool for controlling complex
nonlinear systems under constraints, but often struggles with model
uncertainties and the design of suitable cost functions. To address these
challenges, we discuss an approach that integrates MPC with safe Bayesian
optimization to optimize long-term closed-loop performance despite significant
model-plant mismatches. By parameterizing the MPC stage cost function using a
radial basis function network, we employ Bayesian optimization as a
multi-episode learning strategy to tune the controller without relying on
precise system models. This method mitigates conservativeness introduced by
overly cautious soft constraints in the MPC cost function and provides
probabilistic safety guarantees during learning, ensuring that safety-critical
constraints are met with high probability. As a practical application, we apply
our approach to fast charging of lithium-ion batteries, a challenging task due
to the complicated battery dynamics and strict safety requirements, subject to
the requirement to be implementable in real time. Simulation results
demonstrate that, in the context of model-plant mismatch, our method reduces
charging times compared to traditional MPC methods while maintaining safety.
This work extends previous research by emphasizing closed-loop constraint
satisfaction and offers a promising solution for enhancing performance in
systems where model uncertainties and safety are critical concerns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures, submitted to ACC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaboration! Towards Robust Neural Methods for Routing Problems <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianan Zhou, Yaoxin Wu, Zhiguang Cao, Wen Song, Jie Zhang, Zhiqi Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite enjoying desirable efficiency and reduced reliance on domain
expertise, existing neural methods for vehicle routing problems (VRPs) suffer
from severe robustness issues -- their performance significantly deteriorates
on clean instances with crafted perturbations. To enhance robustness, we
propose an ensemble-based Collaborative Neural Framework (CNF) w.r.t. the
defense of neural VRP methods, which is crucial yet underexplored in the
literature. Given a neural VRP method, we adversarially train multiple models
in a collaborative manner to synergistically promote robustness against
attacks, while boosting standard generalization on clean instances. A neural
router is designed to adeptly distribute training instances among models,
enhancing overall load balancing and collaborative efficacy. Extensive
experiments verify the effectiveness and versatility of CNF in defending
against various attacks across different neural VRP methods. Notably, our
approach also achieves impressive out-of-distribution generalization on
benchmark instances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Failure-Proof Non-Contrastive <span class="highlight-title">Self-Supervised</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Sansone, Tim Lebailly, Tinne Tuytelaars
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We identify sufficient conditions to avoid known failure modes, including
representation, dimensional, cluster and intracluster collapses, occurring in
non-contrastive self-supervised learning. Based on these findings, we propose a
principled design for the projector and loss function. We theoretically
demonstrate that this design introduces an inductive bias that promotes
learning representations that are both decorrelated and clustered without
explicit enforcing these properties and leading to improved generalization. To
the best of our knowledge, this is the first solution that achieves robust
training with respect to these failure modes while guaranteeing enhanced
generalization performance in downstream tasks. We validate our theoretical
findings on image datasets including SVHN, CIFAR10, CIFAR100 and ImageNet-100,
and show that our solution, dubbed FALCON, outperforms existing feature
decorrelation and cluster-based self-supervised learning methods in terms of
generalization to clustering and linear classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting and Approximating Redundant Computational Blocks in Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irene Cannistraci, Emanuele Rodolà, Bastian Rieck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks often learn similar internal representations, both
across different models and within their own layers. While inter-network
similarities have enabled techniques such as model stitching and merging,
intra-network similarities present new opportunities for designing more
efficient architectures. In this paper, we investigate the emergence of these
internal similarities across different layers in diverse neural architectures,
showing that similarity patterns emerge independently of the datataset used. We
introduce a simple metric, Block Redundancy, to detect redundant blocks,
providing a foundation for future architectural optimization methods. Building
on this, we propose Redundant Blocks Approximation (RBA), a general framework
that identifies and approximates one or more redundant computational blocks
using simpler transformations. We show that the transformation $\mathcal{T}$
between two representations can be efficiently computed in closed-form, and it
is enough to replace the redundant blocks from the network. RBA reduces model
parameters and time complexity while maintaining good performance. We validate
our method on classification tasks in the vision domain using a variety of
pretrained foundational models and datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 10 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Next state prediction gives rise to entangled, yet compositional
  representations of objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tankred Saanum, Luca M. Schulze Buschoff, Peter Dayan, Eric Schulz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compositional representations are thought to enable humans to generalize
across combinatorially vast state spaces. Models with learnable object slots,
which encode information about objects in separate latent codes, have shown
promise for this type of generalization but rely on strong architectural
priors. Models with distributed representations, on the other hand, use
overlapping, potentially entangled neural codes, and their ability to support
compositional generalization remains underexplored. In this paper we examine
whether distributed models can develop linearly separable representations of
objects, like slotted models, through unsupervised training on videos of object
interactions. We show that, surprisingly, models with distributed
representations often match or outperform models with object slots in
downstream prediction tasks. Furthermore, we find that linearly separable
object representations can emerge without object-centric priors, with auxiliary
objectives like next-state prediction playing a key role. Finally, we observe
that distributed models' object representations are never fully disentangled,
even if they are linearly separable: Multiple objects can be encoded through
partially overlapping neural populations while still being highly separable
with a linear classifier. We hypothesize that maintaining partially shared
codes enables distributed models to better compress object dynamics,
potentially enhancing generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Goal-Conditioned Terminal Value Estimation for Real-time and Multi-task
  Model Predictive Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitsuki Morita, Satoshi Yamamori, Satoshi Yagi, Norikazu Sugimoto, Jun Morimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While MPC enables nonlinear feedback control by solving an optimal control
problem at each timestep, the computational burden tends to be significantly
large, making it difficult to optimize a policy within the control period. To
address this issue, one possible approach is to utilize terminal value learning
to reduce computational costs. However, the learned value cannot be used for
other tasks in situations where the task dynamically changes in the original
MPC setup. In this study, we develop an MPC framework with goal-conditioned
terminal value learning to achieve multitask policy optimization while reducing
computational time. Furthermore, by using a hierarchical control structure that
allows the upper-level trajectory planner to output appropriate
goal-conditioned trajectories, we demonstrate that a robot model is able to
generate diverse motions. We evaluate the proposed method on a bipedal inverted
pendulum robot model and confirm that combining goal-conditioned terminal value
learning with an upper-level trajectory planner enables real-time control;
thus, the robot successfully tracks a target trajectory on sloped terrain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defense-as-a-Service: Black-box Shielding against Backdoored Graph
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Yang, Kai Zhou, Yuni Lai, Gaolei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the trend of large graph learning models, business owners tend to employ
a model provided by a third party to deliver business services to users.
However, these models might be backdoored, and malicious users can submit
trigger-embedded inputs to manipulate the model predictions. Current graph
backdoor defenses have several limitations: 1) depending on model-related
details, 2) requiring additional model fine-tuning, and 3) relying upon extra
explainability tools, all of which are infeasible under stringent privacy
policies. To address those limitations, we propose GraphProt, which allows
resource-constrained business owners to rely on third parties to avoid backdoor
attacks on GNN-based graph classifiers. Our GraphProt is model-agnostic and
only relies on the input graph. The key insight is to leverage subgraph
information for prediction, thereby mitigating backdoor effects induced by
triggers. GraphProt comprises two components: clustering-based trigger
elimination and robust subgraph ensemble. Specifically, we first propose
feature-topology clustering that aims to remove most of the anomalous subgraphs
(triggers). Moreover, we design subgraph sampling strategies based on
feature-topology clustering to build a robust classifier via majority vote.
Experimental results across three backdoor attacks and six benchmark datasets
demonstrate that GraphProt significantly reduces the backdoor attack success
rate while preserving the model accuracy on regular graph classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decomposition Polyhedra of Piecewise Linear Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marie-Charlotte Brandenburg, Moritz Grillo, Christoph Hertrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we contribute to the frequently studied question of how to
decompose a continuous piecewise linear (CPWL) function into a difference of
two convex CPWL functions. Every CPWL function has infinitely many such
decompositions, but for applications in optimization and neural network theory,
it is crucial to find decompositions with as few linear pieces as possible.
This is a highly challenging problem, as we further demonstrate by disproving a
recently proposed approach by Tran and Wang [Minimal representations of
tropical rational functions. Algebraic Statistics, 15(1):27-59, 2024]. To make
the problem more tractable, we propose to fix an underlying polyhedral complex
determining the possible locus of nonlinearity. Under this assumption, we prove
that the set of decompositions forms a polyhedron that arises as intersection
of two translated cones. We prove that irreducible decompositions correspond to
the bounded faces of this polyhedron and minimal solutions must be vertices. We
then identify cases with a unique minimal decomposition, and illustrate how our
insights have consequences in the theory of submodular functions. Finally, we
improve upon previous constructions of neural networks for a given convex CPWL
function and apply our framework to obtain results in the nonconvex case.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Rank Continual Personalization of Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Łukasz Staniszewski, Katarzyna Zaleska, Kamil Deja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent personalization methods for diffusion models, such as Dreambooth,
allow fine-tuning pre-trained models to generate new concepts. However,
applying these techniques across multiple tasks in order to include, e.g.,
several new objects or styles, leads to mutual interference between their
adapters. While recent studies attempt to mitigate this issue by combining
trained adapters across tasks after fine-tuning, we adopt a more rigorous
regime and investigate the personalization of large diffusion models under a
continual learning scenario, where such interference leads to catastrophic
forgetting of previous knowledge. To that end, we evaluate the na\"ive
continual fine-tuning of customized models and compare this approach with three
methods for consecutive adapters' training: sequentially merging new adapters,
merging orthogonally initialized adapters, and updating only relevant
parameters according to the task. In our experiments, we show that the proposed
approaches mitigate forgetting when compared to the na\"ive approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wide Neural Networks Trained with Weight Decay Provably Exhibit Neural
  Collapse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Jacot, Peter Súkeník, Zihan Wang, Marco Mondelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) at convergence consistently represent the
training data in the last layer via a highly symmetric geometric structure
referred to as neural collapse. This empirical evidence has spurred a line of
theoretical research aimed at proving the emergence of neural collapse, mostly
focusing on the unconstrained features model. Here, the features of the
penultimate layer are free variables, which makes the model data-agnostic and,
hence, puts into question its ability to capture DNN training. Our work
addresses the issue, moving away from unconstrained features and studying DNNs
that end with at least two linear layers. We first prove generic guarantees on
neural collapse that assume (i) low training error and balancedness of the
linear layers (for within-class variability collapse), and (ii) bounded
conditioning of the features before the linear part (for orthogonality of
class-means, as well as their alignment with weight matrices). We then show
that such assumptions hold for gradient descent training with weight decay: (i)
for networks with a wide first layer, we prove low training error and
balancedness, and (ii) for solutions that are either nearly optimal or stable
under large learning rates, we additionally prove the bounded conditioning.
Taken together, our results are the first to show neural collapse in the
end-to-end training of DNNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the Sampling Strategy in KernelSHAP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lars Henry Berge Olsen, Martin Jullum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shapley values are a popular model-agnostic explanation framework for
explaining predictions made by complex machine learning models. The framework
provides feature contribution scores that sum to the predicted response and
represent each feature's importance. The computation of exact Shapley values is
computationally expensive due to estimating an exponential amount of
non-trivial conditional expectations. The KernelSHAP framework enables us to
approximate the Shapley values using a sampled subset of weighted conditional
expectations. We propose three main novel contributions: a stabilizing
technique to reduce the variance of the weights in the current state-of-the-art
strategy, a novel weighing scheme that corrects the Shapley kernel weights
based on sampled subsets, and a straightforward strategy that includes the
important subsets and integrates them with the corrected Shapley kernel
weights. We compare these new approximation strategies against existing ones by
evaluating their Shapley value accuracy as a function of the number of subsets.
The results demonstrate that our sampling strategies significantly enhance the
accuracy of the approximated Shapley value explanations, making them more
reliable in practical applications. This work provides valuable insights and
practical recommendations for researchers and practitioners seeking to
implement Shapley value-based explainability of their models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Optimization and Generalization of Two-layer <span class="highlight-title">Transformer</span>s with
  Sign Gradient Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingrui Li, Wei Huang, Andi Han, Zhanpeng Zhou, Taiji Suzuki, Jun Zhu, Jianfei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Adam optimizer is widely used for transformer optimization in practice,
which makes understanding the underlying optimization mechanisms an important
problem. However, due to the Adam's complexity, theoretical analysis of how it
optimizes transformers remains a challenging task. Fortunately, Sign Gradient
Descent (SignGD) serves as an effective surrogate for Adam. Despite its
simplicity, theoretical understanding of how SignGD optimizes transformers
still lags behind. In this work, we study how SignGD optimizes a two-layer
transformer -- consisting of a softmax attention layer with trainable query-key
parameterization followed by a linear layer -- on a linearly separable noisy
dataset. We identify four stages in the training dynamics, each exhibiting
intriguing behaviors. Based on the training dynamics, we prove the fast
convergence but poor generalization of the learned transformer on the noisy
dataset. We also show that Adam behaves similarly to SignGD in terms of both
optimization and generalization in this setting. Additionally, we find that the
poor generalization of SignGD is not solely due to data noise, suggesting that
both SignGD and Adam requires high-quality data for real-world tasks. Finally,
experiments on synthetic and real-world datasets empirically support our
theoretical results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mastering Chinese Chess AI (Xiangqi) Without Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Chen, Juntong Lin, Zhichao Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We have developed a high-performance Chinese Chess AI that operates without
reliance on search algorithms. This AI has demonstrated the capability to
compete at a level commensurate with the top 0.1\% of human players. By
eliminating the search process typically associated with such systems, this AI
achieves a Queries Per Second (QPS) rate that exceeds those of systems based on
the Monte Carlo Tree Search (MCTS) algorithm by over a thousandfold and
surpasses those based on the AlphaBeta pruning algorithm by more than a
hundredfold. The AI training system consists of two parts: supervised learning
and reinforcement learning. Supervised learning provides an initial human-like
Chinese chess AI, while reinforcement learning, based on supervised learning,
elevates the strength of the entire AI to a new level. Based on this training
system, we carried out enough ablation experiments and discovered that 1. The
same parameter amount of Transformer architecture has a higher performance than
CNN on Chinese chess; 2. Possible moves of both sides as features can greatly
improve the training process; 3. Selective opponent pool, compared to pure
self-play training, results in a faster improvement curve and a higher strength
limit. 4. Value Estimation with Cutoff(VECT) improves the original PPO
algorithm training process and we will give the explanation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Skill Discovery for Robotic Manipulation through Automatic
  Task Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Jansonnie, Bingbing Wu, Julien Perez, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning skills that interact with objects is of major importance for robotic
manipulation. These skills can indeed serve as an efficient prior for solving
various manipulation tasks. We propose a novel Skill Learning approach that
discovers composable behaviors by solving a large and diverse number of
autonomously generated tasks. Our method learns skills allowing the robot to
consistently and robustly interact with objects in its environment. The
discovered behaviors are embedded in primitives which can be composed with
Hierarchical Reinforcement Learning to solve unseen manipulation tasks. In
particular, we leverage Asymmetric Self-Play to discover behaviors and
Multiplicative Compositional Policies to embed them. We compare our method to
Skill Learning baselines and find that our skills are more interactive.
Furthermore, the learned skills can be used to solve a set of unseen
manipulation tasks, in simulation as well as on a real robotic platform.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2024 IEEE-RAS International Conference on Humanoid
  Robots</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TimeCNN: Refining Cross-Variable Interaction on Time Point for Time
  Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao Hu, Dongkai Wang, Yong Dai, Shiyi Qi, Liangjian Wen, Jun Wang, Zhi Chen, Xun Zhou, Zenglin Xu, Jiang Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series forecasting is extensively applied across diverse domains.
Transformer-based models demonstrate significant potential in modeling
cross-time and cross-variable interaction. However, we notice that the
cross-variable correlation of multivariate time series demonstrates
multifaceted (positive and negative correlations) and dynamic progression over
time, which is not well captured by existing Transformer-based models. To
address this issue, we propose a TimeCNN model to refine cross-variable
interactions to enhance time series forecasting. Its key innovation is
timepoint-independent, where each time point has an independent convolution
kernel, allowing each time point to have its independent model to capture
relationships among variables. This approach effectively handles both positive
and negative correlations and adapts to the evolving nature of variable
relationships over time. Extensive experiments conducted on 12 real-world
datasets demonstrate that TimeCNN consistently outperforms state-of-the-art
models. Notably, our model achieves significant reductions in computational
requirements (approximately 60.46%) and parameter count (about 57.50%), while
delivering inference speeds 3 to 4 times faster than the benchmark iTransformer
model
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Strong Model Collapse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elvis Dohmatob, Yunzhen Feng, Julia Kempe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Within the scaling laws paradigm, which underpins the training of large
neural networks like ChatGPT and Llama, we consider a supervised regression
setting and establish the existance of a strong form of the model collapse
phenomenon, a critical performance degradation due to synthetic data in the
training corpus. Our results show that even the smallest fraction of synthetic
data (e.g., as little as 1\% of the total training dataset) can still lead to
model collapse: larger and larger training sets do not enhance performance. We
further investigate whether increasing model size, an approach aligned with
current trends in training large language models, exacerbates or mitigates
model collapse. In a simplified regime where neural networks are approximated
via random projections of tunable size, we both theoretically and empirically
show that larger models can amplify model collapse. Interestingly, our theory
also indicates that, beyond the interpolation threshold (which can be extremely
high for very large datasets), larger models may mitigate the collapse,
although they do not entirely prevent it. Our theoretical findings are
empirically verified through experiments on language models and feed-forward
neural networks for images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Fusion Strategies for Mapping Biophysical Landscape Features <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucia Gordon, Nico Lang, Catherine Ressijac, Andrew Davies
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal aerial data are used to monitor natural systems, and machine
learning can significantly accelerate the classification of landscape features
within such imagery to benefit ecology and conservation. It remains
under-explored, however, how these multiple modalities ought to be fused in a
deep learning model. As a step towards filling this gap, we study three
strategies (Early fusion, Late fusion, and Mixture of Experts) for fusing
thermal, RGB, and LiDAR imagery using a dataset of spatially-aligned
orthomosaics in these three modalities. In particular, we aim to map three
ecologically-relevant biophysical landscape features in African savanna
ecosystems: rhino middens, termite mounds, and water. The three fusion
strategies differ in whether the modalities are fused early or late, and if
late, whether the model learns fixed weights per modality for each class or
generates weights for each class adaptively, based on the input. Overall, the
three methods have similar macro-averaged performance with Late fusion
achieving an AUC of 0.698, but their per-class performance varies strongly,
with Early fusion achieving the best recall for middens and water and Mixture
of Experts achieving the best recall for mounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, ECCV 2024 Workshop in CV for Ecology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taming Gradient Oversmoothing and Expansion in Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        MoonJeong Park, Dongwoo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Oversmoothing has been claimed as a primary bottleneck for multi-layered
graph neural networks (GNNs). Multiple analyses have examined how and why
oversmoothing occurs. However, none of the prior work addressed how
optimization is performed under the oversmoothing regime. In this work, we show
the presence of $\textit{gradient oversmoothing}$ preventing optimization
during training. We further analyze that GNNs with residual connections, a
well-known solution to help gradient flow in deep architecture, introduce
$\textit{gradient expansion}$, a phenomenon of the gradient explosion in
diverse directions. Therefore, adding residual connections cannot be a solution
for making a GNN deep. Our analysis reveals that constraining the Lipschitz
bound of each layer can neutralize the gradient expansion. To this end, we
provide a simple yet effective normalization method to prevent the gradient
expansion. An empirical study shows that the residual GNNs with hundreds of
layers can be efficiently trained with the proposed normalization without
compromising performance. Additional studies show that the empirical
observations corroborate our theoretical analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Informed GNN for non-linear constrained optimization: PINCO a
  solver for the AC-optimal power flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04818v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04818v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Varbella, Damien Briens, Blazhe Gjorgiev, Giuseppe Alessio D'Inverno, Giovanni Sansavini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The energy transition is driving the integration of large shares of
intermittent power sources in the electric power grid. Therefore, addressing
the AC optimal power flow (AC-OPF) effectively becomes increasingly essential.
The AC-OPF, which is a fundamental optimization problem in power systems, must
be solved more frequently to ensure the safe and cost-effective operation of
power systems. Due to its non-linear nature, AC-OPF is often solved in its
linearized form, despite inherent inaccuracies. Non-linear solvers, such as the
interior point method, are typically employed to solve the full OPF problem.
However, these iterative methods may not converge for large systems and do not
guarantee global optimality. This work explores a physics-informed graph neural
network, PINCO, to solve the AC-OPF. We demonstrate that this method provides
accurate solutions in a fraction of the computational time when compared to the
established non-linear programming solvers. Remarkably, PINCO generalizes
effectively across a diverse set of loading conditions in the power system. We
show that our method can solve the AC-OPF without violating inequality
constraints. Furthermore, it can function both as a solver and as a hybrid
universal function approximator. Moreover, the approach can be easily adapted
to different power systems with minimal adjustments to the hyperparameters,
including systems with multiple generators at each bus. Overall, this work
demonstrates an advancement in the field of power system optimization to tackle
the challenges of the energy transition. The code and data utilized in this
paper are available at https://anonymous.4open.science/r/opf_pinn_iclr-B83E/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Interpretable Hierarchical Dynamical Systems Models from Time
  Series Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Brenner, Elias Weber, Georgia Koppe, Daniel Durstewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In science, we are often interested in obtaining a generative model of the
underlying system dynamics from observed time series. While powerful methods
for dynamical systems reconstruction (DSR) exist when data come from a single
domain, how to best integrate data from multiple dynamical regimes and leverage
it for generalization is still an open question. This becomes particularly
important when individual time series are short, and group-level information
may help to fill in for gaps in single-domain data. At the same time, averaging
is not an option in DSR, as it will wipe out crucial dynamical properties
(e.g., limit cycles in one domain vs. chaos in another). Hence, a framework is
needed that enables to efficiently harvest group-level (multi-domain)
information while retaining all single-domain dynamical characteristics. Here
we provide such a hierarchical approach and showcase it on popular DSR
benchmarks, as well as on neuroscientific and medical time series. In addition
to faithful reconstruction of all individual dynamical regimes, our
unsupervised methodology discovers common low-dimensional feature spaces in
which datasets with similar dynamics cluster. The features spanning these
spaces were further dynamically highly interpretable, surprisingly in often
linear relation to control parameters that govern the dynamics of the
underlying system. Finally, we illustrate transfer learning and generalization
to new parameter regimes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedBiP: Heterogeneous One-Shot Federated Learning with Personalized
  Latent Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haokun Chen, Hang Li, Yao Zhang, Gengyuan Zhang, Jinhe Bi, Philip Torr, Jindong Gu, Denis Krompass, Volker Tresp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One-Shot Federated Learning (OSFL), a special decentralized machine learning
paradigm, has recently gained significant attention. OSFL requires only a
single round of client data or model upload, which reduces communication costs
and mitigates privacy threats compared to traditional FL. Despite these
promising prospects, existing methods face challenges due to client data
heterogeneity and limited data quantity when applied to real-world OSFL
systems. Recently, Latent Diffusion Models (LDM) have shown remarkable
advancements in synthesizing high-quality images through pretraining on
large-scale datasets, thereby presenting a potential solution to overcome these
issues. However, directly applying pretrained LDM to heterogeneous OSFL results
in significant distribution shifts in synthetic data, leading to performance
degradation in classification models trained on such data. This issue is
particularly pronounced in rare domains, such as medical imaging, which are
underrepresented in LDM's pretraining data. To address this challenge, we
propose Federated Bi-Level Personalization (FedBiP), which personalizes the
pretrained LDM at both instance-level and concept-level. Hereby, FedBiP
synthesizes images following the client's local data distribution without
compromising the privacy regulations. FedBiP is also the first approach to
simultaneously address feature space heterogeneity and client data scarcity in
OSFL. Our method is validated through extensive experiments on three OSFL
benchmarks with feature space heterogeneity, as well as on challenging medical
and satellite image datasets with label heterogeneity. The results demonstrate
the effectiveness of FedBiP, which substantially outperforms other OSFL
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ mDPO: Conditional Preference Optimization for Multimodal Large Language
  Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Wang, Wenxuan Zhou, James Y. Huang, Nan Xu, Sheng Zhang, Hoifung Poon, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct preference optimization (DPO) has shown to be an effective method for
large language model (LLM) alignment. Recent works have attempted to apply DPO
to multimodal scenarios but have found it challenging to achieve consistent
improvement. Through a comparative experiment, we identify the unconditional
preference problem in multimodal preference optimization, where the model
overlooks the image condition. To address this problem, we propose mDPO, a
multimodal DPO objective that prevents the over-prioritization of language-only
preferences by also optimizing image preference. Moreover, we introduce a
reward anchor that forces the reward to be positive for chosen responses,
thereby avoiding the decrease in their likelihood -- an intrinsic problem of
relative preference optimization. Experiments on two multimodal LLMs of
different sizes and three widely used benchmarks demonstrate that mDPO
effectively addresses the unconditional preference problem in multimodal
preference optimization and significantly improves model performance,
particularly in reducing hallucination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference. Project website:
  https://feiwang96.github.io/mDPO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How
  to Fix It) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17975v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17975v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Meeus, Igor Shilov, Shubham Jain, Manuel Faysse, Marek Rei, Yves-Alexandre de Montjoye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whether LLMs memorize their training data and what this means, from privacy
leakage to detecting copyright violations -- has become a rapidly growing area
of research over the last two years. In recent months, more than 10 new methods
have been proposed to perform Membership Inference Attacks (MIAs) against LLMs.
Contrary to traditional MIAs which rely on fixed -- but randomized -- records
or models, these methods are mostly evaluated on datasets collected post-hoc.
Sets of members and non-members, used to evaluate the MIA, are constructed
using informed guesses after the release of a model. This lack of randomization
raises concerns of a distribution shift between members and non-members. In the
first part, we review the literature on MIAs against LLMs. While most work
focuses on sequence-level MIAs evaluated in post-hoc setups, we show that a
range of target models, motivations and units of interest have been considered
in the literature. We then quantify distribution shifts present in the 6
datasets used in the literature, ranging from books to papers, using a bag of
word classifier. Our analysis reveals that all of them suffer from severe
distribution shifts. This challenges the validity of using such setups to
measure LLM memorization and may undermine the benchmarking of recently
proposed methods. Yet, all hope might not be lost. In the second part, we
introduce important considerations to properly evaluate MIAs against LLMs and
discuss potential ways forward: randomized test splits, injections of
randomized (unique) sequences, randomized finetuning, and post-hoc control
methods. While each option comes with its advantages and limitations, we
believe they collectively provide solid grounds to guide the development of MIA
methods and study LLM memorization. We conclude by proposing comprehensive,
easy-to-use benchmarks for sequence- and document-level MIAs against LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Parameter-Efficient Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00700v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00700v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinmay Savadikar, Xi Song, Tianfu Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Generative Parameter-Efficient Fine-Tuning (GIFT) for adapting
pretrained Transformer backbones on downstream tasks. GIFT learns to generate
the fine-tuned weights for a layer directly from its pretrained weights. The
GIFT network is parameterized in a minimally-simple way by two linear layers
(without bias terms), and is shared by different pretrained layers selected for
fine-tuning (e.g., the Query layers), which result in significantly fewer
trainable parameters compared to the layer-specific methods like Low-Rank
Adapter (LoRA). We also show this formulation bridges parameter-efficient
fine-tuning and representation fine-tuning. We perform comprehensive
experiments on natural language tasks (commonsense and arithmetic reasoning,
instruction tuning, and sequence classification) and computer vision tasks
(fine-grained classification). We obtain the best performance and parameter
efficiency among baselines on commonsense and arithmetic reasoning, and
instruction following using the Llama family of models and on visual
recognition benchmarks using Vision Transformers. Notably, compared to LoRA, we
obtain 5.7% absolute increase in average accuracy with 14 times reduction of
parameters on Commonsense170k using Llama-3 (8B), and 5.4% absolute increase in
the win rate with 4 times reduction of parameters using Llama-2 (7B) during
instruction tuning. Our GIFT also obtains a slightly higher win rate on
instruction tuning than GPT 3.5 (Turbo 1106).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page and code: https://savadikarc.github.io/gift</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jogging the Memory of Unlearned LLMs Through Targeted Relearning Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13356v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13356v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyuan Hu, Yiwei Fu, Zhiwei Steven Wu, Virginia Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning is a promising approach to mitigate undesirable
memorization of training data in LLMs. However, in this work we show that
existing approaches for unlearning in LLMs are surprisingly susceptible to a
simple set of targeted relearning attacks. With access to only a small and
potentially loosely related set of data, we find that we can "jog" the memory
of unlearned models to reverse the effects of unlearning. For example, we show
that relearning on public medical articles can lead an unlearned LLM to output
harmful knowledge about bioweapons, and relearning general wiki information
about the book series Harry Potter can force the model to output verbatim
memorized text. We formalize this unlearning-relearning pipeline, explore the
attack across three popular unlearning benchmarks, and discuss future
directions and guidelines that result from our study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 5 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Successor Features with Distributed Hebbian Temporal Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13391v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13391v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evgenii Dzhivelikian, Petr Kuderov, Aleksandr I. Panov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to address the challenge of online
temporal memory learning for decision-making under uncertainty in
non-stationary, partially observable environments. The proposed algorithm,
Distributed Hebbian Temporal Memory (DHTM), is based on factor graph formalism
and a multicomponent neuron model. DHTM aims to capture sequential data
relationships and make cumulative predictions about future observations,
forming Successor Features (SF). Inspired by neurophysiological models of the
neocortex, the algorithm utilizes distributed representations, sparse
transition matrices, and local Hebbian-like learning rules to overcome the
instability and slow learning process of traditional temporal memory algorithms
like RNN and HMM. Experimental results demonstrate that DHTM outperforms LSTM
and a biologically inspired HMM-like algorithm, CSCG, in the case of
non-stationary datasets. Our findings suggest that DHTM is a promising approach
for addressing the challenges of online sequence learning and planning in
dynamic environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Full Line Code Completion: Bringing AI to Desktop 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.08704v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.08704v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Semenkin, Vitaliy Bibaev, Yaroslav Sokolov, Kirill Krylov, Alexey Kalina, Anna Khannanova, Danila Savenkov, Darya Rovdo, Igor Davidenko, Kirill Karnaukhov, Maxim Vakhrushev, Mikhail Kostyukov, Mikhail Podvitskii, Petr Surkov, Yaroslav Golubev, Nikita Povarov, Timofey Bryksin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, several industrial solutions for the problem of multi-token
code completion appeared, each making a great advance in the area but mostly
focusing on cloud-based runtime and avoiding working on the end user's device.
  In this work, we describe our approach for building a multi-token code
completion feature for the JetBrains' IntelliJ Platform, which we call Full
Line Code Completion. The feature suggests only syntactically correct code and
works fully locally, i.e., data querying and the generation of suggestions
happens on the end user's machine. We share important time and
memory-consumption restrictions, as well as design principles that a code
completion engine should satisfy. Working entirely on the end user's device,
our code completion engine enriches user experience while being not only fast
and compact but also secure. We share a number of useful techniques to meet the
stated development constraints and also describe offline and online evaluation
pipelines that allowed us to make better decisions.
  Our online evaluation shows that the usage of the tool leads to 1.3 times
more Python code in the IDE being produced by code completion. The described
solution was initially started with a help of researchers and was then bundled
into all JetBrains IDEs where it is now used by millions of users. Thus, we
believe that this work is useful for bridging academia and industry, providing
researchers with the knowledge of what happens when complex research-based
solutions are integrated into real products.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stateful Large Language Model Serving with Pensieve 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05516v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05516v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingfan Yu, Jinkun Lin, Jinyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are wildly popular today and it is important to
serve them efficiently. Existing LLM serving systems are stateless across
requests. Consequently, when LLMs are used in the common setting of multi-turn
conversations, a growing log of the conversation history must be processed
alongside any request by the serving system at each turn, resulting in repeated
processing.
  In this paper, we design $Pensieve$, a system optimized for multi-turn
conversation LLM serving. $Pensieve$ maintains the conversation state across
requests by caching previously processed history to avoid duplicate processing.
$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to
efficiently store and retrieve cached data. $Pensieve$ also generalizes the
recent PagedAttention kernel to support attention between multiple input tokens
with a GPU cache spread over non-contiguous memory. Our evaluation shows that
$Pensieve$ can achieve $1.14$-$3.0\times$ the throughput of vLLM and
TensorRT-LLM and significantly reduce latency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Convex Optimization with a Separation Oracle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02476v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02476v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zakaria Mhammedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a new projection-free algorithm for Online Convex
Optimization (OCO) with a state-of-the-art regret guarantee among
separation-based algorithms. Existing projection-free methods based on the
classical Frank-Wolfe algorithm achieve a suboptimal regret bound of
$O(T^{3/4})$, while more recent separation-based approaches guarantee a regret
bound of $O(\kappa \sqrt{T})$, where $\kappa$ denotes the asphericity of the
feasible set, defined as the ratio of the radii of the containing and contained
balls. However, for ill-conditioned sets, $\kappa$ can be arbitrarily large,
potentially leading to poor performance. Our algorithm achieves a regret bound
of $\widetilde{O}(\sqrt{dT} + \kappa d)$, while requiring only
$\widetilde{O}(1)$ calls to a separation oracle per round. Crucially, the main
term in the bound, $\widetilde{O}(\sqrt{d T})$, is independent of $\kappa$,
addressing the limitations of previous methods. Additionally, as a by-product
of our analysis, we recover the $O(\kappa \sqrt{T})$ regret bound of existing
OCO algorithms with a more straightforward analysis and improve the regret
bound for projection-free online exp-concave optimization. Finally, for
constrained stochastic convex optimization, we achieve a state-of-the-art
convergence rate of $\widetilde{O}(\sigma/\sqrt{T} + \kappa d/T)$, where
$\sigma$ represents the noise in the stochastic gradients, while requiring only
$\widetilde{O}(1)$ calls to a separation oracle per iteration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAnDOIT: Causal Discovery with Observational and Interventional Data
  from Time-Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Castri, Sariah Mghames, Marc Hanheide, Nicola Bellotto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of cause-and-effect is of the utmost importance in many branches of
science, but also for many practical applications of intelligent systems. In
particular, identifying causal relationships in situations that include hidden
factors is a major challenge for methods that rely solely on observational data
for building causal models. This paper proposes CAnDOIT, a causal discovery
method to reconstruct causal models using both observational and interventional
time-series data. The use of interventional data in the causal analysis is
crucial for real-world applications, such as robotics, where the scenario is
highly complex and observational data alone are often insufficient to uncover
the correct causal structure. Validation of the method is performed initially
on randomly generated synthetic models and subsequently on a well-known
benchmark for causal structure learning in a robotic manipulation environment.
The experiments demonstrate that the approach can effectively handle data from
interventions and exploit them to enhance the accuracy of the causal analysis.
A Python implementation of CAnDOIT has also been developed and is publicly
available on GitHub: https://github.com/lcastri/causalflow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Advanced Intelligent Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Aggregation of Prediction Intervals under Unsupervised Domain
  Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10302v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10302v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Ge, Debarghya Mukherjee, Jianqing Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As machine learning models are increasingly deployed in dynamic environments,
it becomes paramount to assess and quantify uncertainties associated with
distribution shifts. A distribution shift occurs when the underlying
data-generating process changes, leading to a deviation in the model's
performance. The prediction interval, which captures the range of likely
outcomes for a given prediction, serves as a crucial tool for characterizing
uncertainties induced by their underlying distribution. In this paper, we
propose methodologies for aggregating prediction intervals to obtain one with
minimal width and adequate coverage on the target domain under unsupervised
domain shift, under which we have labeled samples from a related source domain
and unlabeled covariates from the target domain. Our analysis encompasses
scenarios where the source and the target domain are related via i) a bounded
density ratio, and ii) a measure-preserving transformation. Our proposed
methodologies are computationally efficient and easy to implement. Beyond
illustrating the performance of our method through real-world datasets, we also
delve into the theoretical details. This includes establishing rigorous
theoretical guarantees, coupled with finite sample bounds, regarding the
coverage and width of our prediction intervals. Our approach excels in
practical applications and is underpinned by a solid theoretical framework,
ensuring its reliability and effectiveness across diverse contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Principal-Agent Reinforcement Learning: Orchestrating AI Agents with
  Contracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18074v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18074v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dima Ivanov, Paul Dütting, Inbal Talgam-Cohen, Tonghan Wang, David C. Parkes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing deployment of AI is shaping the future landscape of the
internet, which is set to become an integrated ecosystem of AI agents.
Orchestrating the interaction among AI agents necessitates decentralized,
self-sustaining mechanisms that harmonize the tension between individual
interests and social welfare. In this paper we tackle this challenge by
synergizing reinforcement learning with principal-agent theory from economics.
Taken separately, the former allows unrealistic freedom of intervention, while
the latter struggles to scale in sequential settings. Combining them achieves
the best of both worlds. We propose a framework where a principal guides an
agent in a Markov Decision Process (MDP) using a series of contracts, which
specify payments by the principal based on observable outcomes of the agent's
actions. We present and analyze a meta-algorithm that iteratively optimizes the
policies of the principal and agent, showing its equivalence to a contraction
operator on the principal's Q-function, and its convergence to subgame-perfect
equilibrium. We then scale our algorithm with deep Q-learning and analyze its
convergence in the presence of approximation error, both theoretically and
through experiments with randomly generated binary game-trees. Extending our
framework to multiple agents, we apply our methodology to the combinatorial
Coin Game. Addressing this multi-agent sequential social dilemma is a promising
first step toward scaling our approach to more complex, real-world instances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Creative Beam Search: LLM-as-a-Judge For Improving Response Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00099v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00099v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Franceschelli, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are revolutionizing several areas, including artificial
creativity. However, the process of generation in machines profoundly diverges
from that observed in humans. In particular, machine generation is
characterized by a lack of intentionality and an underlying creative process.
We propose a method called Creative Beam Search that uses Diverse Beam Search
and LLM-as-a-Judge to perform response generation and response validation. The
results of a qualitative experiment show how our approach can provide better
output than standard sampling techniques. We also show that the response
validation step is a necessary complement to the response generation step.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented as a short paper at the 15th International Conference on
  Computational Creativity (ICCC'24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Forest Proximities for Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03098v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03098v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Shaw, Jake Rhodes, Soukaina Filali Boubrahimi, Kevin R. Moon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RF-GAP has recently been introduced as an improved random forest proximity
measure. In this paper, we present PF-GAP, an extension of RF-GAP proximities
to proximity forests, an accurate and efficient time series classification
model. We use the forest proximities in connection with Multi-Dimensional
Scaling to obtain vector embeddings of univariate time series, comparing the
embeddings to those obtained using various time series distance measures. We
also use the forest proximities alongside Local Outlier Factors to investigate
the connection between misclassified points and outliers, comparing with
nearest neighbor classifiers which use time series distance measures. We show
that the forest proximities may exhibit a stronger connection between
misclassified points and outliers than nearest neighbor classifiers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Foundation Models as Data Compression: On Information, Model
  Weights and Copyright Law 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13493v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13493v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Franceschelli, Claudia Cevenini, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The training process of foundation models as for other classes of deep
learning systems is based on minimizing the reconstruction error over a
training set. For this reason, they are susceptible to the memorization and
subsequent reproduction of training samples. In this paper, we introduce a
training-as-compressing perspective, wherein the model's weights embody a
compressed representation of the training data. From a copyright standpoint,
this point of view implies that the weights could be considered a reproduction
or a derivative work of a potentially protected set of works. We investigate
the technical and legal challenges that emerge from this framing of the
copyright of outputs generated by foundation models, including their
implications for practitioners and researchers. We demonstrate that adopting an
information-centric approach to the problem presents a promising pathway for
tackling these emerging complex legal issues.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Spotlight presentation at GenLaw'24, see
  https://www.genlaw.org/2024-icml-papers#training-foundation-models-as-data-compression-on-information-model-weights-and-copyright-law</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaMetrics: Calibrating Metrics For Generation Tasks Using Human
  Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genta Indra Winata, David Anugraha, Lucky Susanto, Garry Kuwanto, Derry Tanti Wijaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the quality of a performance evaluation metric is crucial for
ensuring that model outputs align with human preferences. However, it remains
unclear how well each metric captures the diverse aspects of these preferences,
as metrics often excel in one particular area but not across all dimensions. To
address this, it is essential to systematically calibrate metrics to specific
aspects of human preference, catering to the unique characteristics of each
aspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate
generation tasks across different modalities in a supervised manner.
MetaMetrics optimizes the combination of existing metrics to enhance their
alignment with human preferences. Our metric demonstrates flexibility and
effectiveness in both language and vision downstream tasks, showing significant
benefits across various multilingual and multi-domain scenarios. MetaMetrics
aligns closely with human preferences and is highly extendable and easily
integrable into any application. This makes MetaMetrics a powerful tool for
improving the evaluation of generation tasks, ensuring that metrics are more
representative of human judgment across diverse contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning Based Optimal Design of Fibrillar Adhesives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05928v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05928v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Shojaeifard, Matteo Ferraresso, Alessandro Lucantonio, Mattia Bacca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fibrillar adhesion, observed in animals like beetles, spiders, and geckos,
relies on nanoscopic or microscopic fibrils to enhance surface adhesion via
'contact splitting.' This concept has inspired engineering applications across
robotics, transportation, and medicine. Recent studies suggest that functional
grading of fibril properties can improve adhesion, but this is a complex design
challenge that has only been explored in simplified geometries. While machine
learning (ML) has gained traction in adhesive design, no previous attempts have
targeted fibril-array scale optimization. In this study, we propose an ML-based
tool that optimizes the distribution of fibril compliance to maximize adhesive
strength. Our tool, featuring two deep neural networks (DNNs), recovers
previous design results for simple geometries and introduces novel solutions
for complex configurations. The Predictor DNN estimates adhesive strength based
on random compliance distributions, while the Designer DNN optimizes compliance
for maximum strength using gradient-based optimization. Our method
significantly reduces test error and accelerates the optimization process,
offering a high-performance solution for designing fibrillar adhesives and
micro-architected materials aimed at fracture resistance by achieving equal
load sharing (ELS).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02151v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02151v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that even the most recent safety-aligned LLMs are not robust to
simple adaptive jailbreaking attacks. First, we demonstrate how to successfully
leverage access to logprobs for jailbreaking: we initially design an
adversarial prompt template (sometimes adapted to the target LLM), and then we
apply random search on a suffix to maximize a target logprob (e.g., of the
token "Sure"), potentially with multiple restarts. In this way, we achieve 100%
attack success rate -- according to GPT-4 as a judge -- on Vicuna-13B,
Mistral-7B, Phi-3-Mini, Nemotron-4-340B, Llama-2-Chat-7B/13B/70B,
Llama-3-Instruct-8B, Gemma-7B, GPT-3.5, GPT-4o, and R2D2 from HarmBench that
was adversarially trained against the GCG attack. We also show how to jailbreak
all Claude models -- that do not expose logprobs -- via either a transfer or
prefilling attack with a 100% success rate. In addition, we show how to use
random search on a restricted set of tokens for finding trojan strings in
poisoned models -- a task that shares many similarities with jailbreaking --
which is the algorithm that brought us the first place in the SaTML'24 Trojan
Detection Competition. The common theme behind these attacks is that adaptivity
is crucial: different models are vulnerable to different prompting templates
(e.g., R2D2 is very sensitive to in-context learning prompts), some models have
unique vulnerabilities based on their APIs (e.g., prefilling for Claude), and
in some settings, it is crucial to restrict the token search space based on
prior knowledge (e.g., for trojan detection). For reproducibility purposes, we
provide the code, logs, and jailbreak artifacts in the JailbreakBench format at
https://github.com/tml-epfl/llm-adaptive-attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updates in the v3: GPT-4o and Claude 3.5 Sonnet results, improved
  writing. Updates in the v2: more models (Llama3, Phi-3, Nemotron-4-340B),
  jailbreak artifacts for all attacks are available, evaluation with different
  judges (Llama-3-70B and Llama Guard 2), more experiments (convergence plots
  over iterations, ablation on the suffix length for random search), examples
  of jailbroken generation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Model-Agnostic Multi-Group Equivariant Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09675v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09675v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Razan Baltaji, Sourya Basu, Lav R. Varshney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constructing model-agnostic group equivariant networks, such as equitune
(Basu et al., 2023b) and its generalizations (Kim et al., 2023), can be
computationally expensive for large product groups. We address this problem by
providing efficient model-agnostic equivariant designs for two related
problems: one where the network has multiple inputs each with potentially
different groups acting on them, and another where there is a single input but
the group acting on it is a large product group. For the first design, we
initially consider a linear model and characterize the entire equivariant space
that satisfies this constraint. This characterization gives rise to a novel
fusion layer between different channels that satisfies an invariance-symmetry
(IS) constraint, which we call an IS layer. We then extend this design beyond
linear models, similar to equitune, consisting of equivariant and IS layers. We
also show that the IS layer is a universal approximator of invariant-symmetric
functions. Inspired by the first design, we use the notion of the IS property
to design a second efficient model-agnostic equivariant design for large
product groups acting on a single input. For the first design, we provide
experiments on multi-image classification where each view is transformed
independently with transformations such as rotations. We find equivariant
models are robust to such transformations and perform competitively otherwise.
For the second design, we consider three applications: language
compositionality on the SCAN dataset to product groups; fairness in natural
language generation from GPT-2 to address intersectionality; and robust
zero-shot image classification with CLIP. Overall, our methods are simple and
general, competitive with equitune and its variants, while also being
computationally more efficient.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When "A Helpful Assistant" Is Not Really Helpful: Personas in System
  <span class="highlight-title">Prompt</span>s Do Not Improve Performances of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10054v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10054v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingqian Zheng, Jiaxin Pei, Lajanugen Logeswaran, Moontae Lee, David Jurgens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompting serves as the major way humans interact with Large Language Models
(LLM). Commercial AI systems commonly define the role of the LLM in system
prompts. For example, ChatGPT uses "You are a helpful assistant" as part of its
default system prompt. Despite current practices of adding personas to system
prompts, it remains unclear how different personas affect a model's performance
on objective tasks. In this study, we present a systematic evaluation of
personas in system prompts. We curate a list of 162 roles covering 6 types of
interpersonal relationships and 8 domains of expertise. Through extensive
analysis of 4 popular families of LLMs and 2,410 factual questions, we
demonstrate that adding personas in system prompts does not improve model
performance across a range of questions compared to the control setting where
no persona is added. Nevertheless, further analysis suggests that the gender,
type, and domain of the persona can all influence the resulting prediction
accuracies. We further experimented with a list of persona search strategies
and found that, while aggregating results from the best persona for each
question significantly improves prediction accuracy, automatically identifying
the best persona is challenging, with predictions often performing no better
than random selection. Overall, our findings suggest that while adding a
persona may lead to performance gains in certain settings, the effect of each
persona can be largely random. Code and data are available at
https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Steer Markovian Agents under Model Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10207v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10207v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Huang, Vinzenz Thoma, Zebang Shen, Heinrich H. Nax, Niao He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing incentives for an adapting population is a ubiquitous problem in a
wide array of economic applications and beyond. In this work, we study how to
design additional rewards to steer multi-agent systems towards desired policies
\emph{without} prior knowledge of the agents' underlying learning dynamics.
Motivated by the limitation of existing works, we consider a new and general
category of learning dynamics called \emph{Markovian agents}. We introduce a
model-based non-episodic Reinforcement Learning (RL) formulation for our
steering problem. Importantly, we focus on learning a \emph{history-dependent}
steering strategy to handle the inherent model uncertainty about the agents'
learning dynamics. We introduce a novel objective function to encode the
desiderata of achieving a good steering outcome with reasonable cost.
Theoretically, we identify conditions for the existence of steering strategies
to guide agents to the desired policies. Complementing our theoretical
contributions, we provide empirical algorithms to approximately solve our
objective, which effectively tackles the challenge in learning
history-dependent strategies. We demonstrate the efficacy of our algorithms
through empirical evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Fusion: Capturing Dependencies in Contrastive Learning via
  <span class="highlight-title">Transformer</span> Projection Heads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18681v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18681v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Learning (CL) has emerged as a powerful method for training
feature extraction models using unlabeled data. Recent studies suggest that
incorporating a linear projection head post-backbone significantly enhances
model performance. In this work, we investigate the use of a transformer model
as a projection head within the CL framework, aiming to exploit the
transformer's capacity for capturing long-range dependencies across embeddings
to further improve performance. Our key contributions are fourfold: First, we
introduce a novel application of transformers in the projection head role for
contrastive learning, marking the first endeavor of its kind. Second, our
experiments reveal a compelling "Deep Fusion" phenomenon where the attention
mechanism progressively captures the correct relational dependencies among
samples from the same class in deeper layers. Third, we provide a theoretical
framework that explains and supports this "Deep Fusion" behavior. Finally, we
demonstrate through experimental results that our model achieves superior
performance compared to the existing approach of using a feed-forward layer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary Ankner, Rishab Parthasarathy, Aniruddha Nrusimha, Christopher Rinard, Jonathan Ragan-Kelley, William Brandon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To combat the memory bandwidth-bound nature of autoregressive LLM inference,
previous research has proposed the speculative decoding frame-work. To perform
speculative decoding, a small draft model proposes candidate continuations of
the input sequence that are then verified in parallel by the base model. One
way to specify the draft model, as used in the recent Medusa decoding
framework, is as a collection of lightweight heads, called draft heads, that
operate on the base model's hidden states. To date, all existing draft heads
have been sequentially independent, meaning that they speculate tokens in the
candidate continuation independently of any preceding tokens in the candidate
continuation. In this work, we propose Hydra heads: a sequentially-dependent
drop-in replacement for standard draft heads that significantly improves the
accuracy of draft head speculation. We further explore the design space of
Hydra head training objectives and architectures, and propose a carefully tuned
Hydra head recipe, which we call Hydra++, that improves decoding throughput by
up to 1.31x and 2.70x compared to Medusa decoding and autoregressive de-coding
respectively. Overall, Hydra heads are a simple and well-motivated intervention
on standard draft heads that significantly improve the end-to-end speed of
draft head-based speculative decoding. We make our code publicly available at
https://github.com/zankner/Hydra.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The SkipSponge Attack: Sponge Weight Poisoning of Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06357v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06357v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jona te Lintelo, Stefanos Koffas, Stjepan Picek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sponge attacks aim to increase the energy consumption and computation time of
neural networks. In this work, we present a novel sponge attack called
SkipSponge. SkipSponge is the first sponge attack that is performed directly on
the parameters of a pre-trained model using only a few data samples. Our
experiments show that SkipSponge can successfully increase the energy
consumption of image classification models, GANs, and autoencoders requiring
fewer samples than the state-of-the-art (Sponge Poisoning). We show that
poisoning defenses are ineffective if not adjusted specifically for the defense
against SkipSponge (i.e., they decrease target layer bias values). Our work
shows that SkipSponge is more effective on the GANs and the autoencoders than
Sponge Poisoning. Additionally, SkipSponge is stealthier than Sponge Poisoning
as it does not require significant changes in the victim model's weights. Our
experiments indicate that SkipSponge can be performed even when an attacker has
access to only 1% of the entire dataset and reaches up to 13% energy increase.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Multimodal Learning with Missing Modalities via
  Parameter-Efficient Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03986v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03986v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Kaykobad Reza, Ashley Prater-Bennette, M. Salman Asif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal learning seeks to utilize data from multiple sources to improve
the overall performance of downstream tasks. It is desirable for redundancies
in the data to make multimodal systems robust to missing or corrupted
observations in some correlated modalities. However, we observe that the
performance of several existing multimodal networks significantly deteriorates
if one or multiple modalities are absent at test time. To enable robustness to
missing modalities, we propose a simple and parameter-efficient adaptation
procedure for pretrained multimodal networks. In particular, we exploit
modulation of intermediate features to compensate for the missing modalities.
We demonstrate that such adaptation can partially bridge performance drop due
to missing modalities and outperform independent, dedicated networks trained
for the available modality combinations in some cases. The proposed adaptation
requires extremely small number of parameters (e.g., fewer than 1% of the total
parameters) and applicable to a wide range of modality combinations and tasks.
We conduct a series of experiments to highlight the missing modality robustness
of our proposed method on five different multimodal tasks across seven
datasets. Our proposed method demonstrates versatility across various tasks and
datasets, and outperforms existing methods for robust multimodal learning with
missing modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI). 28 pages, 6 figures, 17 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preventing Collapse in Contrastive Learning with Orthonormal Prototypes
  (CLOP) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18699v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18699v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Manh Nguyen, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has emerged as a powerful method in deep learning,
excelling at learning effective representations through contrasting samples
from different distributions. However, neural collapse, where embeddings
converge into a lower-dimensional space, poses a significant challenge,
especially in semi-supervised and self-supervised setups. In this paper, we
first theoretically analyze the effect of large learning rates on contrastive
losses that solely rely on the cosine similarity metric, and derive a
theoretical bound to mitigate this collapse. {Building on these insights, we
propose CLOP, a novel semi-supervised loss function designed to prevent neural
collapse by promoting the formation of orthogonal linear subspaces among class
embeddings.} Unlike prior approaches that enforce a simplex ETF structure, CLOP
focuses on subspace separation, leading to more distinguishable embeddings.
Through extensive experiments on real and synthetic datasets, we demonstrate
that CLOP enhances performance, providing greater stability across different
learning rates and batch sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NoSENSE: Learned unrolled cardiac MRI reconstruction without explicit
  sensitivity maps <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15608v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15608v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Frederik Zimmermann, Andreas Kofler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel learned image reconstruction method for accelerated
cardiac MRI with multiple receiver coils based on deep convolutional neural
networks (CNNs) and algorithm unrolling. In contrast to many existing learned
MR image reconstruction techniques that necessitate coil-sensitivity map (CSM)
estimation as a distinct network component, our proposed approach avoids
explicit CSM estimation. Instead, it implicitly captures and learns to exploit
the inter-coil relationships of the images. Our method consists of a series of
novel learned image and k-space blocks with shared latent information and
adaptation to the acquisition parameters by feature-wise modulation (FiLM), as
well as coil-wise data-consistency (DC) blocks.
  Our method achieved PSNR values of 34.89 and 35.56 and SSIM values of 0.920
and 0.942 in the cine track and mapping track validation leaderboard of the
MICCAI STACOM CMRxRecon Challenge, respectively, ranking 4th among different
teams at the time of writing.
  Code will be made available at https://github.com/fzimmermann89/CMRxRecon
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MICCAI STACOM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation noising effectively prevents harmful fine-tuning on LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14577v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14577v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Domenic Rosati, Jan Wehner, Kai Williams, Łukasz Bartoszcze, David Atanasov, Robie Gonzales, Subhabrata Majumdar, Carsten Maple, Hassan Sajjad, Frank Rudzicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Releasing open-source large language models (LLMs) presents a dual-use risk
since bad actors can easily fine-tune these models for harmful purposes. Even
without the open release of weights, weight stealing and fine-tuning APIs make
closed models vulnerable to harmful fine-tuning attacks (HFAs). While safety
measures like preventing jailbreaks and improving safety guardrails are
important, such measures can easily be reversed through fine-tuning. In this
work, we propose Representation Noising (RepNoise), a defence mechanism that is
effective even when attackers have access to the weights. RepNoise works by
removing information about harmful representations such that it is difficult to
recover them during fine-tuning. Importantly, our defence is also able to
generalize across different subsets of harm that have not been seen during the
defence process as long as they are drawn from the same distribution of the
attack set. Our method does not degrade the general capability of LLMs and
retains the ability to train the model on harmless tasks. We provide empirical
evidence that the effectiveness of our defence lies in its "depth": the degree
to which information about harmful representations is removed across all layers
of the LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPs 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiable and Learnable Wireless Simulation with Geometric
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14995v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14995v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Hehn, Markus Peschl, Tribhuvanesh Orekondy, Arash Behboodi, Johann Brehmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modelling the propagation of electromagnetic wireless signals is critical for
designing modern communication systems. Wireless ray tracing simulators model
signal propagation based on the 3D geometry and other scene parameters, but
their accuracy is fundamentally limited by underlying modelling assumptions and
correctness of parameters. In this work, we introduce Wi-GATr, a
fully-learnable neural simulation surrogate designed to predict the channel
observations based on scene primitives (e.g., surface mesh, antenna position
and orientation). Recognizing the inherently geometric nature of these
primitives, Wi-GATr leverages an equivariant Geometric Algebra Transformer that
operates on a tokenizer specifically tailored for wireless simulation. We
evaluate our approach on a range of tasks (i.e., signal strength and delay
spread prediction, receiver localization, and geometry reconstruction) and find
that Wi-GATr is accurate, fast, sample-efficient, and robust to
symmetry-induced transformations. Remarkably, we find our results also
translate well to the real world: Wi-GATr demonstrates more than 35% lower
error than hybrid techniques, and 70% lower error than a calibrated wireless
tracer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Gradient Estimation of Variational Quantum Circuits with Lie
  Algebraic Symmetries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohsen Heidari, Masih Mozakka, Wojciech Szpankowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hybrid quantum-classical optimization and learning strategies are among the
most promising approaches to harnessing quantum information or gaining a
quantum advantage over classical methods. However, efficient estimation of the
gradient of the objective function in such models remains a challenge due to
several factors including the exponential dimensionality of the Hilbert spaces,
and information loss of quantum measurements. In this work, we developed an
efficient framework that makes the Hadamard test efficiently applicable to
gradient estimation for a broad range of quantum systems, an advance that had
been wanting from the outset. Under certain mild structural assumptions, the
gradient is estimated with the measurement shots that scale logarithmically
with the number of parameters and with polynomial classical and quantum time.
This is an exponential reduction in the measurement cost and polynomial speed
up in time compared to existing works. The structural assumptions are (1) the
dimension of the dynamical Lie algebra is polynomial in the number of qubits,
and (2) the observable has a bounded Hilbert-Schmidt norm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-Tuning and <span class="highlight-title">Prompt</span> Optimization: Two Great Steps that Work Better
  Together <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dilara Soylu, Christopher Potts, Omar Khattab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) systems are increasingly taking the form of
sophisticated modular pipelines, e.g., Retrieval Augmented Generation (RAG),
where each module may involve a distinct Language Model (LM) and an associated
prompt template. These compound systems often lack intermediate labels or
gradient flow to optimize each module, making their end-to-end optimization
challenging. Here we seek strategies to optimize both the module-level LM
weights and the associated prompt templates of such systems to maximize a
downstream task metric. We propose for the first time combining the weight and
prompt optimization strategies to optimize a modular LM pipeline by alternating
between the two to get the same LM to teach itself. In experiments with
multi-hop QA, mathematical reasoning, and feature-based classification using
mistral-7b, llama-2-7b, and llama-3-8b, these BetterTogether strategies
optimizing the weights and prompts of a pipeline together outperform directly
optimizing weights alone and prompts alone by up to 60% and 6%, respectively,
on average across LMs and tasks. BetterTogether optimizer is released in DSPy
at http://dspy.ai
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dr. Jekyll and Mr. Hyde: Two Faces of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03853v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03853v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Gioele Collu, Tom Janssen-Groesbeek, Stefanos Koffas, Mauro Conti, Stjepan Picek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, we have witnessed a rise in the use of Large Language Models
(LLMs), especially in applications like chatbots. Safety mechanisms are
implemented to prevent improper responses from these chatbots. In this work, we
bypass these measures for ChatGPT and Gemini by making them impersonate complex
personas with personality characteristics that are not aligned with a truthful
assistant. First, we create elaborate biographies of these personas, which we
then use in a new session with the same chatbots. Our conversations then follow
a role-play style to elicit prohibited responses. Using personas, we show that
prohibited responses are provided, making it possible to obtain unauthorized,
illegal, or harmful information in both ChatGPT and Gemini. We also introduce
several ways of activating such adversarial personas, showing that both
chatbots are vulnerable to this attack. With the same principle, we introduce
two defenses that push the model to interpret trustworthy personalities and
make it more robust against such attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Neural-Evolutionary Algorithm for Autonomous Transit Network Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07917v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07917v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Holliday, Gregory Dudek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planning a public transit network is a challenging optimization problem, but
essential in order to realize the benefits of autonomous buses. We propose a
novel algorithm for planning networks of routes for autonomous buses. We first
train a graph neural net model as a policy for constructing route networks, and
then use the policy as one of several mutation operators in a evolutionary
algorithm. We evaluate this algorithm on a standard set of benchmarks for
transit network design, and find that it outperforms the learned policy alone
by up to 20% and a plain evolutionary algorithm approach by up to 53% on
realistic benchmark instances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Copyright 2024 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works. arXiv admin note: text overlap with
  arXiv:2306.00720</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Memory-Enhanced Neural Solvers for Efficient Adaptation in Combinatorial
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16424v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16424v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Chalumeau, Refiloe Shabe, Noah De Nicola, Arnu Pretorius, Thomas D. Barrett, Nathan Grinsztajn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combinatorial Optimization is crucial to numerous real-world applications,
yet still presents challenges due to its (NP-)hard nature. Amongst existing
approaches, heuristics often offer the best trade-off between quality and
scalability, making them suitable for industrial use. While Reinforcement
Learning (RL) offers a flexible framework for designing heuristics, its
adoption over handcrafted heuristics remains incomplete within industrial
solvers. Existing learned methods still lack the ability to adapt to specific
instances and fully leverage the available computational budget. The current
best methods either rely on a collection of pre-trained policies, or on
data-inefficient fine-tuning; hence failing to fully utilize newly available
information within the constraints of the budget. In response, we present
MEMENTO, an approach that leverages memory to improve the adaptation of neural
solvers at inference time. MEMENTO enables updating the action distribution
dynamically based on the outcome of previous decisions. We validate its
effectiveness on benchmark problems, in particular Traveling Salesman and
Capacitated Vehicle Routing, demonstrating its superiority over tree-search and
policy-gradient fine-tuning; and showing it can be zero-shot combined with
diversity-based solvers. We successfully train all RL auto-regressive solvers
on large instances, and show that MEMENTO can scale and is data-efficient.
Overall, MEMENTO enables to push the state-of-the-art on 11 out of 12 evaluated
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating Guiding Information for Adaptive Collocation Point
  Sampling in PINNs <span class="chip">CCS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jose Florido, He Wang, Amirul Khan, Peter K. Jimack
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-informed neural networks (PINNs) provide a means of obtaining
approximate solutions of partial differential equations and systems through the
minimisation of an objective function which includes the evaluation of a
residual function at a set of collocation points within the domain. The quality
of a PINNs solution depends upon numerous parameters, including the number and
distribution of these collocation points. In this paper we consider a number of
strategies for selecting these points and investigate their impact on the
overall accuracy of the method. In particular, we suggest that no single
approach is likely to be "optimal" but we show how a number of important
metrics can have an impact in improving the quality of the results obtained
when using a fixed number of residual evaluations. We illustrate these
approaches through the use of two benchmark test problems: Burgers' equation
and the Allen-Cahn equation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures, 2 tables. Published in the conference
  proceedings of the International Conference on Computational Science (ICCS)
  2024. Replacement to correct a typo regarding the value of viscosity listed
  in the captions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UVIP: Model-Free Approach to Evaluate Reinforcement Learning Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.02135v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.02135v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilya Levin, Denis Belomestny, Alexey Naumov, Sergey Samsonov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Policy evaluation is an important instrument for the comparison of different
algorithms in Reinforcement Learning (RL). Yet even a precise knowledge of the
value function $V^{\pi}$ corresponding to a policy $\pi$ does not provide
reliable information on how far is the policy $\pi$ from the optimal one. We
present a novel model-free upper value iteration procedure $({\sf UVIP})$ that
allows us to estimate the suboptimality gap $V^{\star}(x) - V^{\pi}(x)$ from
above and to construct confidence intervals for $V^\star$. Our approach relies
on upper bounds to the solution of the Bellman optimality equation via
martingale approach. We provide theoretical guarantees for ${\sf UVIP}$ under
general assumptions and illustrate its performance on a number of benchmark RL
problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICOMP-2024 camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LayerKV: Optimizing Large Language Model Serving with Layer-wise KV
  Cache Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Xiong, Hao Wu, Changxu Shao, Ziqing Wang, Rui Zhang, Yuhong Guo, Junping Zhao, Ke Zhang, Zhenxuan Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The expanding context windows in large language models (LLMs) have greatly
enhanced their capabilities in various applications, but they also introduce
significant challenges in maintaining low latency, particularly in Time to
First Token (TTFT). This paper identifies that the sharp rise in TTFT as
context length increases is predominantly driven by queuing delays, which are
caused by the growing demands for GPU Key-Value (KV) cache allocation clashing
with the limited availability of KV cache blocks. To address this issue, we
propose LayerKV, a simple yet effective plug-in method that effectively reduces
TTFT without requiring additional hardware or compromising output performance,
while seamlessly integrating with existing parallelism strategies and
scheduling techniques. Specifically, LayerKV introduces layer-wise KV block
allocation, management, and offloading for fine-grained control over system
memory, coupled with an SLO-aware scheduler to optimize overall Service Level
Objectives (SLOs). Comprehensive evaluations on representative models, ranging
from 7B to 70B parameters, across various GPU configurations, demonstrate that
LayerKV improves TTFT latency up to 69x and reduces SLO violation rates by
28.7%, significantly enhancing the user experience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Convergence of Hermitian Dynamic Mode Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03192v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03192v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Boullé, Matthew J. Colbrook
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the convergence of Hermitian Dynamic Mode Decomposition (DMD) to the
spectral properties of self-adjoint Koopman operators. Hermitian DMD is a
data-driven method that approximates the Koopman operator associated with an
unknown nonlinear dynamical system, using discrete-time snapshots. This
approach preserves the self-adjointness of the operator in its
finite-dimensional approximations. \rev{We prove that, under suitably broad
conditions, the spectral measures corresponding to the eigenvalues and
eigenfunctions computed by Hermitian DMD converge to those of the underlying
Koopman operator}. This result also applies to skew-Hermitian systems (after
multiplication by $i$), applicable to generators of continuous-time
measure-preserving systems. Along the way, we establish a general theorem on
the convergence of spectral measures for finite sections of self-adjoint
operators, including those that are unbounded, which is of independent interest
to the wider spectral community. We numerically demonstrate our results by
applying them to two-dimensional Schr\"odinger equations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:2312.00137</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoding Intelligence: A Framework for Certifying Knowledge
  Comprehension in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isha Chaudhary, Vedaant V. Jain, Gagandeep Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge comprehension capability is an important aspect of human
intelligence. As Large Language Models (LLMs) are being envisioned as
superhuman agents, it is crucial for them to be proficient at knowledge
comprehension. However, existing benchmarking studies do not provide
consistent, generalizable, and formal guarantees on the knowledge comprehension
capabilities of LLMs. In this work, we propose the first framework to certify
knowledge comprehension in LLMs with formal probabilistic guarantees. Our
certificates are quantitative -- they consist of high-confidence, tight bounds
on the probability that a target LLM gives the correct answer on any knowledge
comprehension prompt sampled from a distribution. We design and certify novel
specifications that precisely represent distributions of knowledge
comprehension prompts leveraging knowledge graphs. We certify SOTA LLMs for
specifications over the Wikidata5m knowledge graph. We find that the knowledge
comprehension capability improves significantly with scaling the size of the
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transition Path Sampling with Improved Off-Policy Training of Diffusion
  Path Samplers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19961v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19961v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiyoung Seong, Seonghyun Park, Seonghwan Kim, Woo Youn Kim, Sungsoo Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding transition pathways between meta-stable states in molecular
systems is crucial to advance material design and drug discovery. However,
unbiased molecular dynamics simulations are computationally infeasible due to
the high energy barriers separating these states. Although recent machine
learning techniques offer potential solutions, they are often limited to simple
systems or rely on collective variables (CVs) derived from costly domain
expertise. In this paper, we introduce a novel approach that trains diffusion
path samplers (DPS) for transition path sampling (TPS) without the need for
CVs. We recast the problem as an amortized sampling of the target path measure,
minimizing the log-variance divergence between the path measure induced by our
DPS and the target path measure. To ensure scalability for high-dimensional
tasks, we introduce (1) a new off-policy training objective based on learning
control variates with replay buffers and (2) a scale-based equivariant
parameterization of the bias forces. We evaluate our approach, coined TPS-DPS,
on a synthetic double-well potential and three peptides: Alanine Dipeptide,
Polyproline Helix, and Chignolin. Results show that our approach produces more
realistic and diverse transition pathways compared to existing baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, 1 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of
  Large Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14768v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14768v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) need knowledge updates to meet the ever-growing
world facts and correct the hallucinated responses, facilitating the methods of
lifelong model editing. Where the updated knowledge resides in memories is a
fundamental question for model editing. In this paper, we find that editing
either long-term memory (direct model parameters) or working memory
(non-parametric knowledge of neural network activations/representations by
retrieval) will result in an impossible triangle -- reliability,
generalization, and locality can not be realized together in the lifelong
editing settings. For long-term memory, directly editing the parameters will
cause conflicts with irrelevant pretrained knowledge or previous edits (poor
reliability and locality). For working memory, retrieval-based activations can
hardly make the model understand the edits and generalize (poor
generalization). Therefore, we propose WISE to bridge the gap between memories.
In WISE, we design a dual parametric memory scheme, which consists of the main
memory for the pretrained knowledge and a side memory for the edited knowledge.
We only edit the knowledge in the side memory and train a router to decide
which memory to go through when given a query. For continual editing, we devise
a knowledge-sharding mechanism where different sets of edits reside in distinct
subspaces of parameters, and are subsequently merged into a shared memory
without conflicts. Extensive experiments show that WISE can outperform previous
model editing methods and overcome the impossible triangle under lifelong model
editing of question answering, hallucination, and out-of-distribution settings
across trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is
available at https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-MoE: Towards Compositional Large Language Models with
  Self-Specialized Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12034v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12034v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junmo Kang, Leonid Karlinsky, Hongyin Luo, Zhen Wang, Jacob Hansen, James Glass, David Cox, Rameswar Panda, Rogerio Feris, Alan Ritter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Self-MoE, an approach that transforms a monolithic LLM into a
compositional, modular system of self-specialized experts, named MiXSE (MiXture
of Self-specialized Experts). Our approach leverages self-specialization, which
constructs expert modules using self-generated synthetic data, each equipping a
shared base LLM with distinct domain-specific capabilities, activated via
self-optimized routing. This allows for dynamic and capability-specific
handling of various target tasks, enhancing overall capabilities, without
extensive human-labeled data and added parameters. Our empirical results reveal
that specializing LLMs may exhibit potential trade-offs in performances on
non-specialized tasks. On the other hand, our Self-MoE demonstrates substantial
improvements (6.5%p on average) over the base LLM across diverse benchmarks
such as knowledge, reasoning, math, and coding. It also consistently
outperforms other methods, including instance merging and weight merging, while
offering better flexibility and interpretability by design with semantic
experts and routing. Our findings highlight the critical role of modularity,
the applicability of Self-MoE to multiple base LLMs, and the potential of
self-improvement in achieving efficient, scalable, and adaptable systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Elementary Predictor Obtaining $2\sqrt{T}+1$ Distance to Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11410v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11410v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eshwar Ram Arunachaleswaran, Natalie Collina, Aaron Roth, Mirah Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blasiok et al. [2023] proposed distance to calibration as a natural measure
of calibration error that unlike expected calibration error (ECE) is
continuous. Recently, Qiao and Zheng [2024] gave a non-constructive argument
establishing the existence of an online predictor that can obtain $O(\sqrt{T})$
distance to calibration in the adversarial setting, which is known to be
impossible for ECE. They leave as an open problem finding an explicit,
efficient algorithm. We resolve this problem and give an extremely simple,
efficient, deterministic algorithm that obtains distance to calibration error
at most $2\sqrt{T}+1$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Function-Space MCMC for Bayesian Wide Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14325v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14325v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucia Pezzetti, Stefano Favaro, Stefano Peluchetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian Neural Networks represent a fascinating confluence of deep learning
and probabilistic reasoning, offering a compelling framework for understanding
uncertainty in complex predictive models. In this paper, we investigate the use
of the preconditioned Crank-Nicolson algorithm and its Langevin version to
sample from the reparametrised posterior distribution of the weights as the
widths of Bayesian Neural Networks grow larger. In addition to being robust in
the infinite-dimensional setting, we prove that the acceptance probabilities of
the proposed methods approach 1 as the width of the network increases,
independently of any stepsize tuning. Moreover, we examine and compare how the
mixing speeds of the underdamped Langevin Monte Carlo, the preconditioned
Crank-Nicolson and the preconditioned Crank-Nicolson Langevin samplers are
influenced by changes in the network width in some real-world cases. Our
findings suggest that, in wide Bayesian Neural Networks configurations, the
preconditioned Crank-Nicolson method allows for more efficient sampling of the
reparametrised posterior distribution, as evidenced by a higher effective
sample size and improved diagnostic results compared with the other analysed
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Centric Foundation Models in Computational Healthcare: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02458v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02458v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunkun Zhang, Jin Gao, Zheling Tan, Lingfeng Zhou, Kexin Ding, Mu Zhou, Shaoting Zhang, Dequan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of foundation models (FMs) as an emerging suite of AI techniques
has struck a wave of opportunities in computational healthcare. The interactive
nature of these models, guided by pre-training data and human instructions, has
ignited a data-centric AI paradigm that emphasizes better data
characterization, quality, and scale. In healthcare AI, obtaining and
processing high-quality clinical data records has been a longstanding
challenge, ranging from data quantity, annotation, patient privacy, and ethics.
In this survey, we investigate a wide range of data-centric approaches in the
FM era (from model pre-training to inference) towards improving the healthcare
workflow. We discuss key perspectives in AI security, assessment, and alignment
with human values. Finally, we offer a promising outlook of FM-based analytics
to enhance the performance of patient outcome and clinical workflow in the
evolving landscape of healthcare and medicine. We provide an up-to-date list of
healthcare-related foundation models and datasets at
https://github.com/Yunkun-Zhang/Data-Centric-FM-Healthcare .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Survey content updated to include recent research work and progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geodesic Optimization for Predictive Shift Adaptation on EEG data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03878v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03878v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Apolline Mellot, Antoine Collas, Sylvain Chevallier, Alexandre Gramfort, Denis A. Engemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electroencephalography (EEG) data is often collected from diverse contexts
involving different populations and EEG devices. This variability can induce
distribution shifts in the data $X$ and in the biomedical variables of interest
$y$, thus limiting the application of supervised machine learning (ML)
algorithms. While domain adaptation (DA) methods have been developed to
mitigate the impact of these shifts, such methods struggle when distribution
shifts occur simultaneously in $X$ and $y$. As state-of-the-art ML models for
EEG represent the data by spatial covariance matrices, which lie on the
Riemannian manifold of Symmetric Positive Definite (SPD) matrices, it is
appealing to study DA techniques operating on the SPD manifold. This paper
proposes a novel method termed Geodesic Optimization for Predictive Shift
Adaptation (GOPSA) to address test-time multi-source DA for situations in which
source domains have distinct $y$ distributions. GOPSA exploits the geodesic
structure of the Riemannian manifold to jointly learn a domain-specific
re-centering operator representing site-specific intercepts and the regression
model. We performed empirical benchmarks on the cross-site generalization of
age-prediction models with resting-state EEG data from a large multi-national
dataset (HarMNqEEG), which included $14$ recording sites and more than $1500$
human participants. Compared to state-of-the-art methods, our results showed
that GOPSA achieved significantly higher performance on three regression
metrics ($R^2$, MAE, and Spearman's $\rho$) for several source-target site
combinations, highlighting its effectiveness in tackling multi-source DA with
predictive shifts in EEG data analysis. Our method has the potential to combine
the advantages of mixed-effects modeling with machine learning for biomedical
applications of EEG, such as multicenter clinical trials.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLAME: Adaptive and Reactive Concept Drift Mitigation for Federated
  Learning Deployments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01386v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01386v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioannis Mavromatis, Stefano De Feo, Aftab Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Federated Learning with Adaptive Monitoring and
Elimination (FLAME), a novel solution capable of detecting and mitigating
concept drift in Federated Learning (FL) Internet of Things (IoT) environments.
Concept drift poses significant challenges for FL models deployed in dynamic
and real-world settings. FLAME leverages an FL architecture, considers a
real-world FL pipeline, and proves capable of maintaining model performance and
accuracy while addressing bandwidth and privacy constraints. Introducing
various features and extensions on previous works, FLAME offers a robust
solution to concept drift, significantly reducing computational load and
communication overhead. Compared to well-known lightweight mitigation methods,
FLAME demonstrates superior performance in maintaining high F1 scores and
reducing resource utilisation in large-scale IoT deployments, making it a
promising approach for real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for Publication at ACM EWSN 2024 - EMERGE Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Long Range Dependencies on Graphs via Random Walks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03386v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03386v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dexiong Chen, Till Hendrik Schulz, Karsten Borgwardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Message-passing graph neural networks (GNNs) excel at capturing local
relationships but struggle with long-range dependencies in graphs. In contrast,
graph transformers (GTs) enable global information exchange but often
oversimplify the graph structure by representing graphs as sets of fixed-length
vectors. This work introduces a novel architecture that overcomes the
shortcomings of both approaches by combining the long-range information of
random walks with local message passing. By treating random walks as sequences,
our architecture leverages recent advances in sequence models to effectively
capture long-range dependencies within these walks. Based on this concept, we
propose a framework that offers (1) more expressive graph representations
through random walk sequences, (2) the ability to utilize any sequence model
for capturing long-range dependencies, and (3) the flexibility by integrating
various GNN and GT architectures. Our experimental evaluations demonstrate that
our approach achieves significant performance improvements on 19 graph and node
benchmark datasets, notably outperforming existing methods by up to 13\% on the
PascalVoc-SP and COCO-SP datasets. The code is available at
https://github.com/BorgwardtLab/NeuralWalker.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Pricing in Securities Lending Market: Application in Revenue
  Optimization for an Agent Lender Portfolio 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13687v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13687v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Xu, Yung-Cheng Hsu, William Biscarri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Securities lending is an important part of the financial market structure,
where agent lenders help long term institutional investors to lend out their
securities to short sellers in exchange for a lending fee. Agent lenders within
the market seek to optimize revenue by lending out securities at the highest
rate possible. Typically, this rate is set by hard-coded business rules or
standard supervised machine learning models. These approaches are often
difficult to scale and are not adaptive to changing market conditions. Unlike a
traditional stock exchange with a centralized limit order book, the securities
lending market is organized similarly to an e-commerce marketplace, where agent
lenders and borrowers can transact at any agreed price in a bilateral fashion.
This similarity suggests that the use of typical methods for addressing dynamic
pricing problems in e-commerce could be effective in the securities lending
market. We show that existing contextual bandit frameworks can be successfully
utilized in the securities lending market. Using offline evaluation on real
historical data, we show that the contextual bandit approach can consistently
outperform typical approaches by at least 15% in terms of total revenue
generated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I Bet You Did Not Mean That: Testing Semantic Importance via Betting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19146v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19146v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacopo Teneggi, Jeremias Sulam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have extended notions of feature importance to semantic concepts
that are inherently interpretable to the users interacting with a black-box
predictive model. Yet, precise statistical guarantees, such as false positive
rate and false discovery rate control, are needed to communicate findings
transparently and to avoid unintended consequences in real-world scenarios. In
this paper, we formalize the global (i.e., over a population) and local (i.e.,
for a sample) statistical importance of semantic concepts for the predictions
of opaque models by means of conditional independence, which allows for
rigorous testing. We use recent ideas of sequential kernelized independence
testing (SKIT) to induce a rank of importance across concepts, and showcase the
effectiveness and flexibility of our framework on synthetic datasets as well as
on image classification tasks using several and diverse vision-language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Can <span class="highlight-title">Transformer</span>s Count to n? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15160v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15160v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gilad Yehudai, Haim Kaplan, Asma Ghandeharioun, Mor Geva, Amir Globerson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models based on the transformer architectures can solve highly
complex tasks. But are there simple tasks that such models cannot solve? Here
we focus on very simple counting tasks, that involve counting how many times a
token in the vocabulary have appeared in a string. We show that if the
dimension of the transformer state is linear in the context length, this task
can be solved. However, the solution we propose does not scale beyond this
limit, and we provide theoretical arguments for why it is likely impossible for
a size limited transformer to implement this task. Our empirical results
demonstrate the same phase-transition in performance, as anticipated by the
theoretical argument. Our results demonstrate the importance of understanding
how transformers can solve simple tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ softmax is not enough (for sharp out-of-distribution) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01104v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01104v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Petar Veličković, Christos Perivolaropoulos, Federico Barbero, Razvan Pascanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key property of reasoning systems is the ability to make sharp decisions on
their input data. For contemporary AI systems, a key carrier of sharp behaviour
is the softmax function, with its capability to perform differentiable
query-key lookups. It is a common belief that the predictive power of networks
leveraging softmax arises from "circuits" which sharply perform certain kinds
of computations consistently across many diverse inputs. However, for these
circuits to be robust, they would need to generalise well to arbitrary valid
inputs. In this paper, we dispel this myth: even for tasks as simple as finding
the maximum key, any learned circuitry must disperse as the number of items
grows at test time. We attribute this to a fundamental limitation of the
softmax function to robustly approximate sharp functions, prove this phenomenon
theoretically, and propose adaptive temperature as an ad-hoc technique for
improving the sharpness of softmax at inference time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Comments welcome. 15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Bits and Bandits: Quantifying the Regret-Information Trade-off 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16581v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16581v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Itai Shufaro, Nadav Merlis, Nir Weinberger, Shie Mannor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many sequential decision problems, an agent performs a repeated task. He
then suffers regret and obtains information that he may use in the following
rounds. However, sometimes the agent may also obtain information and avoid
suffering regret by querying external sources. We study the trade-off between
the information an agent accumulates and the regret it suffers. We invoke
information-theoretic methods for obtaining regret lower bounds, that also
allow us to easily re-derive several known lower bounds. We introduce the first
Bayesian regret lower bounds that depend on the information an agent
accumulates. We also prove regret upper bounds using the amount of information
the agent accumulates. These bounds show that information measured in bits, can
be traded off for regret, measured in reward. Finally, we demonstrate the
utility of these bounds in improving the performance of a question-answering
task with large language models, allowing us to obtain valuable insights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Collaborative Filtering to Detect Anomalies in Human Semantic
  Trajectories <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueyang Liu, Lance Kennedy, Hossein Amiri, Andreas Züfle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human trajectory anomaly detection has become increasingly important across a
wide range of applications, including security surveillance and public health.
However, existing trajectory anomaly detection methods are primarily focused on
vehicle-level traffic, while human-level trajectory anomaly detection remains
under-explored. Since human trajectory data is often very sparse, machine
learning methods have become the preferred approach for identifying complex
patterns. However, concerns regarding potential biases and the robustness of
these models have intensified the demand for more transparent and explainable
alternatives. In response to these challenges, our research focuses on
developing a lightweight anomaly detection model specifically designed to
detect anomalies in human trajectories. We propose a Neural Collaborative
Filtering approach to model and predict normal mobility. Our method is designed
to model users' daily patterns of life without requiring prior knowledge,
thereby enhancing performance in scenarios where data is sparse or incomplete,
such as in cold start situations. Our algorithm consists of two main modules.
The first is the collaborative filtering module, which applies collaborative
filtering to model normal mobility of individual humans to places of interest.
The second is the neural module, responsible for interpreting the complex
spatio-temporal relationships inherent in human trajectory data. To validate
our approach, we conducted extensive experiments using simulated and real-world
datasets comparing to numerous state-of-the-art trajectory anomaly detection
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the 1st ACM SIGSPATIAL International
  Workshop on Geospatial Anomaly Detection (GeoAnomalies'24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectrum Extraction and Clipping for Implicitly Linear Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16017v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16017v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Ebrahimpour Boroojeny, Matus Telgarsky, Hari Sundaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show the effectiveness of automatic differentiation in efficiently and
correctly computing and controlling the spectrum of implicitly linear
operators, a rich family of layer types including all standard convolutional
and dense layers. We provide the first clipping method which is correct for
general convolution layers, and illuminate the representational limitation that
caused correctness issues in prior work. We study the effect of the batch
normalization layers when concatenated with convolutional layers and show how
our clipping method can be applied to their composition. By comparing the
accuracy and performance of our algorithms to the state-of-the-art methods,
using various experiments, we show they are more precise and efficient and lead
to better generalization and adversarial robustness. We provide the code for
using our methods at https://github.com/Ali-E/FastClip.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flat Posterior Does Matter For Bayesian Model Averaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15664v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15664v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungjun Lim, Jeyoon Yeom, Sooyon Kim, Hoyoon Byun, Jinho Kang, Yohan Jung, Jiyoung Jung, Kyungwoo Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian neural network (BNN) approximates the posterior distribution of
model parameters and utilizes the posterior for prediction via Bayesian Model
Averaging (BMA). The quality of the posterior approximation is critical for
achieving accurate and robust predictions. It is known that flatness in the
loss landscape is strongly associated with generalization performance, and it
necessitates consideration to improve the quality of the posterior
approximation. In this work, we empirically demonstrate that BNNs often
struggle to capture the flatness. Moreover, we provide both experimental and
theoretical evidence showing that BMA can be ineffective without ensuring
flatness. To address this, we propose Sharpness-Aware Bayesian Model Averaging
(SA-BMA), a novel optimizer that seeks flat posteriors by calculating
divergence in the parameter space. SA-BMA aligns with the intrinsic nature of
BNN and the generalized version of existing sharpness-aware optimizers for DNN.
In addition, we suggest a Bayesian Transfer Learning scheme to efficiently
leverage pre-trained DNN. We validate the efficacy of SA-BMA in enhancing
generalization performance in few-shot classification and distribution shift by
ensuring flat posterior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TabDDPM: Modelling Tabular Data with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.15421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.15421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, Artem Babenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising diffusion probabilistic models are currently becoming the leading
paradigm of generative modeling for many important data modalities. Being the
most prevalent in the computer vision community, diffusion models have also
recently gained some attention in other domains, including speech, NLP, and
graph-like data. In this work, we investigate if the framework of diffusion
models can be advantageous for general tabular problems, where datapoints are
typically represented by vectors of heterogeneous features. The inherent
heterogeneity of tabular data makes it quite challenging for accurate modeling,
since the individual features can be of completely different nature, i.e., some
of them can be continuous and some of them can be discrete. To address such
data types, we introduce TabDDPM -- a diffusion model that can be universally
applied to any tabular dataset and handles any type of feature. We extensively
evaluate TabDDPM on a wide set of benchmarks and demonstrate its superiority
over existing GAN/VAE alternatives, which is consistent with the advantage of
diffusion models in other fields. Additionally, we show that TabDDPM is
eligible for privacy-oriented setups, where the original datapoints cannot be
publicly shared.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>code https://github.com/yandex-research/tab-ddpm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reconstruct Your Previous Conversations! Comprehensively Investigating
  Privacy Leakage Risks in Conversations with <span class="highlight-title">GPT</span> Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02987v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02987v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Chu, Zeyang Sha, Michael Backes, Yang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant advancements have recently been made in large language models
represented by GPT models. Users frequently have multi-round private
conversations with cloud-hosted GPT models for task optimization. Yet, this
operational paradigm introduces additional attack surfaces, particularly in
custom GPTs and hijacked chat sessions. In this paper, we introduce a
straightforward yet potent Conversation Reconstruction Attack. This attack
targets the contents of previous conversations between GPT models and benign
users, i.e., the benign users' input contents during their interaction with GPT
models. The adversary could induce GPT models to leak such contents by querying
them with designed malicious prompts. Our comprehensive examination of privacy
risks during the interactions with GPT models under this attack reveals GPT-4's
considerable resilience. We present two advanced attacks targeting improved
reconstruction of past conversations, demonstrating significant privacy leakage
across all models under these advanced techniques. Evaluating various defense
mechanisms, we find them ineffective against these attacks. Our findings
highlight the ease with which privacy can be compromised in interactions with
GPT models, urging the community to safeguard against potential abuses of these
models' capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in EMNLP 2024. 14 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Question Decomposition on Multimodal Large Language Models <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19339v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19339v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Zhang, Jianzhe Liu, Zhen Han, Shuo Chen, Bailan He, Volker Tresp, Zhiqiang Xu, Jindong Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question decomposition has emerged as an effective strategy for prompting
Large Language Models (LLMs) to answer complex questions. However, while
existing methods primarily focus on unimodal language models, the question
decomposition capability of Multimodal Large Language Models (MLLMs) has yet to
be explored. To this end, this paper explores visual question decomposition on
MLLMs. Specifically, we introduce a systematic evaluation framework including a
dataset and several evaluation criteria to assess the quality of the decomposed
sub-questions, revealing that existing MLLMs struggle to produce high-quality
sub-questions. To address this limitation, we propose a specific finetuning
dataset, DecoVQA+, for enhancing the model's question decomposition capability.
Aiming at enabling models to perform appropriate selective decomposition, we
propose an efficient finetuning pipeline. The finetuning pipeline consists of
our proposed dataset and a training objective for selective decomposition.
Finetuned MLLMs demonstrate significant improvements in the quality of
sub-questions and the policy of selective question decomposition. Additionally,
the models also achieve higher accuracy with selective decomposition on VQA
benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty-Aware Decision <span class="highlight-title">Transformer</span> for Stochastic Driving
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16397v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16397v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zenan Li, Fan Nie, Qiao Sun, Fang Da, Hang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline Reinforcement Learning (RL) enables policy learning without active
interactions, making it especially appealing for self-driving tasks. Recent
successes of Transformers inspire casting offline RL as sequence modeling,
which, however, fails in stochastic environments with incorrect assumptions
that identical actions can consistently achieve the same goal. In this paper,
we introduce an UNcertainty-awaRE deciSion Transformer (UNREST) for planning in
stochastic driving environments without introducing additional transition or
complex generative models. Specifically, UNREST estimates uncertainties by
conditional mutual information between transitions and returns. Discovering
'uncertainty accumulation' and 'temporal locality' properties of driving
environments, we replace the global returns in decision transformers with
truncated returns less affected by environments to learn from actual outcomes
of actions rather than environment transitions. We also dynamically evaluate
uncertainty at inference for cautious planning. Extensive experiments
demonstrate UNREST's superior performance in various driving scenarios and the
power of our uncertainty estimation strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Residual Stream Analysis with Multi-Layer SAEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Lawson, Lucy Farnik, Conor Houghton, Laurence Aitchison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse autoencoders (SAEs) are a promising approach to interpreting the
internal representations of transformer language models. However, SAEs are
usually trained separately on each transformer layer, making it difficult to
use them to study how information flows across layers. To solve this problem,
we introduce the multi-layer SAE (MLSAE): a single SAE trained on the residual
stream activation vectors from every transformer layer. Given that the residual
stream is understood to preserve information across layers, we expected MLSAE
latents to `switch on' at a token position and remain active at later layers.
Interestingly, we find that individual latents are often active at a single
layer for a given token or prompt, but this layer may differ for different
tokens or prompts. We quantify these phenomena by defining a distribution over
layers and considering its variance. We find that the variance of the
distributions of latent activations over layers is about two orders of
magnitude greater when aggregating over tokens compared with a single token.
For larger underlying models, the degree to which latents are active at
multiple layers increases, which is consistent with the fact that the residual
stream activation vectors at adjacent layers become more similar. Finally, we
relax the assumption that the residual stream basis is the same at every layer
by applying pre-trained tuned-lens transformations, but our findings remain
qualitatively similar. Our results represent a new approach to understanding
how representations change as they flow through transformers. We release our
code to train and analyze MLSAEs at https://github.com/tim-lawson/mlsae.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 26 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine listening in a neonatal intensive care unit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11439v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11439v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Modan Tailleur, Vincent Lostanlen, Jean-Philippe Rivière, Pierre Aumond
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Oxygenators, alarm devices, and footsteps are some of the most common sound
sources in a hospital. Detecting them has scientific value for environmental
psychology but comes with challenges of its own: namely, privacy preservation
and limited labeled data. In this paper, we address these two challenges via a
combination of edge computing and cloud computing. For privacy preservation, we
have designed an acoustic sensor which computes third-octave spectrograms on
the fly instead of recording audio waveforms. For sample-efficient machine
learning, we have repurposed a pretrained audio neural network (PANN) via
spectral transcoding and label space adaptation. A small-scale study in a
neonatological intensive care unit (NICU) confirms that the time series of
detected events align with another modality of measurement: i.e., electronic
badges for parents and healthcare professionals. Hence, this paper demonstrates
the feasibility of polyphonic machine listening in a hospital ward while
guaranteeing privacy by design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient, Multimodal, and Derivative-Free Bayesian Inference With
  Fisher-Rao Gradient Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17263v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17263v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Chen, Daniel Zhengyu Huang, Jiaoyang Huang, Sebastian Reich, Andrew M. Stuart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study efficient approximate sampling for probability
distributions known up to normalization constants. We specifically focus on a
problem class arising in Bayesian inference for large-scale inverse problems in
science and engineering applications. The computational challenges we address
with the proposed methodology are: (i) the need for repeated evaluations of
expensive forward models; (ii) the potential existence of multiple modes; and
(iii) the fact that gradient of, or adjoint solver for, the forward model might
not be feasible.
  While existing Bayesian inference methods meet some of these challenges
individually, we propose a framework that tackles all three systematically. Our
approach builds upon the Fisher-Rao gradient flow in probability space,
yielding a dynamical system for probability densities that converges towards
the target distribution at a uniform exponential rate. This rapid convergence
is advantageous for the computational burden outlined in (i). We apply Gaussian
mixture approximations with operator splitting techniques to simulate the flow
numerically; the resulting approximation can capture multiple modes thus
addressing (ii). Furthermore, we employ the Kalman methodology to facilitate a
derivative-free update of these Gaussian components and their respective
weights, addressing the issue in (iii).
  The proposed methodology results in an efficient derivative-free sampler
flexible enough to handle multi-modal distributions: Gaussian Mixture Kalman
Inversion (GMKI). The effectiveness of GMKI is demonstrated both theoretically
and numerically in several experiments with multimodal target distributions,
including proof-of-concept and two-dimensional examples, as well as a
large-scale application: recovering the Navier-Stokes initial condition from
solution data at positive times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An active learning method for solving competitive multi-agent
  decision-making and control problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.12561v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.12561v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippo Fabiani, Alberto Bemporad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To identify a stationary action profile for a population of competitive
agents, each executing private strategies, we introduce a novel active-learning
scheme where a centralized external observer (or entity) can probe the agents'
reactions and recursively update simple local parametric estimates of the
action-reaction mappings. Under very general working assumptions (not even
assuming that a stationary profile exists), sufficient conditions are
established to assess the asymptotic properties of the proposed active learning
methodology so that, if the parameters characterizing the action-reaction
mappings converge, a stationary action profile is achieved. Such conditions
hence act also as certificates for the existence of such a profile. Extensive
numerical simulations involving typical competitive multi-agent control and
decision-making problems illustrate the practical effectiveness of the proposed
learning-based approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Python package available at https://github.com/bemporad/gnep-learn</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Consistency Trajectory Models for Image Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12510v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12510v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beomsu Kim, Jaemin Kim, Jeongsol Kim, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) excel in unconditional generation, as well as on
applications such as image editing and restoration. The success of DMs lies in
the iterative nature of diffusion: diffusion breaks down the complex process of
mapping noise to data into a sequence of simple denoising tasks. Moreover, we
are able to exert fine-grained control over the generation process by injecting
guidance terms into each denoising step. However, the iterative process is also
computationally intensive, often taking from tens up to thousands of function
evaluations. Although consistency trajectory models (CTMs) enable traversal
between any time points along the probability flow ODE (PFODE) and score
inference with a single function evaluation, CTMs only allow translation from
Gaussian noise to data. This work aims to unlock the full potential of CTMs by
proposing generalized CTMs (GCTMs), which translate between arbitrary
distributions via ODEs. We discuss the design space of GCTMs and demonstrate
their efficacy in various image manipulation tasks such as image-to-image
translation, restoration, and editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-agent reinforcement learning using echo-state network and its
  application to pedestrian dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11834v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11834v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hisato Komatsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, simulations of pedestrians using the multi-agent
reinforcement learning (MARL) have been studied. This study considered the
roads on a grid-world environment, and implemented pedestrians as MARL agents
using an echo-state network and the least squares policy iteration method.
Under this environment, the ability of these agents to learn to move forward by
avoiding other agents was investigated. Specifically, we considered two types
of tasks: the choice between a narrow direct route and a broad detour, and the
bidirectional pedestrian flow in a corridor. The simulations results indicated
that the learning was successful when the density of the agents was not that
high.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Contrastive Feature Representations for Facial Action Unit
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06165v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06165v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiao Shang, Bin Liu, Fengmao Lv, Fei Teng, Tianrui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial action unit (AU) detection has long encountered the challenge of
detecting subtle feature differences when AUs activate. Existing methods often
rely on encoding pixel-level information of AUs, which not only encodes
additional redundant information but also leads to increased model complexity
and limited generalizability. Additionally, the accuracy of AU detection is
negatively impacted by the class imbalance issue of each AU type, and the
presence of noisy and false AU labels. In this paper, we introduce a novel
contrastive learning framework aimed for AU detection that incorporates both
self-supervised and supervised signals, thereby enhancing the learning of
discriminative features for accurate AU detection. To tackle the class
imbalance issue, we employ a negative sample re-weighting strategy that adjusts
the step size of updating parameters for minority and majority class samples.
Moreover, to address the challenges posed by noisy and false AU labels, we
employ a sampling technique that encompasses three distinct types of positive
sample pairs. This enables us to inject self-supervised signals into the
supervised signal, effectively mitigating the adverse effects of noisy labels.
Our experimental assessments, conducted on four widely-utilized benchmark
datasets (BP4D, DISFA, GFT and Aff-Wild2), underscore the superior performance
of our approach compared to state-of-the-art methods of AU detection. Our code
is available at \url{https://github.com/Ziqiao-Shang/AUNCE}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 17 figures, submitted to IEEE Transactions on Circuits and
  Systems for Video Technology (TCSVT)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QMP: Q-switch Mixture of Policies for Multi-Task Behavior Sharing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00671v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00671v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grace Zhang, Ayush Jain, Injune Hwang, Shao-Hua Sun, Joseph J. Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task reinforcement learning (MTRL) aims to learn several tasks
simultaneously for better sample efficiency than learning them separately.
Traditional methods achieve this by sharing parameters or relabeled data
between tasks. In this work, we introduce a new framework for sharing
behavioral policies across tasks, which can be used in addition to existing
MTRL methods. The key idea is to improve each task's off-policy data collection
by employing behaviors from other task policies. Selectively sharing helpful
behaviors acquired in one task to collect training data for another task can
lead to higher-quality trajectories, leading to more sample-efficient MTRL.
Thus, we introduce a simple and principled framework called Q-switch mixture of
policies (QMP) that selectively shares behavior between different task policies
by using the task's Q-function to evaluate and select useful shareable
behaviors. We theoretically analyze how QMP improves the sample efficiency of
the underlying RL algorithm. Our experiments show that QMP's behavioral policy
sharing provides complementary gains over many popular MTRL algorithms and
outperforms alternative ways to share behaviors in various manipulation,
locomotion, and navigation environments. Videos are available at
https://qmp-mtrl.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DALL-M: Context-Aware Clinical Data Augmentation with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08227v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08227v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chihcheng Hsieh, Catarina Moreira, Isabel Blanco Nobre, Sandra Costa Sousa, Chun Ouyang, Margot Brereton, Joaquim Jorge, Jacinto C. Nascimento
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  X-ray images are vital in medical diagnostics, but their effectiveness is
limited without clinical context. Radiologists often find chest X-rays
insufficient for diagnosing underlying diseases, necessitating comprehensive
clinical features and data integration. We present a novel framework to enhance
the clinical context through augmentation techniques with clinical tabular
data, thereby improving its applicability and reliability in AI medical
diagnostics. We introduce a pioneering approach to clinical data augmentation
that employs large language models to generate patient contextual synthetic
data. This methodology is crucial for training more robust deep learning models
in healthcare. It preserves the integrity of real patient data while enriching
the dataset with contextually relevant synthetic features, significantly
enhancing model performance. Our methodology, termed DALL-M, uses a three-phase
feature generation process: (i)clinical context storage, (ii)expert query
generation, and (iii)context-aware feature augmentation. DALL-M generates new,
clinically relevant features by synthesizing chest X-ray images and reports.
Applied to 799 cases using nine features from the MIMIC-IV dataset, it created
an augmented set of 91 features. This is the first work to generate contextual
values for patients' X-ray reports. Specifically, we provide (i)the capacity of
LLMs to generate contextual synthetic values for existing clinical features and
(ii)their ability to create entirely new clinically relevant features.
Empirical validation with machine learning models showed significant
performance improvements. Incorporating augmented features increased the F1
score by 16.5% and Precision and Recall by approximately 25%. DALL-M addresses
a critical gap in clinical data augmentation, offering a robust framework for
generating contextually enriched datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>we introduce a pioneering approach to clinical data augmentation that
  employs large language models (LLMs) to generate patient contextual synthetic
  data. It preserves the integrity of real patient data while enriching the
  dataset with contextually relevant synthetic features, significantly
  enhancing model performance</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Classification of All Blood Cell Images using ML and DL Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.06300v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.06300v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rabia Asghar, Sanjay Kumar, Paul Hynds, Abeera Mahfooz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human blood primarily comprises plasma, red blood cells, white blood cells,
and platelets. It plays a vital role in transporting nutrients to different
organs, where it stores essential health-related data about the human body.
Blood cells are utilized to defend the body against diverse infections,
including fungi, viruses, and bacteria. Hence, blood analysis can help
physicians assess an individual's physiological condition. Blood cells have
been sub-classified into eight groups: Neutrophils, eosinophils, basophils,
lymphocytes, monocytes, immature granulocytes (promyelocytes, myelocytes, and
metamyelocytes), erythroblasts, and platelets or thrombocytes on the basis of
their nucleus, shape, and cytoplasm. Traditionally, pathologists and
hematologists in laboratories have examined these blood cells using a
microscope before manually classifying them. The manual approach is slower and
more prone to human error. Therefore, it is essential to automate this process.
In our paper, transfer learning with CNN pre-trained models. VGG16, VGG19,
ResNet-50, ResNet-101, ResNet-152, InceptionV3, MobileNetV2, and DenseNet-20
applied to the PBC dataset's normal DIB. The overall accuracy achieved with
these models lies between 91.375 and 94.72%. Hence, inspired by these
pre-trained architectures, a model has been proposed to automatically classify
the ten types of blood cells with increased accuracy. A novel CNN-based
framework has been presented to improve accuracy. The proposed CNN model has
been tested on the PBC dataset normal DIB. The outcomes of the experiments
demonstrate that our CNN-based framework designed for blood cell classification
attains an accuracy of 99.91% on the PBC dataset. Our proposed convolutional
neural network model performs competitively when compared to earlier results
reported in the literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AQMLator -- An Auto Quantum Machine Learning E-Platform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18338v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18338v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomasz Rybotycki, Piotr Gawron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A successful Machine Learning (ML) model implementation requires three main
components: training dataset, suitable model architecture and training
procedure. Given dataset and task, finding an appropriate model might be
challenging. AutoML, a branch of ML, focuses on automatic architecture search
-- a meta method that aims at moving human from ML system design process. The
success of ML and the development of quantum computing (QC) in recent years led
to a birth of new fascinating field called Quantum Machine Learning (QML) that,
amongst others, incorporates quantum computers into ML models. In this paper we
present AQMLator, an Auto Quantum Machine Learning platform that aims to
automatically propose and train the quantum layers of an ML model with minimal
input from the user. This way, data scientists can bypass the entry barrier for
QC and use QML. AQMLator uses standard ML libraries, making it easy to
introduce into existing ML pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 3 figures, links to software in the text</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OptiBench Meets ReSocratic: Measure and Improve LLMs for Optimization
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09887v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09887v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicheng Yang, Yiwei Wang, Yinya Huang, Zhijiang Guo, Wei Shi, Xiongwei Han, Liang Feng, Linqi Song, Xiaodan Liang, Jing Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have exhibited their problem-solving abilities
in mathematical reasoning. Solving realistic optimization (OPT) problems in
application scenarios requires advanced and applied mathematics ability.
However, current OPT benchmarks that merely solve linear programming are far
from complex realistic situations. In this work, we propose OptiBench, a
benchmark for End-to-end optimization problem-solving with human-readable
inputs and outputs. OptiBench contains rich optimization problems, including
linear and nonlinear programming with or without tabular data, which can
comprehensively evaluate LLMs' solving ability. In our benchmark, LLMs are
required to call a code solver to provide precise numerical answers.
Furthermore, to alleviate the data scarcity for optimization problems, and to
bridge the gap between open-source LLMs on a small scale (e.g., Llama-3-8b) and
closed-source LLMs (e.g., GPT-4), we further propose a data synthesis method
namely ReSocratic. Unlike general data synthesis methods that proceed from
questions to answers, \ReSocratic first incrementally synthesizes formatted
optimization demonstration with mathematical formulations step by step and then
back-translates the generated demonstrations into questions. Based on this, we
synthesize the ReSocratic-29k dataset. We further conduct supervised
fine-tuning with ReSocratic-29k on multiple open-source models. Experimental
results show that ReSocratic-29k significantly improves the performance of
open-source models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AlignedCoT: <span class="highlight-title">Prompt</span>ing Large Language Models via Native-Speaking
  Demonstrations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13538v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13538v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicheng Yang, Yinya Huang, Jing Xiong, Liang Feng, Xiaodan Liang, Yiwei Wang, Jing Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models prompting, such as using in-context demonstrations, is
a mainstream technique for invoking LLMs to perform high-performance and solid
complex reasoning (e.g., mathematical reasoning, commonsense reasoning), and
has the potential for further human-machine collaborative scientific findings.
However, current LLMs are delicate and elusive in prompt words and styles. And
there is an unseen gap between LLM understanding and human-written prompts.
This paper introduces Alignedcot, an LLM-acquainted prompting technique that
includes proficient ``native-speaking'' in in-context learning for the LLMs.
Specifically, it achieves consistent and correct step-wise prompts in zero-shot
scenarios by progressively probing, refining, and formatting the LLM chain of
thoughts so that free from handcrafted few-shot demonstrations while
maintaining the prompt quality. We conduct experiments on mathematical
reasoning and commonsense reasoning. We find that LLMs with Alignedcot perform
significantly superior to them with human-crafted demonstrations. We further
apply Alignedcot for rewriting the GSM8K training set, resulting in a
GSM8K-Align dataset. We observe its benefits for retrieval augmented
generation. The code and data can be found at
https://github.com/yangzhch6/AlignedCoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning an Actionable Discrete Diffusion Policy via Large-Scale
  Actionless Video <span class="highlight-title">Pre-Train</span>ing <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14407v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14407v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran He, Chenjia Bai, Ling Pan, Weinan Zhang, Bin Zhao, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning a generalist embodied agent capable of completing multiple tasks
poses challenges, primarily stemming from the scarcity of action-labeled
robotic datasets. In contrast, a vast amount of human videos exist, capturing
intricate tasks and interactions with the physical world. Promising prospects
arise for utilizing actionless human videos for pre-training and transferring
the knowledge to facilitate robot policy learning through limited robot
demonstrations. However, it remains a challenge due to the domain gap between
humans and robots. Moreover, it is difficult to extract useful information
representing the dynamic world from human videos, because of its noisy and
multimodal data structure. In this paper, we introduce a novel framework to
tackle these challenges, which leverages a unified discrete diffusion to
combine generative pre-training on human videos and policy fine-tuning on a
small number of action-labeled robot videos. We start by compressing both human
and robot videos into unified video tokens. In the pre-training stage, we
employ a discrete diffusion model with a mask-and-replace diffusion strategy to
predict future video tokens in the latent space. In the fine-tuning stage, we
harness the imagined future videos to guide low-level action learning with a
limited set of robot data. Experiments demonstrate that our method generates
high-fidelity future videos for planning and enhances the fine-tuned policies
compared to previous state-of-the-art approaches with superior performance. Our
project website is available at https://video-diff.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. 24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Large Language Models Understand Symbolic Graphics Programs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08313v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08313v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Z. Xiao, Katherine M. Collins, Joshua B. Tenenbaum, Adrian Weller, Michael J. Black, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Against the backdrop of enthusiasm for large language models (LLMs), there is
an urgent need to scientifically assess their capabilities and shortcomings.
This is nontrivial in part because it is difficult to find tasks which the
models have not encountered during training. Utilizing symbolic graphics
programs, we propose a domain well-suited to test multiple spatial-semantic
reasoning skills of LLMs. Popular in computer graphics, these programs
procedurally generate visual data. While LLMs exhibit impressive skills in
general program synthesis and analysis, symbolic graphics programs offer a new
layer of evaluation: they allow us to test an LLM's ability to answer
different-grained semantic-level questions of the images or 3D geometries
without a vision encoder. To semantically understand the symbolic programs,
LLMs would need to possess the ability to "imagine" and reason how the
corresponding graphics content would look with only the symbolic description.
We use this task to evaluate LLMs by creating a large benchmark for the
semantic visual understanding of symbolic graphics programs, built procedurally
with minimal human effort. Particular emphasis is placed on transformations of
images that leave the image level semantics invariant while introducing
significant changes to the underlying program. We evaluate commercial and
open-source LLMs on our benchmark to assess their ability to reason about
visual output of programs, finding that LLMs considered stronger at reasoning
generally perform better. Lastly, we introduce a novel method to improve this
ability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned
with pre-collected instruction data on symbolic graphics programs.
Interestingly, we find that SIT not only improves LLM's understanding on
symbolic programs, but it also improves general reasoning ability on various
other benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report v2 (46 pages, 24 figures, project page:
  https://sgp-bench.github.io/, substantial update from v1)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cost Estimation in Unit Commitment Problems Using Simulation-Based
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03588v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03588v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Pirlet, Adrien Bolland, Gilles Louppe, Damien Ernst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Unit Commitment (UC) problem is a key optimization task in power systems
to forecast the generation schedules of power units over a finite time period
by minimizing costs while meeting demand and technical constraints. However,
many parameters required by the UC problem are unknown, such as the costs. In
this work, we estimate these unknown costs using simulation-based inference on
an illustrative UC problem, which provides an approximated posterior
distribution of the parameters given observed generation schedules and demands.
Our results highlight that the learned posterior distribution effectively
captures the underlying distribution of the data, providing a range of possible
values for the unknown parameters given a past observation. This posterior
allows for the estimation of past costs using observed past generation
schedules, enabling operators to better forecast future costs and make more
robust generation scheduling forecasts. We present avenues for future research
to address overconfidence in posterior estimation, enhance the scalability of
the methodology and apply it to more complex UC problems modeling the network
constraints and renewable energy sources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Dynamic Model of Performative Human-ML Collaboration: Theory and
  Empirical Evidence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13753v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13753v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Sühr, Samira Samadi, Chiara Farronato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) models are increasingly used in various applications,
from recommendation systems in e-commerce to diagnosis prediction in
healthcare. In this paper, we present a novel dynamic framework for thinking
about the deployment of ML models in a performative, human-ML collaborative
system. In our framework, the introduction of ML recommendations changes the
data-generating process of human decisions, which are only a proxy to the
ground truth and which are then used to train future versions of the model. We
show that this dynamic process in principle can converge to different stable
points, i.e. where the ML model and the Human+ML system have the same
performance. Some of these stable points are suboptimal with respect to the
actual ground truth. As a proof of concept, we conduct an empirical user study
with 1,408 participants. In the study, humans solve instances of the knapsack
problem with the help of machine learning predictions of varying performance.
This is an ideal setting because we can identify the actual ground truth, and
evaluate the performance of human decisions supported by ML recommendations. We
find that for many levels of ML performance, humans can improve upon the ML
predictions. We also find that the improvement could be even higher if humans
rationally followed the ML recommendations. Finally, we test whether monetary
incentives can increase the quality of human decisions, but we fail to find any
positive effect. Using our empirical data to approximate our collaborative
system suggests that the learning process would dynamically reach an
equilibrium performance that is around 92% of the maximum knapsack value. Our
results have practical implications for the deployment of ML models in contexts
where human decisions may deviate from the indisputable ground truth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 Pages and appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProteinBench: A Holistic Evaluation of Protein Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Ye, Zaixiang Zheng, Dongyu Xue, Yuning Shen, Lihao Wang, Yiming Ma, Yan Wang, Xinyou Wang, Xiangxin Zhou, Quanquan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed a surge in the development of protein foundation
models, significantly improving performance in protein prediction and
generative tasks ranging from 3D structure prediction and protein design to
conformational dynamics. However, the capabilities and limitations associated
with these models remain poorly understood due to the absence of a unified
evaluation framework. To fill this gap, we introduce ProteinBench, a holistic
evaluation framework designed to enhance the transparency of protein foundation
models. Our approach consists of three key components: (i) A taxonomic
classification of tasks that broadly encompass the main challenges in the
protein domain, based on the relationships between different protein
modalities; (ii) A multi-metric evaluation approach that assesses performance
across four key dimensions: quality, novelty, diversity, and robustness; and
(iii) In-depth analyses from various user objectives, providing a holistic view
of model performance. Our comprehensive evaluation of protein foundation models
reveals several key findings that shed light on their current capabilities and
limitations. To promote transparency and facilitate further research, we
release the evaluation dataset, code, and a public leaderboard publicly for
further analysis and a general modular toolkit. We intend for ProteinBench to
be a living benchmark for establishing a standardized, in-depth evaluation
framework for protein foundation models, driving their development and
application while fostering collaboration within the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 2 figures and 15 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Programming Languages <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Calculus for Scoped Effects & Handlers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.09697v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.09697v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roger Bosman, Birthe van den Berg, Wenhao Tang, Tom Schrijvers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algebraic effects & handlers have become a standard approach for side-effects
in functional programming. Their modular composition with other effects and
clean separation of syntax and semantics make them attractive to a wide
audience. However, not all effects can be classified as algebraic; some need a
more sophisticated handling. In particular, effects that have or create a
delimited scope need special care, as their continuation consists of two
parts-in and out of the scope-and their modular composition introduces
additional complexity. These effects are called scoped and have gained
attention by their growing applicability and adoption in popular libraries.
While calculi have been designed with algebraic effects & handlers built in to
facilitate their use, a calculus that supports scoped effects & handlers in a
similar manner does not yet exist. This work fills this gap: we present
$\lambda_{\mathit{sc}}$, a calculus with native support for both algebraic and
scoped effects & handlers. It addresses the need for polymorphic handlers and
explicit clauses for forwarding unknown scoped operations to other handlers.
Our calculus is based on Eff, an existing calculus for algebraic effects,
extended with Koka-style row polymorphism, and consists of a formal grammar,
operational semantics, a (type-safe) type-and-effect system and type inference.
We demonstrate $\lambda_{\mathit{sc}}$ on a range of examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RTLCoder: Fully Open-Source and Efficient LLM-Assisted RTL Code
  Generation Technique 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08617v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08617v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shang Liu, Wenji Fang, Yao Lu, Jing Wang, Qijun Zhang, Hongce Zhang, Zhiyao Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automatic generation of RTL code (e.g., Verilog) using natural language
instructions and large language models (LLMs) has attracted significant
research interest recently. However, most existing approaches heavily rely on
commercial LLMs such as ChatGPT, while open-source LLMs tailored for this
specific design generation task exhibit notably inferior performance. The
absence of high-quality open-source solutions restricts the flexibility and
data privacy of this emerging technique. In this study, we present a new
customized LLM solution with a modest parameter count of only 7B, achieving
better performance than GPT-3.5 on all representative benchmarks for RTL code
generation. Especially, it outperforms GPT-4 in VerilogEval Machine benchmark.
This remarkable balance between accuracy and efficiency is made possible by
leveraging our new RTL code dataset and a customized LLM algorithm, both of
which have been made fully open-source. Furthermore, we have successfully
quantized our LLM to 4-bit with a total size of 4GB, enabling it to function on
a single laptop with only slight performance degradation. This efficiency
allows the RTL generator to serve as a local assistant for engineers, ensuring
all design privacy concerns are addressed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Computer-Aided Design of Integrated
  Circuits and Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving the Phase Ordering Problem $\ne$ Generating the Globally Optimal
  Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03120v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03120v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Wang, Hongyu Chen, Ke Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Phase ordering problem has been a long-standing challenge in compiler
optimizations. Over the past four decades, a significant amount of effort has
been devoted, and indeed, substantial progress has been made. However, in this
paper, we raise questions about the overall significance of solving the phase
ordering problem in the first place, as pursuing a solution to this problem may
not align with the fundamental goal of compiler optimizations, i.e., generating
the globally optimal code among all programs that compilers deem semantically
equivalent to an input program.
  Our findings, supported by both theoretical and empirical evidence, show that
solving the phase ordering problem is not equivalent to generating such
globally optimal code. The fundamental reason that applying the optimal phase
ordering may still result in suboptimal code is the exclusion of programs of
less efficiency during the optimization process. Motivated by this insight, we
propose a theoretical approach, called \textit{infinitive iterative
bi-directional optimizations} (\textit{IIBO}), which is guaranteed to converge
to the globally optimal code for any input program. We realize IIBO into a
practical algorithm and apply it to optimize real-world programs. Results show
that IIBO frequently generates more efficient code than GCC/LLVM, two
state-of-the-art industry compilers, as well as exhaustive search, which can be
deemed the solution to the phasing ordering problem.% input programs.
  Given the significance and impact of our results, we are currently in active
discussions with LLVM engineers on the possible incorporation of our findings
into their next release. In general, we expect our work to inspire new design
principles for compiler development in the pursuit of generating the globally
optimal code.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Networking and Internet Architecture <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Smart Jamming Attack and Mitigation on Deep Transfer Reinforcement
  Learning Enabled Resource Allocation for Network Slicing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shavbo Salehi, Hao Zhou, Medhat Elsayed, Majid Bavand, Raimundas Gaigalas, Yigit Ozcan, Melike Erol-Kantarci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network slicing is a pivotal paradigm in wireless networks enabling
customized services to users and applications. Yet, intelligent jamming attacks
threaten the performance of network slicing. In this paper, we focus on the
security aspect of network slicing over a deep transfer reinforcement learning
(DTRL) enabled scenario. We first demonstrate how a deep reinforcement learning
(DRL)-enabled jamming attack exposes potential risks. In particular, the
attacker can intelligently jam resource blocks (RBs) reserved for slices by
monitoring transmission signals and perturbing the assigned resources. Then, we
propose a DRL-driven mitigation model to mitigate the intelligent attacker.
Specifically, the defense mechanism generates interference on unallocated RBs
where another antenna is used for transmitting powerful signals. This causes
the jammer to consider these RBs as allocated RBs and generate interference for
those instead of the allocated RBs. The analysis revealed that the intelligent
DRL-enabled jamming attack caused a significant 50% degradation in network
throughput and 60% increase in latency in comparison with the no-attack
scenario. However, with the implemented mitigation measures, we observed 80%
improvement in network throughput and 70% reduction in latency in comparison to
the under-attack scenario.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PAMLR: A Passive-Active Multi-Armed Bandit-Based Solution for LoRa
  Channel Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihoon Yun, Chengzhang Li, Anish Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving low duty cycle operation in low-power wireless networks in urban
environments is complicated by the complex and variable dynamics of external
interference and fading. We explore the use of reinforcement learning for
achieving low power consumption for the task of optimal selection of channels.
The learning relies on a hybrid of passive channel sampling for dealing with
external interference and active channel sampling for dealing with fading. Our
solution, Passive-Active Multi-armed bandit for LoRa (PAMLR, pronounced
"Pamela"), balances the two types of samples to achieve energy-efficient
channel selection: active channel measurements are tuned to an appropriately
low level to update noise thresholds, and to compensate passive channel
measurements are tuned to an appropriately high level for selecting the
top-most channels from channel exploration using the noise thresholds. The
rates of both types of samples are adapted in response to channel dynamics.
Based on extensive testing in multiple environments in different cities, we
validate that PAMLR can maintain excellent communication quality, as
demonstrated by a low SNR regret compared to the optimal channel allocation
policy, while substantially minimizing the energy cost associated with channel
measurements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiable and Learnable Wireless Simulation with Geometric
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14995v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14995v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Hehn, Markus Peschl, Tribhuvanesh Orekondy, Arash Behboodi, Johann Brehmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modelling the propagation of electromagnetic wireless signals is critical for
designing modern communication systems. Wireless ray tracing simulators model
signal propagation based on the 3D geometry and other scene parameters, but
their accuracy is fundamentally limited by underlying modelling assumptions and
correctness of parameters. In this work, we introduce Wi-GATr, a
fully-learnable neural simulation surrogate designed to predict the channel
observations based on scene primitives (e.g., surface mesh, antenna position
and orientation). Recognizing the inherently geometric nature of these
primitives, Wi-GATr leverages an equivariant Geometric Algebra Transformer that
operates on a tokenizer specifically tailored for wireless simulation. We
evaluate our approach on a range of tasks (i.e., signal strength and delay
spread prediction, receiver localization, and geometry reconstruction) and find
that Wi-GATr is accurate, fast, sample-efficient, and robust to
symmetry-induced transformations. Remarkably, we find our results also
translate well to the real world: Wi-GATr demonstrates more than 35% lower
error than hybrid techniques, and 70% lower error than a calibrated wireless
tracer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Throughput Requirements for RAN Functional Splits in 3D-Networks <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        MohammadAmin Vakilifard, Tim Düe, Mohammad Rihan, Maik Röper, Dirk Wübben, Carsten Bockelmann, Armin Dekorsy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of non-terrestrial communication necessitates its
integration with existing terrestrial networks, as highlighted in 3GPP Releases
16 and 17. This paper analyses the concept of functional splits in 3D-Networks.
To manage this complex structure effectively, the adoption of a Radio Access
Network (RAN) architecture with Functional Split (FS) offers advantages in
flexibility, scalability, and cost-efficiency. RAN achieves this by
disaggregating functionalities into three separate units. Analogous to the
terrestrial network approach, 3GPP is extending this concept to non-terrestrial
platforms as well. This work presents a general analysis of the requested
Fronthaul (FH) data rate on feeder link between a non-terrestrial platform and
the ground-station. Each split option is a trade-of between FH data rate and
the respected complexity. Since flying nodes face more limitations regarding
power consumption and complexity on board in comparison to terrestrial ones, we
are investigating the split options between lower and higher physical layer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14th International ITG Conference on Systems, Communications and
  Coding (SCC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Performance Analysis of 6TiSCH Networks Using Discrete Events Simulator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03383v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03383v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guilherme de Santi Peron, Marcos Eduardo Pivaro Monteiro, João Luís Verdegay de Barros, Jamil Farhat, Glauber Brante
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Internet of Things (IoT) empowers small devices to sense, react, and
communicate, with applications ranging from smart ordinary household objects to
complex industrial processes. To provide access to an increasing number of IoT
devices, particularly in long-distance communication scenarios, a robust
low-power wide area network (LPWAN) protocol becomes essential. A widely
adopted protocol for this purpose is 6TiSCH, which builds upon the IEEE
802.15.4 standard. It introduces time-slotted channel hopping (TSCH) mode as a
new medium access control (MAC) layer operating mode, in conjunction with IEEE
802.15.4g, which also defines both MAC and physical layer (PHY) layers and
provides IPv6 connectivity for LPWAN. Notably, 6TiSCH has gained adoption in
significant standards such as Wireless Intelligent Ubiquitous Networks
(Wi-SUN). This study evaluates the scalability of 6TiSCH, with a focus on key
parameters such as queue size, the maximum number of single-hop retries, and
the slotframe length. Computational simulations were performed using an
open-source simulator and obtained the following results: increasing the
transmission queue size, along with adjusting the number of retries and
slotframe length, leads to a reduction in the packet error rate (PER). Notably,
the impact of the number of retries is particularly pronounced. Furthermore,
the effect on latency varies based on the specific combination of these
parameters as the network scales.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistic Allocation of Payload Code Rate and Header Copies in
  LR-FHSS Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamil de Araujo Farhat, Jean Michel de Souza Sant'Ana, João Luiz Rebelatto, Nurul Huda Mahmood, Gianni Pasolini, Richard Demo Souza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We evaluate the performance of the LoRaWAN Long-Range Frequency Hopping
Spread Spectrum (LR-FHSS) technique using a device-level probabilistic strategy
for code rate and header replica allocation. Specifically, we investigate the
effects of different header replica and code rate allocations at each
end-device, guided by a probability distribution provided by the network
server. As a benchmark, we compare the proposed strategy with the standardized
LR-FHSS data rates DR8 and DR9. Our numerical results demonstrate that the
proposed strategy consistently outperforms the DR8 and DR9 standard data rates
across all considered scenarios. Notably, our findings reveal that the optimal
distribution rarely includes data rate DR9, while data rate DR8 significantly
contributes to the goodput and energy efficiency optimizations.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Seesaw Model Attack Algorithm for Distributed Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Yang, Tianyi Luo, Yanjie Dong, Aohan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the Byzantine attack problem within the context of model
training in distributed learning systems. While ensuring the convergence of
current model training processes, common solvers (e.g. SGD, Adam, RMSProp,
etc.) can be easily compromised by malicious nodes in these systems.
Consequently, the training process may either converge slowly or even diverge.
To develop effective secure distributed learning solvers, it is crucial to
first examine attack methods to assess the robustness of these solvers. In this
work, we contribute to the design of attack strategies by initially
highlighting the limitations of finite-norm attacks. We then introduce the
seesaw attack, which has been demonstrated to be more effective than the
finite-norm attack. Through numerical experiments, we evaluate the efficacy of
the seesaw attack across various gradient aggregation rules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at IEEE SmartIoT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Digital Twin Framework for Liquid-cooled Supercomputers as
  Demonstrated at Exascale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wesley Brewer, Matthias Maiterth, Vineet Kumar, Rafal Wojda, Sedrick Bouknight, Jesse Hines, Woong Shin, Scott Greenwood, David Grant, Wesley Williams, Feiyi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ExaDigiT, an open-source framework for developing comprehensive
digital twins of liquid-cooled supercomputers. It integrates three main
modules: (1) a resource allocator and power simulator, (2) a transient
thermo-fluidic cooling model, and (3) an augmented reality model of the
supercomputer and central energy plant. The framework enables the study of
"what-if" scenarios, system optimizations, and virtual prototyping of future
systems. Using Frontier as a case study, we demonstrate the framework's
capabilities by replaying six months of system telemetry for systematic
verification and validation. Such a comprehensive analysis of a liquid-cooled
exascale supercomputer is the first of its kind. ExaDigiT elucidates complex
transient cooling system dynamics, runs synthetic or real workloads, and
predicts energy losses due to rectification and voltage conversion. Throughout
our paper, we present lessons learned to benefit HPC practitioners developing
similar digital twins. We envision the digital twin will be a key enabler for
sustainable, energy-efficient supercomputing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures, To be published in the Proceedings of the
  International Conference for High Performance Computing, Networking, Storage
  and Analysis. 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DIMS: Distributed Index for Similarity Search in Metric Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Zhu, Chengyang Luo, Tang Qian, Lu Chen, Yunjun Gao, Baihua Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Similarity search finds objects that are similar to a given query object
based on a similarity metric. As the amount and variety of data continue to
grow, similarity search in metric spaces has gained significant attention.
Metric spaces can accommodate any type of data and support flexible distance
metrics, making similarity search in metric spaces beneficial for many
real-world applications, such as multimedia retrieval, personalized
recommendation, trajectory analytics, data mining, decision planning, and
distributed servers. However, existing studies mostly focus on indexing metric
spaces on a single machine, which faces efficiency and scalability limitations
with increasing data volume and query amount. Recent advancements in similarity
search turn towards distributed methods, while they face challenges including
inefficient local data management, unbalanced workload, and low concurrent
search efficiency. To this end, we propose DIMS, an efficient Distributed Index
for similarity search in Metric Spaces. First, we design a novel three-stage
heterogeneous partition to achieve workload balance. Then, we present an
effective three-stage indexing structure to efficiently manage objects. We also
develop concurrent search methods with filtering and validation techniques that
support efficient distributed similarity search. Additionally, we devise a
cost-based optimization model to balance communication and computation cost.
Extensive experiments demonstrate that DIMS significantly outperforms existing
distributed similarity search approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast State Restoration in LLM Serving with HCache <span class="chip">EuroSys 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiwei Gao, Youmin Chen, Jiwu Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing complexity of LLM usage today, e.g., multi-round conversation and
retrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)
reusable across user requests. Given the capacity constraints of GPU memory,
only a limited number of contexts can be cached on GPU for reusing. Existing
inference systems typically evict part of the KV cache and restore it by
recomputing it from the original tokens or offloading it to host storage for
later retrieval, both of which introduce substantial computational or I/O
overheads. We propose HCache, a novel LLM state restoration method. Its key
idea is to restore LLM states from intermediate activations and thus utilize
computational and I/O resources with low overhead. We enhance HCache with two
techniques, including i) a bubble-free restoration scheduler that integrates
resource-complementary methods to optimize the balance between computation and
IO tasks; and ii) a chunk-based storage manager to address the layout mismatch
issue (i.e., layer-before-token saving versus token-before-layer restoration).
Our evaluations, conducted using real-world tasks, show that HCache reduces the
TTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less
storage space; compared to token recomputation, HCache achieves up to 5.73X
reduction in TTFT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EuroSys 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cloud-Based Scheduling Mechanism for Scalable and Resource-Efficient
  Centralized Controllers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Achilleas Santi Seisa, Sumeet Gajanan Satpute, George Nikolakopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel approach to address the challenges of deploying
complex robotic software in large-scale systems, i.e., Centralized Nonlinear
Model Predictive Controllers (CNMPCs) for multi-agent systems. The proposed
approach is based on a Kubernetes-based scheduling mechanism designed to
monitor and optimize the operation of CNMPCs, while addressing the scalability
limitation of centralized control schemes. By leveraging a cluster in a
real-time cloud environment, the proposed mechanism effectively offloads the
computational burden of CNMPCs. Through experiments, we have demonstrated the
effectiveness and performance of our system, especially in scenarios where the
number of robots is subject to change. Our work contributes to the advancement
of cloud-based control strategies and lays the foundation for enhanced
performance in cloud-controlled robotic systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, IECON 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedBiP: Heterogeneous One-Shot Federated Learning with Personalized
  Latent Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haokun Chen, Hang Li, Yao Zhang, Gengyuan Zhang, Jinhe Bi, Philip Torr, Jindong Gu, Denis Krompass, Volker Tresp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One-Shot Federated Learning (OSFL), a special decentralized machine learning
paradigm, has recently gained significant attention. OSFL requires only a
single round of client data or model upload, which reduces communication costs
and mitigates privacy threats compared to traditional FL. Despite these
promising prospects, existing methods face challenges due to client data
heterogeneity and limited data quantity when applied to real-world OSFL
systems. Recently, Latent Diffusion Models (LDM) have shown remarkable
advancements in synthesizing high-quality images through pretraining on
large-scale datasets, thereby presenting a potential solution to overcome these
issues. However, directly applying pretrained LDM to heterogeneous OSFL results
in significant distribution shifts in synthetic data, leading to performance
degradation in classification models trained on such data. This issue is
particularly pronounced in rare domains, such as medical imaging, which are
underrepresented in LDM's pretraining data. To address this challenge, we
propose Federated Bi-Level Personalization (FedBiP), which personalizes the
pretrained LDM at both instance-level and concept-level. Hereby, FedBiP
synthesizes images following the client's local data distribution without
compromising the privacy regulations. FedBiP is also the first approach to
simultaneously address feature space heterogeneity and client data scarcity in
OSFL. Our method is validated through extensive experiments on three OSFL
benchmarks with feature space heterogeneity, as well as on challenging medical
and satellite image datasets with label heterogeneity. The results demonstrate
the effectiveness of FedBiP, which substantially outperforms other OSFL
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stateful Large Language Model Serving with Pensieve 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05516v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05516v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingfan Yu, Jinkun Lin, Jinyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are wildly popular today and it is important to
serve them efficiently. Existing LLM serving systems are stateless across
requests. Consequently, when LLMs are used in the common setting of multi-turn
conversations, a growing log of the conversation history must be processed
alongside any request by the serving system at each turn, resulting in repeated
processing.
  In this paper, we design $Pensieve$, a system optimized for multi-turn
conversation LLM serving. $Pensieve$ maintains the conversation state across
requests by caching previously processed history to avoid duplicate processing.
$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to
efficiently store and retrieve cached data. $Pensieve$ also generalizes the
recent PagedAttention kernel to support attention between multiple input tokens
with a GPU cache spread over non-contiguous memory. Our evaluation shows that
$Pensieve$ can achieve $1.14$-$3.0\times$ the throughput of vLLM and
TensorRT-LLM and significantly reduce latency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeBS-Flow: Benchmarking Serverless Cloud Function Workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03480v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03480v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Larissa Schmid, Marcin Copik, Alexandru Calotoiu, Laurin Brandner, Anne Koziolek, Torsten Hoefler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Serverless computing has emerged as a prominent paradigm, with a significant
adoption rate among cloud customers. While this model offers advantages such as
abstraction from the deployment and resource scheduling, it also poses
limitations in handling complex use cases due to the restricted nature of
individual functions. Serverless workflows address this limitation by
orchestrating multiple functions into a cohesive application. However, existing
serverless workflow platforms exhibit significant differences in their
programming models and infrastructure, making fair and consistent performance
evaluations difficult in practice. To address this gap, we propose the first
serverless workflow benchmarking suite SeBS-Flow, providing a platform-agnostic
workflow model that enables consistent benchmarking across various platforms.
SeBS-Flow includes six real-world application benchmarks and four
microbenchmarks representing different computational patterns. We conduct
comprehensive evaluations on three major cloud platforms, assessing
performance, cost, scalability, and runtime deviations. We make our benchmark
suite open-source, enabling rigorous and comparable evaluations of serverless
workflows over time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LayerKV: Optimizing Large Language Model Serving with Layer-wise KV
  Cache Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Xiong, Hao Wu, Changxu Shao, Ziqing Wang, Rui Zhang, Yuhong Guo, Junping Zhao, Ke Zhang, Zhenxuan Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The expanding context windows in large language models (LLMs) have greatly
enhanced their capabilities in various applications, but they also introduce
significant challenges in maintaining low latency, particularly in Time to
First Token (TTFT). This paper identifies that the sharp rise in TTFT as
context length increases is predominantly driven by queuing delays, which are
caused by the growing demands for GPU Key-Value (KV) cache allocation clashing
with the limited availability of KV cache blocks. To address this issue, we
propose LayerKV, a simple yet effective plug-in method that effectively reduces
TTFT without requiring additional hardware or compromising output performance,
while seamlessly integrating with existing parallelism strategies and
scheduling techniques. Specifically, LayerKV introduces layer-wise KV block
allocation, management, and offloading for fine-grained control over system
memory, coupled with an SLO-aware scheduler to optimize overall Service Level
Objectives (SLOs). Comprehensive evaluations on representative models, ranging
from 7B to 70B parameters, across various GPU configurations, demonstrate that
LayerKV improves TTFT latency up to 69x and reduces SLO violation rates by
28.7%, significantly enhancing the user experience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Space-time deterministic graph rewriting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05838v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05838v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Arrighi, Marin Costes, Gilles Dowek, Luidnel Maignan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study non-terminating graph rewriting models, whose local rules are
applied non-deterministically -- and yet enjoy a strong form of determinism,
namely space-time determinism. Of course in the case of terminating computation
it is well-known that the mess introduced by asynchronous rule applications may
not matter to the end result, as confluence conspires to produce a unique
normal form. In the context of non-terminating computation however, confluence
is a very weak property, and (almost) synchronous rule applications is always
preferred e.g. when it comes to simulating dynamical systems. Here we provide
sufficient conditions so that asynchronous local rule applications conspire to
produce well-determined events in the space-time unfolding of the graph,
regardless of their application orders. Our first example is an asynchronous
simulation of a dynamical system. Our second example features time dilation, in
the spirit of general relativity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable and Adaptively Secure Any-Trust Distributed Key Generation and
  All-hands Checkpointing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09592v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09592v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanwen Feng, Tiancheng Mai, Qiang Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The classical distributed key generation protocols (DKG) are resurging due to
their widespread applications in blockchain. While efforts have been made to
improve DKG communication, practical large-scale deployments are still yet to
come due to various challenges, including the heavy computation and
communication (particularly broadcast) overhead in their adversarial cases. In
this paper, we propose a practical DKG for DLog-based cryptosystems, which
achieves (quasi-)linear computation and communication per-node cost with the
help of a common coin, even in the face of the maximal amount of Byzantine
nodes. Moreover, our protocol is secure against adaptive adversaries, which can
corrupt less than half of all nodes. The key to our improvements lies in
delegating the most costly operations to an Any-Trust group together with a set
of techniques for adaptive security. This group is randomly sampled and
consists of a small number of individuals. The population only trusts that at
least one member in the group is honest, without knowing which one. Moreover,
we present a generic transformer that enables us to efficiently deploy a
conventional distributed protocol like our DKG, even when the participants have
different weights. Additionally, we introduce an extended broadcast channel
based on a blockchain and data dispersal network (such as IPFS), enabling
reliable broadcasting of arbitrary-size messages at the cost of constant-size
blockchain storage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-06T00:00:00Z">2024-10-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Cross-Lingual Meta-Learning Method Based on Domain Adaptation for
  Speech Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David-Gabriel Ion, Răzvan-Alexandru Smădu, Dumitru-Clementin Cercel, Florin Pop, Mihaela-Claudia Cercel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Best-performing speech models are trained on large amounts of data in the
language they are meant to work for. However, most languages have sparse data,
making training models challenging. This shortage of data is even more
prevalent in speech emotion recognition. Our work explores the model's
performance in limited data, specifically for speech emotion recognition.
Meta-learning specializes in improving the few-shot learning. As a result, we
employ meta-learning techniques on speech emotion recognition tasks, accent
recognition, and person identification. To this end, we propose a series of
improvements over the multistage meta-learning method. Unlike other works
focusing on smaller models due to the high computational cost of meta-learning
algorithms, we take a more practical approach. We incorporate a large
pre-trained backbone and a prototypical network, making our methods more
feasible and applicable. Our most notable contribution is an improved
fine-tuning technique during meta-testing that significantly boosts the
performance on out-of-distribution datasets. This result, together with
incremental improvements from several other works, helped us achieve accuracy
scores of 83.78% and 56.30% for Greek and Romanian speech emotion recognition
datasets not included in the training or validation splits in the context of
4-way 5-shot learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 1 figure, Accepted by WISE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Survey</span> of Query-based Text Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Yu, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query-based text summarization is an important real world problem that
requires to condense the prolix text data into a summary under the guidance of
the query information provided by users. The topic has been studied for a long
time and there are many existing interesting research related to query-based
text summarization. Yet much of the work is not systematically surveyed. This
survey aims at summarizing some interesting work in query-based text
summarization methods as well as related generic text summarization methods.
Not all taxonomies in this paper exist the related work to the best of our
knowledge and some analysis will be presented.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>s have evil twins <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07064v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07064v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rimon Melamed, Lucas H. McCabe, Tanay Wakhare, Yejin Kim, H. Howie Huang, Enric Boix-Adsera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We discover that many natural-language prompts can be replaced by
corresponding prompts that are unintelligible to humans but that provably
elicit similar behavior in language models. We call these prompts "evil twins"
because they are obfuscated and uninterpretable (evil), but at the same time
mimic the functionality of the original natural-language prompts (twins).
Remarkably, evil twins transfer between models. We find these prompts by
solving a maximum-likelihood problem which has applications of independent
interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main, camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Adversarial Perspective on Machine Unlearning for AI Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18025v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18025v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Łucki, Boyi Wei, Yangsibo Huang, Peter Henderson, Florian Tramèr, Javier Rando
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are finetuned to refuse questions about hazardous
knowledge, but these protections can often be bypassed. Unlearning methods aim
at completely removing hazardous capabilities from models and make them
inaccessible to adversaries. This work challenges the fundamental differences
between unlearning and traditional safety post-training from an adversarial
perspective. We demonstrate that existing jailbreak methods, previously
reported as ineffective against unlearning, can be successful when applied
carefully. Furthermore, we develop a variety of adaptive methods that recover
most supposedly unlearned capabilities. For instance, we show that finetuning
on 10 unrelated examples or removing specific directions in the activation
space can recover most hazardous capabilities for models edited with RMU, a
state-of-the-art unlearning method. Our findings challenge the robustness of
current unlearning approaches and question their advantages over safety
training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Efficient Language and Vision Assistants for Visually-Situated
  Natural Language Understanding: What Matters in Reading and Reasoning <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geewook Kim, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in language and vision assistants have showcased
impressive capabilities but suffer from a lack of transparency, limiting
broader research and reproducibility. While open-source models handle general
image tasks effectively, they face challenges with the high computational
demands of complex visually-situated text understanding. Such tasks often
require increased token inputs and large vision modules to harness
high-resolution information. Striking a balance between model size and data
importance remains an open question. This study aims to redefine the design of
vision-language models by identifying key components and creating efficient
models with constrained inference costs. By strategically formulating datasets,
optimizing vision modules, and enhancing supervision techniques, we achieve
significant improvements in inference throughput while maintaining high
performance. Extensive experiments across models ranging from 160M to 13B
parameters offer insights into model optimization. We will fully open-source
our codebase, models, and datasets at https://github.com/naver-ai/elva.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks
  of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08926v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08926v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andy K. Zhang, Neil Perry, Riya Dulepet, Joey Ji, Justin W. Lin, Eliot Jones, Celeste Menders, Gashon Hussein, Samantha Liu, Donovan Jasper, Pura Peetathawatchai, Ari Glenn, Vikram Sivashankar, Daniel Zamoshchin, Leo Glikbarg, Derek Askaryar, Mike Yang, Teddy Zhang, Rishi Alluri, Nathan Tran, Rinnara Sangpisit, Polycarpos Yiorkadjis, Kenny Osele, Gautham Raghupathi, Dan Boneh, Daniel E. Ho, Percy Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Model (LM) agents for cybersecurity that are capable of autonomously
identifying vulnerabilities and executing exploits have the potential to cause
real-world impact. Policymakers, model providers, and other researchers in the
AI and cybersecurity communities are interested in quantifying the capabilities
of such agents to help mitigate cyberrisk and investigate opportunities for
penetration testing. Toward that end, we introduce Cybench, a framework for
specifying cybersecurity tasks and evaluating agents on those tasks. We include
40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF
competitions, chosen to be recent, meaningful, and spanning a wide range of
difficulties. Each task includes its own description, starter files, and is
initialized in an environment where an agent can execute bash commands and
observe outputs. Since many tasks are beyond the capabilities of existing LM
agents, we introduce subtasks for each task, which break down a task into
intermediary steps for a more detailed evaluation. To evaluate agent
capabilities, we construct a cybersecurity agent and evaluate 8 models: GPT-4o,
OpenAI o1-preview, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct,
Gemini 1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. Without subtask
guidance, agents leveraging Claude 3.5 Sonnet, GPT-4o, OpenAI o1-preview, and
Claude 3 Opus successfully solved complete tasks that took human teams up to 11
minutes to solve. In comparison, the most difficult task took human teams 24
hours and 54 minutes to solve. All code and data are publicly available at
https://cybench.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>78 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for
  Aligning Large Language Models to Online Communities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12074v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12074v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao He, Minh Duc Chu, Rebecca Dorn, Siyi Guo, Kristina Lerman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social scientists use surveys to probe the opinions and beliefs of
populations, but these methods are slow, costly, and prone to biases. Recent
advances in large language models (LLMs) enable the creating of computational
representations or "digital twins" of populations that generate human-like
responses mimicking the population's language, styles, and attitudes. We
introduce Community-Cross-Instruct, an unsupervised framework for aligning LLMs
to online communities to elicit their beliefs. Given a corpus of a community's
online discussions, Community-Cross-Instruct automatically generates
instruction-output pairs by an advanced LLM to (1) finetune a foundational LLM
to faithfully represent that community, and (2) evaluate the alignment of the
finetuned model to the community. We demonstrate the method's utility in
accurately representing political and diet communities on Reddit. Unlike prior
methods requiring human-authored instructions, Community-Cross-Instruct
generates instructions in a fully unsupervised manner, enhancing scalability
and generalization across domains. This work enables cost-effective and
automated surveying of diverse online communities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Superposed Decoding: Multiple Generations from a Single Autoregressive
  Inference Pass 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18400v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18400v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Shen, Alan Fan, Sarah M. Pratt, Jae Sung Park, Matthew Wallingford, Sham M. Kakade, Ari Holtzman, Ranjay Krishna, Ali Farhadi, Aditya Kusupati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many applications today provide users with multiple auto-complete drafts as
they type, including GitHub's code completion, Gmail's smart compose, and
Apple's messaging auto-suggestions. Under the hood, language models support
this by running an autoregressive inference pass to provide a draft.
Consequently, providing $k$ drafts to the user requires running an expensive
language model $k$ times. To alleviate the computation cost of running $k$
inference passes, we propose Superposed Decoding, a new decoding algorithm that
generates $k$ drafts at the computation cost of one autoregressive inference
pass. We achieve this by feeding a superposition of the most recent token
embeddings from the $k$ drafts as input to the next decoding step of the
language model. At every inference step we combine the $k$ drafts with the
top-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,
using an n-gram interpolation with minimal compute overhead to filter out
incoherent generations. Our experiments show that $k$ drafts from Superposed
Decoding are at least as coherent and factual as Nucleus Sampling and Greedy
Decoding respectively, while being at least $2.44\times$ faster for $k\ge3$. In
a compute-normalized setting, user evaluations demonstrably favor text
generated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can
also be combined with other decoding strategies, resulting in universal
coverage gains when scaling inference time compute. Code and more examples
open-sourced at https://github.com/RAIVNLab/SuperposedDecoding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Software Engineering <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of Code LLMs on Geospatial Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Gramacki, Bruno Martins, Piotr Szymański
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software development support tools have been studied for a long time, with
recent approaches using Large Language Models (LLMs) for code generation. These
models can generate Python code for data science and machine learning
applications. LLMs are helpful for software engineers because they increase
productivity in daily work. An LLM can also serve as a "mentor" for
inexperienced software developers, and be a viable learning support.
High-quality code generation with LLMs can also be beneficial in geospatial
data science. However, this domain poses different challenges, and code
generation LLMs are typically not evaluated on geospatial tasks. Here, we show
how we constructed an evaluation benchmark for code generation models, based on
a selection of geospatial tasks. We categorised geospatial tasks based on their
complexity and required tools. Then, we created a dataset with tasks that test
model capabilities in spatial reasoning, spatial data processing, and
geospatial tools usage. The dataset consists of specific coding problems that
were manually created for high quality. For every problem, we proposed a set of
test scenarios that make it possible to automatically check the generated code
for correctness. In addition, we tested a selection of existing code generation
LLMs for code generation in the geospatial domain. We share our dataset and
reproducible evaluation code on a public GitHub repository, arguing that this
can serve as an evaluation benchmark for new LLMs in the future. Our dataset
will hopefully contribute to the development new models capable of solving
geospatial coding tasks with high accuracy. These models will enable the
creation of coding assistants tailored for geospatial applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hammer: Robust Function-Calling for On-Device Language Models via
  Function Masking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiqiang Lin, Muning Wen, Qiuying Peng, Guanyu Nie, Junwei Liao, Jun Wang, Xiaoyun Mo, Jiamu Zhou, Cheng Cheng, Yin Zhao, Jun Wang, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have demonstrated impressive value in performing as
autonomous agents when equipped with external tools and API calls. Nonetheless,
effectively harnessing their potential for executing complex tasks crucially
relies on enhancements in their function calling capabilities. This paper
identifies a critical gap in existing function calling models, where
performance varies significantly across benchmarks, often due to being misled
by specific naming conventions. To address such an issue, we introduce Hammer,
a novel family of foundation models specifically engineered for on-device
function calling. Hammer employs an augmented dataset that enhances models'
sensitivity to irrelevant functions and incorporates function masking
techniques to minimize misleading. Our empirical evaluations reveal that Hammer
not only outperforms larger models but also demonstrates robust generalization
across diverse benchmarks, achieving sota results. Our open source
contributions include a specialized dataset for irrelevance detection, a tuning
framework for enhanced generalization, and the Hammer models, establishing a
new standard for function calling performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Large-Scale Exploit Instrumentation Study of AI/ML Supply Chain
  Attacks in Hugging Face Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beatrice Casey, Joanna C. S. Santos, Mehdi Mirakhorli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of machine learning (ML) techniques has led to ample
opportunities for developers to develop and deploy their own models. Hugging
Face serves as an open source platform where developers can share and download
other models in an effort to make ML development more collaborative. In order
for models to be shared, they first need to be serialized. Certain Python
serialization methods are considered unsafe, as they are vulnerable to object
injection. This paper investigates the pervasiveness of these unsafe
serialization methods across Hugging Face, and demonstrates through an
exploitation approach, that models using unsafe serialization methods can be
exploited and shared, creating an unsafe environment for ML developers. We
investigate to what extent Hugging Face is able to flag repositories and files
using unsafe serialization methods, and develop a technique to detect malicious
models. Our results show that Hugging Face is home to a wide range of
potentially vulnerable models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Potential of Conversational Test Suite Based Program
  Repair on SWE-bench 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Cheshkov, Pavel Zadorozhny, Rodion Levichev, Evgeny Maslov, Ronaldo Franco Jaldin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic program repair at project level may open yet to be seen
opportunities in various fields of human activity. Since the SWE-Bench
challenge was presented, we have seen numerous of solutions. Patch generation
is a part of program repair, and test suite-based conversational patch
generation has proven its effectiveness. However, the potential of
conversational patch generation has not yet specifically estimated on
SWE-Bench. This study reports experimental results aimed at evaluating the
individual effectiveness of conversational patch generation on problems from
SWE-Bench. The experiments show that a simple conversational pipeline based on
LLaMA 3.1 70B can generate valid patches in 47\% of cases, which is comparable
to the state-of-the-art in program repair on SWE-Bench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, 2 figures, 1 algorithm, appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WISE: Unraveling Business Process Metrics with Domain Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Urszula Jessen, Dirk Fahland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomalies in complex industrial processes are often obscured by high
variability and complexity of event data, which hinders their identification
and interpretation using process mining. To address this problem, we introduce
WISE (Weighted Insights for Evaluating Efficiency), a novel method for
analyzing business process metrics through the integration of domain knowledge,
process mining, and machine learning.
  The methodology involves defining business goals and establishing Process
Norms with weighted constraints at the activity level, incorporating input from
domain experts and process analysts. Individual process instances are scored
based on these constraints, and the scores are normalized to identify features
impacting process goals.
  Evaluation using the BPIC 2019 dataset and real industrial contexts
demonstrates that WISE enhances automation in business process analysis and
effectively detects deviations from desired process flows. While LLMs support
the analysis, the inclusion of domain experts ensures the accuracy and
relevance of the findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Android Malware Detection: The Influence of Chat<span class="highlight-title">GPT</span> on
  Decision-centric Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Li, Sen Fang, Tao Zhang, Haipeng Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of large language models, such as ChatGPT, non-decisional
models have been applied to various tasks. Moreover, ChatGPT has drawn
attention to the traditional decision-centric task of Android malware
detection. Despite effective detection methods proposed by scholars, they face
low interpretability issues. Specifically, while these methods excel in
classifying applications as benign or malicious and can detect malicious
behavior, they often fail to provide detailed explanations for the decisions
they make. This challenge raises concerns about the reliability of existing
detection schemes and questions their true ability to understand complex data.
In this study, we investigate the influence of the non-decisional model,
ChatGPT, on the traditional decision-centric task of Android malware detection.
We choose three state-of-the-art solutions, Drebin, XMAL, and MaMaDroid,
conduct a series of experiments on publicly available datasets, and carry out a
comprehensive comparison and analysis. Our findings indicate that these
decision-driven solutions primarily rely on statistical patterns within
datasets to make decisions, rather than genuinely understanding the underlying
data. In contrast, ChatGPT, as a non-decisional model, excels in providing
comprehensive analysis reports, substantially enhancing interpretability.
Furthermore, we conduct surveys among experienced developers. The result
highlights developers' preference for ChatGPT, as it offers in-depth insights
and enhances efficiency and understanding of challenges. Meanwhile, these
studies and analyses offer profound insights, presenting developers with a
novel perspective on Android malware detection--enhancing the reliability of
detection results from a non-decisional perspective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Assistants for Incident Lifecycle in a Microservice Environment: A
  Systematic Literature <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dahlia Ziqi Zhou, Marios Fokaefs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incidents in microservice environments can be costly and challenging to
recover from due to their complexity and distributed nature. Recent
advancements in artificial intelligence (AI) offer promising solutions for
improving incident management. This paper systematically reviews primary
studies on AI assistants designed to support different phases of the incident
lifecycle. It highlights successful applications of AI, identifies gaps in
current research, and suggests future opportunities for enhancing incident
management through AI. By examining these studies, the paper aims to provide
insights into the effectiveness of AI tools and their potential to address
ongoing challenges in incident recovery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Debugging Deep Reinforcement Learning Programs with RLExplorer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rached Bouchoucha, Ahmed Haj Yahmed, Darshan Patil, Janarthanan Rajendran, Amin Nikanjam, Sarath Chandar, Foutse Khomh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep reinforcement learning (DRL) has shown success in diverse domains such
as robotics, computer games, and recommendation systems. However, like any
other software system, DRL-based software systems are susceptible to faults
that pose unique challenges for debugging and diagnosing. These faults often
result in unexpected behavior without explicit failures and error messages,
making debugging difficult and time-consuming. Therefore, automating the
monitoring and diagnosis of DRL systems is crucial to alleviate the burden on
developers. In this paper, we propose RLExplorer, the first fault diagnosis
approach for DRL-based software systems. RLExplorer automatically monitors
training traces and runs diagnosis routines based on properties of the DRL
learning dynamics to detect the occurrence of DRL-specific faults. It then logs
the results of these diagnoses as warnings that cover theoretical concepts,
recommended practices, and potential solutions to the identified faults. We
conducted two sets of evaluations to assess RLExplorer. Our first evaluation of
faulty DRL samples from Stack Overflow revealed that our approach can
effectively diagnose real faults in 83% of the cases. Our second evaluation of
RLExplorer with 15 DRL experts/developers showed that (1) RLExplorer could
identify 3.6 times more defects than manual debugging and (2) RLExplorer is
easily integrated into DRL applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in The International Conference on Software
  Maintenance and Evolution (ICSME 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GUing: A Mobile GUI Search Engine using a Vision-Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00145v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00145v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialiang Wei, Anne-Lise Courbis, Thomas Lambolais, Binbin Xu, Pierre Louis Bernard, Gérard Dray, Walid Maalej
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphical User Interfaces (GUIs) are central to app development projects. App
developers may use the GUIs of other apps as a means of requirements refinement
and rapid prototyping or as a source of inspiration for designing and improving
their own apps. Recent research has thus suggested retrieving relevant GUI
designs that match a certain text query from screenshot datasets acquired
through crowdsourced or automated exploration of GUIs. However, such
text-to-GUI retrieval approaches only leverage the textual information of the
GUI elements, neglecting visual information such as icons or background images.
In addition, retrieved screenshots are not steered by app developers and lack
app features that require particular input data.
  To overcome these limitations, this paper proposes GUing, a GUI search engine
based on a vision-language model called GUIClip, which we trained specifically
for the problem of designing app GUIs. For this, we first collected from Google
Play app introduction images which display the most representative screenshots
and are often captioned (i.e.~labelled) by app vendors. Then, we developed an
automated pipeline to classify, crop, and extract the captions from these
images. This resulted in a large dataset which we share with this paper:
including 303k app screenshots, out of which 135k have captions. We used this
dataset to train a novel vision-language model, which is, to the best of our
knowledge, the first of its kind for GUI retrieval. We evaluated our approach
on various datasets from related work and in a manual experiment. The results
demonstrate that our model outperforms previous approaches in text-to-GUI
retrieval achieving a Recall@10 of up to 0.69 and a HIT@10 of 0.91. We also
explored the performance of GUIClip for other GUI tasks including GUI
classification and sketch-to-GUI retrieval with encouraging results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Transactions on Software Engineering and Methodology
  (TOSEM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EffiBench: Benchmarking the Efficiency of Automatically Generated Code <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02037v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02037v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Huang, Yuhao Qing, Weiyi Shang, Heming Cui, Jie M. Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code generation models have increasingly become integral to aiding software
development. Although current research has thoroughly examined the correctness
of the code produced by code generation models, a vital aspect that plays a
pivotal role in green computing and sustainability efforts has often been
neglected. This paper presents EffiBench, a benchmark with 1,000
efficiency-critical coding problems to assess the efficiency of code generated
by code generation models. EffiBench contains a diverse set of LeetCode coding
problems. Each problem is paired with an executable human-written canonical
solution, which obtains the SOTA efficiency on the LeetCode solution
leaderboard. With EffiBench, we empirically examine the ability of 42 large
language models (35 open-source and 7 closed-source) to generate efficient
code. Our evaluation results demonstrate that the efficiency of the code
generated by LLMs is generally worse than the efficiency of human-written
canonical solutions. For example, GPT-4 generated code has an average
\textbf{3.12} times execution time that of the human-written canonical
solutions. In the most extreme cases, the execution time and total memory usage
of GPT-4 generated code are \textbf{13.89} and \textbf{43.92} times that of the
canonical solutions. The source code of EffiBench is released on
https://github.com/huangd1999/EffiBench. We also provide the LeaderBoard at
https://huggingface.co/spaces/EffiBench/effibench-leaderboard.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera Ready for NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-guided Fuzzing of Distributed Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02307v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02307v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ege Berkay Gulcan, Burcu Kulahcioglu Ozkan, Rupak Majumdar, Srinidhi Nagendra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a coverage-guided testing algorithm for distributed systems
implementations. Our main innovation is the use of an abstract formal model of
the system that is used to define coverage. Such abstract models are frequently
developed in early phases of protocol design and verification but are
infrequently used at testing time. We show that guiding random test generation
using model coverage can be effective in covering interesting points in the
implementation state space. We have implemented a fuzzer for distributed system
implementations and abstract models written in TLA+. Our algorithm shows better
coverage over purely random exploration as well as random exploration guided by
different notions of scheduler coverage and mutation. In particular, we show
consistently higher coverage and detect bugs faster on implementations of
distributed consensus protocols such as those in Etcd-raft and RedisRaft.
Moreover, we discovered 13 previously unknown bugs in their implementations,
four of which could only be detected by model-guided fuzzing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AppPoet: Large Language Model based Android malware detection via
  multi-view <span class="highlight-title">prompt</span> engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18816v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18816v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxiang Zhao, Juntao Wu, Zhaoyi Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the vast array of Android applications, their multifarious functions
and intricate behavioral semantics, attackers can adopt various tactics to
conceal their genuine attack intentions within legitimate functions. However,
numerous learning-based methods suffer from a limitation in mining behavioral
semantic information, thus impeding the accuracy and efficiency of Android
malware detection. Besides, the majority of existing learning-based methods are
weakly interpretive and fail to furnish researchers with effective and readable
detection reports. Inspired by the success of the Large Language Models (LLMs)
in natural language understanding, we propose AppPoet, a LLM-assisted
multi-view system for Android malware detection. Firstly, AppPoet employs a
static method to comprehensively collect application features and formulate
various observation views. Then, using our carefully crafted multi-view prompt
templates, it guides the LLM to generate function descriptions and behavioral
summaries for each view, enabling deep semantic analysis of the views. Finally,
we collaboratively fuse the multi-view information to efficiently and
accurately detect malware through a deep neural network (DNN) classifier and
then generate the human-readable diagnostic reports. Experimental results
demonstrate that our method achieves a detection accuracy of 97.15% and an F1
score of 97.21%, which is superior to the baseline methods. Furthermore, the
case study evaluates the effectiveness of our generated diagnostic reports.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Exploratory Study on Automatic Identification of Assumptions in the
  Development of Deep Learning Frameworks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03653v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03653v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Yang, Peng Liang, Zinan Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stakeholders constantly make assumptions in the development of deep learning
(DL) frameworks. These assumptions are related to various types of software
artifacts (e.g., requirements, design decisions, and technical debt) and can
turn out to be invalid, leading to system failures. Existing approaches and
tools for assumption management usually depend on manual identification of
assumptions. However, assumptions are scattered in various sources (e.g., code
comments, commits, pull requests, and issues) of DL framework development, and
manually identifying assumptions has high costs. This study intends to evaluate
different classification models for the purpose of identification with respect
to assumptions from the point of view of developers and users in the context of
DL framework projects (i.e., issues, pull requests, and commits) on GitHub.
First, we constructed a new and largest dataset (i.e., the AssuEval dataset) of
assumptions collected from the TensorFlow and Keras repositories on GitHub.
Then we explored the performance of seven non-transformers based models (e.g.,
Support Vector Machine, Classification and Regression Trees), the ALBERT model,
and three decoder-only models (i.e., ChatGPT, Claude, and Gemini) for
identifying assumptions on the AssuEval dataset. The study results show that
ALBERT achieves the best performance (f1-score: 0.9584) for identifying
assumptions on the AssuEval dataset, which is much better than the other models
(the 2nd best f1-score is 0.8858, achieved by the Claude 3.5 Sonnet model).
Though ChatGPT, Claude, and Gemini are popular models, we do not recommend
using them to identify assumptions in DL framework development because of their
low performance. Fine-tuning ChatGPT, Claude, Gemini, or other language models
(e.g., Llama3, Falcon, and BLOOM) specifically for assumptions might improve
their performance for assumption identification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint accepted for publication in Science of Computer Programming,
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning for Actionable Warning Identification: A Comprehensive
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00324v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00324v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiuting Ge, Chunrong Fang, Xuanye Li, Weisong Sun, Daoyuan Wu, Juan Zhai, Shangwei Lin, Zhihong Zhao, Yang Liu, Zhenyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Actionable Warning Identification (AWI) plays a crucial role in improving the
usability of static code analyzers. With recent advances in Machine Learning
(ML), various approaches have been proposed to incorporate ML techniques into
AWI. These ML-based AWI approaches, benefiting from ML's strong ability to
learn subtle and previously unseen patterns from historical data, have
demonstrated superior performance. However, a comprehensive overview of these
approaches is missing, which could hinder researchers/practitioners from
understanding the current process and discovering potential for future
improvement in the ML-based AWI community. In this paper, we systematically
review the state-of-the-art ML-based AWI approaches. First, we employ a
meticulous survey methodology and gather 51 primary studies from 2000/01/01 to
2023/09/01. Then, we outline the typical ML-based AWI workflow, including
warning dataset preparation, preprocessing, AWI model construction, and
evaluation stages. In such a workflow, we categorize ML-based AWI approaches
based on the warning output format. Besides, we analyze the techniques used in
each stage, along with their strengths, weaknesses, and distribution. Finally,
we provide practical research directions for future ML-based AWI approaches,
focusing on aspects like data improvement (e.g., enhancing the warning labeling
strategy) and model exploration (e.g., exploring large language models for
AWI).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CSUR</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">61</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal 3D Fusion and In-Situ Learning for Spatially Aware AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyuan Xu, Radha Kumaran, Noah Stier, Kangyou Yu, Tobias Höllerer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Seamless integration of virtual and physical worlds in augmented reality
benefits from the system semantically "understanding" the physical environment.
AR research has long focused on the potential of context awareness,
demonstrating novel capabilities that leverage the semantics in the 3D
environment for various object-level interactions. Meanwhile, the computer
vision community has made leaps in neural vision-language understanding to
enhance environment perception for autonomous tasks. In this work, we introduce
a multimodal 3D object representation that unifies both semantic and linguistic
knowledge with the geometric representation, enabling user-guided machine
learning involving physical objects. We first present a fast multimodal 3D
reconstruction pipeline that brings linguistic understanding to AR by fusing
CLIP vision-language features into the environment and object models. We then
propose "in-situ" machine learning, which, in conjunction with the multimodal
representation, enables new tools and interfaces for users to interact with
physical spaces and objects in a spatially and linguistically meaningful
manner. We demonstrate the usefulness of the proposed system through two
real-world AR applications on Magic Leap 2: a) spatial search in physical
environments with natural language and b) an intelligent inventory system that
tracks object changes over time. We also make our full implementation and demo
data available at (https://github.com/cy-xu/spatially_aware_AI) to encourage
further exploration and research in spatially aware AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, accepted to IEEE ISMAR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaptDiff: Cross-Modality Domain Adaptation via Weak Conditional
  Semantic Diffusion for Retinal Vessel Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dewei Hu, Hao Li, Han Liu, Jiacheng Wang, Xing Yao, Daiwei Lu, Ipek Oguz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has shown remarkable performance in medical image segmentation.
However, despite its promise, deep learning has many challenges in practice due
to its inability to effectively transition to unseen domains, caused by the
inherent data distribution shift and the lack of manual annotations to guide
domain adaptation. To tackle this problem, we present an unsupervised domain
adaptation (UDA) method named AdaptDiff that enables a retinal vessel
segmentation network trained on fundus photography (FP) to produce satisfactory
results on unseen modalities (e.g., OCT-A) without any manual labels. For all
our target domains, we first adopt a segmentation model trained on the source
domain to create pseudo-labels. With these pseudo-labels, we train a
conditional semantic diffusion probabilistic model to represent the target
domain distribution. Experimentally, we show that even with low quality
pseudo-labels, the diffusion model can still capture the conditional semantic
information. Subsequently, we sample on the target domain with binary vessel
masks from the source domain to get paired data, i.e., target domain synthetic
images conditioned on the binary vessel map. Finally, we fine-tune the
pre-trained segmentation network using the synthetic paired data to mitigate
the domain gap. We assess the effectiveness of AdaptDiff on seven publicly
available datasets across three distinct modalities. Our results demonstrate a
significant improvement in segmentation performance across all unseen datasets.
Our code is publicly available at https://github.com/DeweiHu/AdaptDiff.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mode-GS: Monocular Depth Guided Anchored 3D Gaussian Splatting for
  Robust Ground-View Scene Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghan Lee, Jaehoon Choi, Dongki Jung, Jaeseong Yun, Soohyun Ryu, Dinesh Manocha, Suyong Yeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel-view rendering algorithm, Mode-GS, for ground-robot
trajectory datasets. Our approach is based on using anchored Gaussian splats,
which are designed to overcome the limitations of existing 3D Gaussian
splatting algorithms. Prior neural rendering methods suffer from severe splat
drift due to scene complexity and insufficient multi-view observation, and can
fail to fix splats on the true geometry in ground-robot datasets. Our method
integrates pixel-aligned anchors from monocular depths and generates Gaussian
splats around these anchors using residual-form Gaussian decoders. To address
the inherent scale ambiguity of monocular depth, we parameterize anchors with
per-view depth-scales and employ scale-consistent depth loss for online scale
calibration. Our method results in improved rendering performance, based on
PSNR, SSIM, and LPIPS metrics, in ground scenes with free trajectory patterns,
and achieves state-of-the-art rendering performance on the R3LIVE odometry
dataset and the Tanks and Temples dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Tiered Self-Contrastive Learning for Medical Microwave Radiometry
  (MWR) Breast Cancer Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoforos Galazis, Huiyi Wu, Igor Goryanin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pursuit of enhanced breast cancer detection and monitoring techniques is
a paramount healthcare objective, driving the need for innovative imaging
technologies and diagnostic approaches. This study introduces a novel
multi-tiered self-contrastive model tailored for the application of microwave
radiometry (MWR) breast cancer detection. Our approach encompasses three
distinct models: Local-MWR (L-MWR), Regional-MWR (R-MWR), and Global-MWR
(G-MWR), each engineered to analyze varying sub-regional comparisons within the
breasts. These models are cohesively integrated through the Joint-MWR (J-MWR)
network, which leverages the self-contrastive data generated at each analytical
level to enhance detection capabilities. Employing a dataset comprising 4,932
cases of female patients, our research showcases the effectiveness of our
proposed models. Notably, the J-MWR model distinguishes itself by achieving a
Matthews correlation coefficient of 0.74 $\pm$ 0.018, surpassing existing MWR
neural networks and contrastive methods. These results highlight the
significant potential of self-contrastive learning techniques in improving both
the diagnostic accuracy and generalizability of MWR-based breast cancer
detection processes. Such advancements hold considerable promise for further
investigative and clinical endeavors. The source code is available at:
https://github.com/cgalaz01/self_contrastive_mwr
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is What You Ask For What You Get? Investigating Concept Associations in
  Text-to-Image Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salma Abdel Magid, Weiwei Pan, Simon Warchol, Grace Guo, Junsik Kim, Mahia Rahman, Hanspeter Pfister
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) models are increasingly used in impactful real-life
applications. As such, there is a growing need to audit these models to ensure
that they generate desirable, task-appropriate images. However, systematically
inspecting the associations between prompts and generated content in a
human-understandable way remains challenging. To address this, we propose
\emph{Concept2Concept}, a framework where we characterize conditional
distributions of vision language models using interpretable concepts and
metrics that can be defined in terms of these concepts. This characterization
allows us to use our framework to audit models and prompt-datasets. To
demonstrate, we investigate several case studies of conditional distributions
of prompts, such as user defined distributions or empirical, real world
distributions. Lastly, we implement Concept2Concept as an open-source
interactive visualization tool facilitating use by non-technical end-users.
  Warning: This paper contains discussions of harmful content, including CSAM
and NSFW material, which may be disturbing to some readers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Unsupervised Blind Face Restoration using Diffusion Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianshu Kuai, Sina Honari, Igor Gilitschenski, Alex Levinshtein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blind face restoration methods have shown remarkable performance,
particularly when trained on large-scale synthetic datasets with supervised
learning. These datasets are often generated by simulating low-quality face
images with a handcrafted image degradation pipeline. The models trained on
such synthetic degradations, however, cannot deal with inputs of unseen
degradations. In this paper, we address this issue by using only a set of input
images, with unknown degradations and without ground truth targets, to
fine-tune a restoration model that learns to map them to clean and contextually
consistent outputs. We utilize a pre-trained diffusion model as a generative
prior through which we generate high quality images from the natural image
distribution while maintaining the input image content through consistency
constraints. These generated images are then used as pseudo targets to
fine-tune a pre-trained restoration model. Unlike many recent approaches that
employ diffusion models at test time, we only do so during training and thus
maintain an efficient inference-time performance. Extensive experiments show
that the proposed approach can consistently improve the perceptual quality of
pre-trained blind face restoration models while maintaining great consistency
with the input contents. Our best model also achieves the state-of-the-art
results on both synthetic and real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://dt-bfr.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VISTA: A Visual and Textual Attention <span class="highlight-title">Dataset</span> for Interpreting
  Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Harshit, Tolga Tasdizen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent developments in deep learning led to the integration of natural
language processing (NLP) with computer vision, resulting in powerful
integrated Vision and Language Models (VLMs). Despite their remarkable
capabilities, these models are frequently regarded as black boxes within the
machine learning research community. This raises a critical question: which
parts of an image correspond to specific segments of text, and how can we
decipher these associations? Understanding these connections is essential for
enhancing model transparency, interpretability, and trustworthiness. To answer
this question, we present an image-text aligned human visual attention dataset
that maps specific associations between image regions and corresponding text
segments. We then compare the internal heatmaps generated by VL models with
this dataset, allowing us to analyze and better understand the model's
decision-making process. This approach aims to enhance model transparency,
interpretability, and trustworthiness by providing insights into how these
models align visual and linguistic information. We conducted a comprehensive
study on text-guided visual saliency detection in these VL models. This study
aims to understand how different models prioritize and focus on specific visual
elements in response to corresponding text segments, providing deeper insights
into their internal mechanisms and improving our ability to interpret their
outputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing 3D Human Pose Estimation Amidst Severe Occlusion with Dual
  <span class="highlight-title">Transformer</span> Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehwish Ghafoor, Arif Mahmood, Muhammad Bilal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of 3D Human Pose Estimation from monocular videos, the presence
of diverse occlusion types presents a formidable challenge. Prior research has
made progress by harnessing spatial and temporal cues to infer 3D poses from 2D
joint observations. This paper introduces a Dual Transformer Fusion (DTF)
algorithm, a novel approach to obtain a holistic 3D pose estimation, even in
the presence of severe occlusions. Confronting the issue of occlusion-induced
missing joint data, we propose a temporal interpolation-based occlusion
guidance mechanism. To enable precise 3D Human Pose Estimation, our approach
leverages the innovative DTF architecture, which first generates a pair of
intermediate views. Each intermediate-view undergoes spatial refinement through
a self-refinement schema. Subsequently, these intermediate-views are fused to
yield the final 3D human pose estimation. The entire system is end-to-end
trainable. Through extensive experiments conducted on the Human3.6M and
MPI-INF-3DHP datasets, our method's performance is rigorously evaluated.
Notably, our approach outperforms existing state-of-the-art methods on both
datasets, yielding substantial improvements. The code is available here:
https://github.com/MehwishG/DTF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning De-Biased Representations for Remote-Sensing Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Tian, Zhaozheng Chen, Qianru Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing (RS) imagery, requiring specialized satellites to collect and
being difficult to annotate, suffers from data scarcity and class imbalance in
certain spectrums. Due to data scarcity, training any large-scale RS models
from scratch is unrealistic, and the alternative is to transfer pre-trained
models by fine-tuning or a more data-efficient method LoRA. Due to class
imbalance, transferred models exhibit strong bias, where features of the major
class dominate over those of the minor class. In this paper, we propose
debLoRA, a generic training approach that works with any LoRA variants to yield
debiased features. It is an unsupervised learning approach that can diversify
minor class features based on the shared attributes with major classes, where
the attributes are obtained by a simple step of clustering. To evaluate it, we
conduct extensive experiments in two transfer learning scenarios in the RS
domain: from natural to optical RS images, and from optical RS to
multi-spectrum RS images. We perform object classification and oriented object
detection tasks on the optical RS dataset DOTA and the SAR dataset FUSRS.
Results show that our debLoRA consistently surpasses prior arts across these RS
adaptation settings, yielding up to 3.3 and 4.7 percentage points gains on the
tail classes for natural to optical RS and optical RS to multi-spectrum RS
adaptations, respectively, while preserving the performance on head classes,
substantiating its efficacy and adaptability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniMuMo: Unified Text, Music and Motion Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Yang, Kun Su, Yutong Zhang, Jiaben Chen, Kaizhi Qian, Gaowen Liu, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce UniMuMo, a unified multimodal model capable of taking arbitrary
text, music, and motion data as input conditions to generate outputs across all
three modalities. To address the lack of time-synchronized data, we align
unpaired music and motion data based on rhythmic patterns to leverage existing
large-scale music-only and motion-only datasets. By converting music, motion,
and text into token-based representation, our model bridges these modalities
through a unified encoder-decoder transformer architecture. To support multiple
generation tasks within a single framework, we introduce several architectural
improvements. We propose encoding motion with a music codebook, mapping motion
into the same feature space as music. We introduce a music-motion parallel
generation scheme that unifies all music and motion generation tasks into a
single transformer decoder architecture with a single training task of
music-motion joint generation. Moreover, the model is designed by fine-tuning
existing pre-trained single-modality models, significantly reducing
computational demands. Extensive experiments demonstrate that UniMuMo achieves
competitive results on all unidirectional generation benchmarks across music,
motion, and text modalities. Quantitative results are available in the
\href{https://hanyangclarence.github.io/unimumo_demo/}{project page}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-Place Panoptic Radiance Field Segmentation with Perceptual Prior for
  3D Scene Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenghao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate 3D scene representation and panoptic understanding are essential for
applications such as virtual reality, robotics, and autonomous driving.
However, challenges persist with existing methods, including precise 2D-to-3D
mapping, handling complex scene characteristics like boundary ambiguity and
varying scales, and mitigating noise in panoptic pseudo-labels. This paper
introduces a novel perceptual-prior-guided 3D scene representation and panoptic
understanding method, which reformulates panoptic understanding within neural
radiance fields as a linear assignment problem involving 2D semantics and
instance recognition. Perceptual information from pre-trained 2D panoptic
segmentation models is incorporated as prior guidance, thereby synchronizing
the learning processes of appearance, geometry, and panoptic understanding
within neural radiance fields. An implicit scene representation and
understanding model is developed to enhance generalization across indoor and
outdoor scenes by extending the scale-encoded cascaded grids within a
reparameterized domain distillation framework. This model effectively manages
complex scene attributes and generates 3D-consistent scene representations and
panoptic understanding outcomes for various scenes. Experiments and ablation
studies under challenging conditions, including synthetic and real-world
scenes, demonstrate the proposed method's effectiveness in enhancing 3D scene
representation and panoptic segmentation accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Look Around and Find Out: OOD Detection with Relative Angles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Berker Demirel, Marco Fumero, Francesco Locatello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning systems deployed in real-world applications often encounter
data that is different from their in-distribution (ID). A reliable system
should ideally abstain from making decisions in this out-of-distribution (OOD)
setting. Existing state-of-the-art methods primarily focus on feature
distances, such as k-th nearest neighbors and distances to decision boundaries,
either overlooking or ineffectively using in-distribution statistics. In this
work, we propose a novel angle-based metric for OOD detection that is computed
relative to the in-distribution structure. We demonstrate that the angles
between feature representations and decision boundaries, viewed from the mean
of in-distribution features, serve as an effective discriminative factor
between ID and OOD data. Our method achieves state-of-the-art performance on
CIFAR-10 and ImageNet benchmarks, reducing FPR95 by 0.88% and 7.74%
respectively. Our score function is compatible with existing feature space
regularization techniques, enhancing performance. Additionally, its
scale-invariance property enables creating an ensemble of models for OOD
detection via simple score summation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MC-CoT: A Modular Collaborative CoT Framework for Zero-shot Medical-VQA
  with LLM and MLLM Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lai Wei, Wenkai Wang, Xiaoyu Shen, Yu Xie, Zhihao Fan, Xiaojin Zhang, Zhongyu Wei, Wei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent advancements, multimodal large language models (MLLMs) have been
fine-tuned on specific medical image datasets to address medical visual
question answering (Med-VQA) tasks. However, this common approach of
task-specific fine-tuning is costly and necessitates separate models for each
downstream task, limiting the exploration of zero-shot capabilities. In this
paper, we introduce MC-CoT, a modular cross-modal collaboration
Chain-of-Thought (CoT) framework designed to enhance the zero-shot performance
of MLLMs in Med-VQA by leveraging large language models (LLMs). MC-CoT improves
reasoning and information extraction by integrating medical knowledge and
task-specific guidance, where LLM provides various complex medical reasoning
chains and MLLM provides various observations of medical images based on
instructions of the LLM. Our experiments on datasets such as SLAKE, VQA-RAD,
and PATH-VQA show that MC-CoT surpasses standalone MLLMs and various
multimodality CoT frameworks in recall rate and accuracy. These findings
highlight the importance of incorporating background information and detailed
guidance in addressing complex zero-shot Med-VQA tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 14 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object
  Hallucination <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Gong, Tianshi Ming, Xinpeng Wang, Zhihua Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the great success of Large Vision-Language Models (LVLMs), they
inevitably suffer from hallucination. As we know, both the visual encoder and
the Large Language Model (LLM) decoder in LVLMs are Transformer-based, allowing
the model to extract visual information and generate text outputs via attention
mechanisms. We find that the attention distribution of LLM decoder on image
tokens is highly consistent with the visual encoder and both distributions tend
to focus on particular background tokens rather than the referred objects in
the image. We attribute to the unexpected attention distribution to an inherent
flaw in the visual encoder itself, which misguides LLMs to over emphasize the
redundant information and generate object hallucination. To address the issue,
we propose DAMRO, a novel training-free strategy that $D$ive into $A$ttention
$M$echanism of LVLM to $R$educe $O$bject Hallucination. Specifically, our
approach employs classification token (CLS) of ViT to filter out high-attention
outlier tokens scattered in the background and then eliminate their influence
during decoding stage. We evaluate our method on LVLMs including LLaVA-1.5,
LLaVA-NeXT and InstructBLIP, using various benchmarks such as POPE, CHAIR, MME
and GPT-4V Aided Evaluation. The results demonstrate that our approach
significantly reduces the impact of these outlier tokens, thus effectively
alleviating the hallucination of LVLMs. The code of our method will be released
soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Realizing Video Summarization from the Path of Language-based Semantic
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuan-Chen Mu, Zhi-Yi Chin, Wei-Chen Chiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent development of Video-based Large Language Models (VideoLLMs), has
significantly advanced video summarization by aligning video features and, in
some cases, audio features with Large Language Models (LLMs). Each of these
VideoLLMs possesses unique strengths and weaknesses. Many recent methods have
required extensive fine-tuning to overcome the limitations of these models,
which can be resource-intensive. In this work, we observe that the strengths of
one VideoLLM can complement the weaknesses of another. Leveraging this insight,
we propose a novel video summarization framework inspired by the Mixture of
Experts (MoE) paradigm, which operates as an inference-time algorithm without
requiring any form of fine-tuning. Our approach integrates multiple VideoLLMs
to generate comprehensive and coherent textual summaries. It effectively
combines visual and audio content, provides detailed background descriptions,
and excels at identifying keyframes, which enables more semantically meaningful
retrieval compared to traditional computer vision approaches that rely solely
on visual information, all without the need for additional fine-tuning.
Moreover, the resulting summaries enhance performance in downstream tasks such
as summary video generation, either through keyframe selection or in
combination with text-to-image models. Our language-driven approach offers a
semantically rich alternative to conventional methods and provides flexibility
to incorporate newer VideoLLMs, enhancing adaptability and performance in video
summarization tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MECFormer: Multi-task Whole Slide Image Classification with Expert
  Consultation Network <span class="chip">ACCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Doanh C. Bui, Jin Tae Kwak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole slide image (WSI) classification is a crucial problem for cancer
diagnostics in clinics and hospitals. A WSI, acquired at gigapixel size, is
commonly tiled into patches and processed by multiple-instance learning (MIL)
models. Previous MIL-based models designed for this problem have only been
evaluated on individual tasks for specific organs, and the ability to handle
multiple tasks within a single model has not been investigated. In this study,
we propose MECFormer, a generative Transformer-based model designed to handle
multiple tasks within one model. To leverage the power of learning multiple
tasks simultaneously and to enhance the model's effectiveness in focusing on
each individual task, we introduce an Expert Consultation Network, a projection
layer placed at the beginning of the Transformer-based model. Additionally, to
enable flexible classification, autoregressive decoding is incorporated by a
language decoder for WSI classification. Through extensive experiments on five
datasets involving four different organs, one cancer classification task, and
four cancer subtyping tasks, MECFormer demonstrates superior performance
compared to individual state-of-the-art multiple-instance learning models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at ACCV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizability analysis of deep learning predictions of human brain
  responses to augmented and semantically novel visual stimuli 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentyn Piskovskyi, Riccardo Chimisso, Sabrina Patania, Tom Foulsham, Giuseppe Vizzari, Dimitri Ognibene
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The purpose of this work is to investigate the soundness and utility of a
neural network-based approach as a framework for exploring the impact of image
enhancement techniques on visual cortex activation. In a preliminary study, we
prepare a set of state-of-the-art brain encoding models, selected among the top
10 methods that participated in The Algonauts Project 2023 Challenge [16]. We
analyze their ability to make valid predictions about the effects of various
image enhancement techniques on neural responses. Given the impossibility of
acquiring the actual data due to the high costs associated with brain imaging
procedures, our investigation builds up on a series of experiments.
Specifically, we analyze the ability of brain encoders to estimate the cerebral
reaction to various augmentations by evaluating the response to augmentations
targeting objects (i.e., faces and words) with known impact on specific areas.
Moreover, we study the predicted activation in response to objects unseen
during training, exploring the impact of semantically out-of-distribution
stimuli. We provide relevant evidence for the generalization ability of the
models forming the proposed framework, which appears to be promising for the
identification of the optimal visual augmentation filter for a given task,
model-driven design strategies as well as for AR and VR applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpret Your Decision: Logical Reasoning Regularization for
  Generalization in Visual Classification <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaorui Tan, Xi Yang, Qiufeng Wang, Anh Nguyen, Kaizhu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision models excel in image classification but struggle to generalize to
unseen data, such as classifying images from unseen domains or discovering
novel categories. In this paper, we explore the relationship between logical
reasoning and deep learning generalization in visual classification. A logical
regularization termed L-Reg is derived which bridges a logical analysis
framework to image classification. Our work reveals that L-Reg reduces the
complexity of the model in terms of the feature distribution and classifier
weights. Specifically, we unveil the interpretability brought by L-Reg, as it
enables the model to extract the salient features, such as faces to persons,
for classification. Theoretical analysis and experiments demonstrate that L-Reg
enhances generalization across various scenarios, including multi-domain
generalization and generalized category discovery. In complex real-world
scenarios where images span unknown classes and unseen domains, L-Reg
consistently improves generalization, highlighting its practical efficacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024 as Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SITCOM: Step-wise Triple-Consistent Diffusion Sampling for Inverse
  Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ismail Alkhouri, Shijun Liang, Cheng-Han Huang, Jimmy Dai, Qing Qu, Saiprasad Ravishankar, Rongrong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) are a class of generative models that allow sampling
from a distribution learned over a training set. When applied to solving
inverse imaging problems (IPs), the reverse sampling steps of DMs are typically
modified to approximately sample from a measurement-conditioned distribution in
the image space. However, these modifications may be unsuitable for certain
settings (such as in the presence of measurement noise) and non-linear tasks,
as they often struggle to correct errors from earlier sampling steps and
generally require a large number of optimization and/or sampling steps. To
address these challenges, we state three conditions for achieving
measurement-consistent diffusion trajectories. Building on these conditions, we
propose a new optimization-based sampling method that not only enforces the
standard data manifold measurement consistency and forward diffusion
consistency, as seen in previous studies, but also incorporates backward
diffusion consistency that maintains a diffusion trajectory by optimizing over
the input of the pre-trained model at every sampling step. By enforcing these
conditions, either implicitly or explicitly, our sampler requires significantly
fewer reverse steps. Therefore, we refer to our accelerated method as Step-wise
Triple-Consistent Sampling (SITCOM). Compared to existing state-of-the-art
baseline methods, under different levels of measurement noise, our extensive
experiments across five linear and three non-linear image restoration tasks
demonstrate that SITCOM achieves competitive or superior results in terms of
standard image similarity metrics while requiring a significantly reduced
run-time across all considered tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tensor-Train Point Cloud Compression and Efficient Approximate
  Nearest-Neighbor Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgii Novikov, Alexander Gneushev, Alexey Kadeishvili, Ivan Oseledets
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nearest-neighbor search in large vector databases is crucial for various
machine learning applications. This paper introduces a novel method using
tensor-train (TT) low-rank tensor decomposition to efficiently represent point
clouds and enable fast approximate nearest-neighbor searches. We propose a
probabilistic interpretation and utilize density estimation losses like Sliced
Wasserstein to train TT decompositions, resulting in robust point cloud
compression. We reveal an inherent hierarchical structure within TT point
clouds, facilitating efficient approximate nearest-neighbor searches. In our
paper, we provide detailed insights into the methodology and conduct
comprehensive comparisons with existing methods. We demonstrate its
effectiveness in various scenarios, including out-of-distribution (OOD)
detection problems and approximate nearest-neighbor (ANN) search tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-net based prediction of cerebrospinal fluid distribution and
  ventricular reflux grading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melanie Rieff, Fabian Holzberger, Oksana Lapina, Geir Ringstad, Lars Magnus Valnes, Bogna Warsza, Kent-Andre Mardal, Per Kristian Eide, Barbara Wohlmuth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous work shows evidence that cerebrospinal fluid (CSF) plays a crucial
role in brain waste clearance processes, and that altered flow patterns are
associated with various diseases of the central nervous system. In this study,
we investigate the potential of deep learning to predict the distribution in
human brain of a gadolinium-based CSF contrast agent (tracer) administered
intrathecal. For this, T1-weighted magnetic resonance imaging (MRI) scans taken
at multiple time points before and after intrathecal injection were utilized.
We propose a U-net-based supervised learning model to predict pixel-wise signal
increases at their peak after 24 hours. Its performance is evaluated based on
different tracer distribution stages provided during training, including
predictions from baseline scans taken before injection. Our findings indicate
that using imaging data from just the first two hours post-injection for
training yields tracer flow predictions comparable to those trained with
additional later-stage scans. The model was further validated by comparing
ventricular reflux gradings provided by neuroradiologists, and inter-rater
grading among medical experts and the model showed excellent agreement. Our
results demonstrate the potential of deep learning-based methods for CSF flow
prediction, suggesting that fewer MRI scans could be sufficient for clinical
analysis, which might significantly improve clinical efficiency, patient
well-being, and lower healthcare costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video Summarization Techniques: A Comprehensive <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toqa Alaa, Ahmad Mongy, Assem Bakr, Mariam Diab, Walid Gomaa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid expansion of video content across a variety of industries,
including social media, education, entertainment, and surveillance, has made
video summarization an essential field of study. The current work is a survey
that explores the various approaches and methods created for video summarizing,
emphasizing both abstractive and extractive strategies. The process of
extractive summarization involves the identification of key frames or segments
from the source video, utilizing methods such as shot boundary recognition, and
clustering. On the other hand, abstractive summarization creates new content by
getting the essential content from the video, using machine learning models
like deep neural networks and natural language processing, reinforcement
learning, attention mechanisms, generative adversarial networks, and
multi-modal learning. We also include approaches that incorporate the two
methodologies, along with discussing the uses and difficulties encountered in
real-world implementations. The paper also covers the datasets used to
benchmark these techniques. This review attempts to provide a state-of-the-art
thorough knowledge of the current state and future directions of video
summarization research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention Shift: Steering AI Away from Unsafe Content 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivank Garg, Manyana Tiwari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the generation of unsafe or harmful content in
state-of-the-art generative models, focusing on methods for restricting such
generations. We introduce a novel training-free approach using attention
reweighing to remove unsafe concepts without additional training during
inference. We compare our method against existing ablation methods, evaluating
the performance on both, direct and adversarial jailbreak prompts, using
qualitative and quantitative metrics. We hypothesize potential reasons for the
observed results and discuss the limitations and broader implications of
content restriction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimising for the Unknown: Domain Alignment for Cephalometric Landmark
  Detection <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Wyatt, Irina Voiculescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cephalometric Landmark Detection is the process of identifying key areas for
cephalometry. Each landmark is a single GT point labelled by a clinician. A
machine learning model predicts the probability locus of a landmark represented
by a heatmap. This work, for the 2024 CL-Detection MICCAI Challenge, proposes a
domain alignment strategy with a regional facial extraction module and an X-ray
artefact augmentation procedure. The challenge ranks our method's results as
the best in MRE of 1.186mm and third in the 2mm SDR of 82.04% on the online
validation leaderboard. The code is available at
https://github.com/Julian-Wyatt/OptimisingfortheUnknown.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI CL-Detection2024: Cephalometric Landmark Detection in Lateral
  X-ray Images</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Detection of Defects on Metal Surfaces using Vision
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toqa Alaa, Mostafa Kotb, Arwa Zakaria, Mariam Diab, Walid Gomaa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metal manufacturing often results in the production of defective products,
leading to operational challenges. Since traditional manual inspection is
time-consuming and resource-intensive, automatic solutions are needed. The
study utilizes deep learning techniques to develop a model for detecting metal
surface defects using Vision Transformers (ViTs). The proposed model focuses on
the classification and localization of defects using a ViT for feature
extraction. The architecture branches into two paths: classification and
localization. The model must approach high classification accuracy while
keeping the Mean Square Error (MSE) and Mean Absolute Error (MAE) as low as
possible in the localization process. Experimental results show that it can be
utilized in the process of automated defects detection, improve operational
efficiency, and reduce errors in metal manufacturing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empowering Backbone Models for Visual Text Generation with Input
  Granularity Control and Glyph-Aware Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbo Li, Guohao Li, Zhibin Lan, Xue Xu, Wanru Zhuang, Jiachen Liu, Xinyan Xiao, Jinsong Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based text-to-image models have demonstrated impressive
achievements in diversity and aesthetics but struggle to generate images with
legible visual texts. Existing backbone models have limitations such as
misspelling, failing to generate texts, and lack of support for Chinese text,
but their development shows promising potential. In this paper, we propose a
series of methods, aiming to empower backbone models to generate visual texts
in English and Chinese. We first conduct a preliminary study revealing that
Byte Pair Encoding (BPE) tokenization and the insufficient learning of
cross-attention modules restrict the performance of the backbone models. Based
on these observations, we make the following improvements: (1) We design a
mixed granularity input strategy to provide more suitable text representations;
(2) We propose to augment the conventional training objective with three
glyph-aware training losses, which enhance the learning of cross-attention
modules and encourage the model to focus on visual texts. Through experiments,
we demonstrate that our methods can effectively empower backbone models to
generate semantic relevant, aesthetically appealing, and accurate visual text
images, while maintaining their fundamental image generation quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Mathematical Explanation of UNet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xue-Cheng Tai, Hao Liu, Raymond H. Chan, Lingfeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The UNet architecture has transformed image segmentation. UNet's versatility
and accuracy have driven its widespread adoption, significantly advancing
fields reliant on machine learning problems with images. In this work, we give
a clear and concise mathematical explanation of UNet. We explain what is the
meaning and function of each of the components of UNet. We will show that UNet
is solving a control problem. We decompose the control variables using
multigrid methods. Then, operator-splitting techniques is used to solve the
problem, whose architecture exactly recovers the UNet architecture. Our result
shows that UNet is a one-step operator-splitting algorithm for the control
problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAPEEN: Image Captioning with Early Exits and Knowledge Distillation <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divya Jyoti Bajpai, Manjesh Kumar Hanawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) have made significant progress in recognizing
visual elements and generating descriptive text in image-captioning tasks.
However, their improved performance comes from increased computational burden
and inference latency. Early Exit (EE) strategies can be used to enhance their
efficiency, but their adaptation presents challenges in image captioning as it
requires varying levels of semantic information for accurate predictions. To
overcome this, we introduce CAPEEN to improve the performance of EE strategies
using knowledge distillation. Inference in CAPEEN is completed at intermediary
layers if prediction confidence exceeds a predefined value learned from the
training data. To account for real-world deployments, where target
distributions could drift from that of training samples, we introduce a variant
A-CAPEEN to adapt the thresholds on the fly using Multiarmed bandits framework.
Experiments on the MS COCO and Flickr30k datasets show that CAPEEN gains
speedup of 1.77x while maintaining competitive performance compared to the
final layer, and A-CAPEEN additionally offers robustness against distortions.
The source code is available at https://github.com/Div290/CapEEN
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in EMNLP (finding) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoVLM: Leveraging Consensus from Vision-Language Models for
  Semi-supervised Multi-modal Fake News Detection <span class="chip">ACCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Devank, Jayateja Kalla, Soma Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we address the real-world, challenging task of out-of-context
misinformation detection, where a real image is paired with an incorrect
caption for creating fake news. Existing approaches for this task assume the
availability of large amounts of labeled data, which is often impractical in
real-world, since it requires extensive manual intervention and domain
expertise. In contrast, since obtaining a large corpus of unlabeled image-text
pairs is much easier, here, we propose a semi-supervised protocol, where the
model has access to a limited number of labeled image-text pairs and a large
corpus of unlabeled pairs. Additionally, the occurrence of fake news being much
lesser compared to the real ones, the datasets tend to be highly imbalanced,
thus making the task even more challenging. Towards this goal, we propose a
novel framework, Consensus from Vision-Language Models (CoVLM), which generates
robust pseudo-labels for unlabeled pairs using thresholds derived from the
labeled data. This approach can automatically determine the right threshold
parameters of the model for selecting the confident pseudo-labels. Experimental
results on benchmark datasets across challenging conditions and comparisons
with state-of-the-art approaches demonstrate the effectiveness of our
framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangling Regional Primitives for Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengting Chen, Lei Cheng, Lianghui Ding, Quanshi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a method to explain the internal representation structure
of a neural network for image generation. Specifically, our method disentangles
primitive feature components from the intermediate-layer feature of the neural
network, which ensures that each feature component is exclusively used to
generate a specific set of image regions. In this way, the generation of the
entire image can be considered as the superposition of different pre-encoded
primitive regional patterns, each being generated by a feature component. We
find that the feature component can be represented as an OR relationship
between the demands for generating different image regions, which is encoded by
the neural network. Therefore, we extend the Harsanyi interaction to represent
such an OR interaction to disentangle the feature component. Experiments show a
clear correspondence between each feature component and the generation of
specific image regions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiteVLoc: Map-Lite Visual Localization for Image Goal Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhao Jiao, Jinhao He, Changkun Liu, Sebastian Aegidius, Xiangcheng Hu, Tristan Braud, Dimitrios Kanoulas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents LiteVLoc, a hierarchical visual localization framework
that uses a lightweight topo-metric map to represent the environment. The
method consists of three sequential modules that estimate camera poses in a
coarse-to-fine manner. Unlike mainstream approaches relying on detailed 3D
representations, LiteVLoc reduces storage overhead by leveraging learning-based
feature matching and geometric solvers for metric pose estimation. A novel
dataset for the map-free relocalization task is also introduced. Extensive
experiments including localization and navigation in both simulated and
real-world scenarios have validate the system's performance and demonstrated
its precision and efficiency for large-scale deployment. Code and data will be
made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SparseVLM: Visual Token Sparsification for Efficient Vision-Language
  Model Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, Shanghang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In vision-language models (VLMs), visual tokens usually consume a significant
amount of computational overhead, despite their sparser information density
compared to text tokens. To address this, most existing methods learn a network
to prune redundant visual tokens and require additional training data.
Differently, we propose an efficient training-free token optimization mechanism
dubbed SparseVLM without extra parameters or fine-tuning costs. Concretely,
given that visual tokens complement text tokens in VLMs for linguistic
reasoning, we select visual-relevant text tokens to rate the significance of
vision tokens within the self-attention matrix extracted from the VLMs. Then we
progressively prune irrelevant tokens. To maximize sparsity while retaining
essential information, we introduce a rank-based strategy to adaptively
determine the sparsification ratio for each layer, alongside a token recycling
method that compresses pruned tokens into more compact representations.
Experimental results show that our SparseVLM improves the efficiency of various
VLMs across a range of image and video understanding tasks. In particular,
LLaVA equipped with SparseVLM reduces 61% to 67% FLOPs with a compression ratio
of 78% while maintaining 93% of the accuracy. Our code is available at
https://github.com/Gumpest/SparseVLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deformable NeRF using Recursively Subdivided Tetrahedra 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zherui Qiu, Chenqu Ren, Kaiwen Song, Xiaoyi Zeng, Leyuan Yang, Juyong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While neural radiance fields (NeRF) have shown promise in novel view
synthesis, their implicit representation limits explicit control over object
manipulation. Existing research has proposed the integration of explicit
geometric proxies to enable deformation. However, these methods face two
primary challenges: firstly, the time-consuming and computationally demanding
tetrahedralization process; and secondly, handling complex or thin structures
often leads to either excessive, storage-intensive tetrahedral meshes or
poor-quality ones that impair deformation capabilities. To address these
challenges, we propose DeformRF, a method that seamlessly integrates the
manipulability of tetrahedral meshes with the high-quality rendering
capabilities of feature grid representations. To avoid ill-shaped tetrahedra
and tetrahedralization for each object, we propose a two-stage training
strategy. Starting with an almost-regular tetrahedral grid, our model initially
retains key tetrahedra surrounding the object and subsequently refines object
details using finer-granularity mesh in the second stage. We also present the
concept of recursively subdivided tetrahedra to create higher-resolution meshes
implicitly. This enables multi-resolution encoding while only necessitating the
storage of the coarse tetrahedral mesh generated in the first training stage.
We conduct a comprehensive evaluation of our DeformRF on both synthetic and
real-captured datasets. Both quantitative and qualitative results demonstrate
the effectiveness of our method for novel view synthesis and deformation tasks.
Project page: https://ustc3dv.github.io/DeformRF/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM Multimedia 2024. Project Page:
  https://ustc3dv.github.io/DeformRF/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffusionFake: Enhancing Generalization in Deepfake Detection via Guided
  Stable Diffusion <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Sun, Shen Chen, Taiping Yao, Hong Liu, Xiaoshuai Sun, Shouhong Ding, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid progress of Deepfake technology has made face swapping highly
realistic, raising concerns about the malicious use of fabricated facial
content. Existing methods often struggle to generalize to unseen domains due to
the diverse nature of facial manipulations. In this paper, we revisit the
generation process and identify a universal principle: Deepfake images
inherently contain information from both source and target identities, while
genuine faces maintain a consistent identity. Building upon this insight, we
introduce DiffusionFake, a novel plug-and-play framework that reverses the
generative process of face forgeries to enhance the generalization of detection
models. DiffusionFake achieves this by injecting the features extracted by the
detection model into a frozen pre-trained Stable Diffusion model, compelling it
to reconstruct the corresponding target and source images. This guided
reconstruction process constrains the detection network to capture the source
and target related features to facilitate the reconstruction, thereby learning
rich and disentangled representations that are more resilient to unseen
forgeries. Extensive experiments demonstrate that DiffusionFake significantly
improves cross-domain generalization of various detector architectures without
introducing additional parameters during inference. Our Codes are available in
https://github.com/skJack/DiffusionFake.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Efficient Language and Vision Assistants for Visually-Situated
  Natural Language Understanding: What Matters in Reading and Reasoning <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geewook Kim, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in language and vision assistants have showcased
impressive capabilities but suffer from a lack of transparency, limiting
broader research and reproducibility. While open-source models handle general
image tasks effectively, they face challenges with the high computational
demands of complex visually-situated text understanding. Such tasks often
require increased token inputs and large vision modules to harness
high-resolution information. Striking a balance between model size and data
importance remains an open question. This study aims to redefine the design of
vision-language models by identifying key components and creating efficient
models with constrained inference costs. By strategically formulating datasets,
optimizing vision modules, and enhancing supervision techniques, we achieve
significant improvements in inference throughput while maintaining high
performance. Extensive experiments across models ranging from 160M to 13B
parameters offer insights into model optimization. We will fully open-source
our codebase, models, and datasets at https://github.com/naver-ai/elva.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a vision foundation model for comprehensive assessment of
  Cardiac MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Athira J Jacob, Indraneel Borgohain, Teodora Chitiboi, Puneet Sharma, Dorin Comaniciu, Daniel Rueckert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiac magnetic resonance imaging (CMR), considered the gold standard for
noninvasive cardiac assessment, is a diverse and complex modality requiring a
wide variety of image processing tasks for comprehensive assessment of cardiac
morphology and function. Advances in deep learning have enabled the development
of state-of-the-art (SoTA) models for these tasks. However, model training is
challenging due to data and label scarcity, especially in the less common
imaging sequences. Moreover, each model is often trained for a specific task,
with no connection between related tasks. In this work, we introduce a vision
foundation model trained for CMR assessment, that is trained in a
self-supervised fashion on 36 million CMR images. We then finetune the model in
supervised way for 9 clinical tasks typical to a CMR workflow, across
classification, segmentation, landmark localization, and pathology detection.
We demonstrate improved accuracy and robustness across all tasks, over a range
of available labeled dataset sizes. We also demonstrate improved few-shot
learning with fewer labeled samples, a common challenge in medical image
analyses. We achieve an out-of-box performance comparable to SoTA for most
clinical tasks. The proposed method thus presents a resource-efficient, unified
framework for CMR assessment, with the potential to accelerate the development
of deep learning-based solutions for image analysis tasks, even with few
annotated data available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Cat Is A Cat (Not A Dog!): Unraveling Information Mix-ups in
  Text-to-Image Encoders through Causal Analysis and Embedding Optimization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00321v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00321v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chieh-Yun Chen, Li-Wu Tsao, Chiang Tseng, Hong-Han Shuai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper analyzes the impact of causal manner in the text encoder of
text-to-image (T2I) diffusion models, which can lead to information bias and
loss. Previous works have focused on addressing the issues through the
denoising process. However, there is no research discussing how text embedding
contributes to T2I models, especially when generating more than one object. In
this paper, we share a comprehensive analysis of text embedding: i) how text
embedding contributes to the generated images and ii) why information gets lost
and biases towards the first-mentioned object. Accordingly, we propose a simple
but effective text embedding balance optimization method, which is
training-free, with an improvement of 90.05% on information balance in stable
diffusion. Furthermore, we propose a new automatic evaluation metric that
quantifies information loss more accurately than existing methods, achieving
81% concordance with human assessments. This metric effectively measures the
presence and accuracy of objects, addressing the limitations of current
distribution scores like CLIP's text-image similarities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KISS-Matcher: Fast and Robust Point Cloud Registration Revisited 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15615v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15615v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyungtae Lim, Daebeom Kim, Gunhee Shin, Jingnan Shi, Ignacio Vizzo, Hyun Myung, Jaesik Park, Luca Carlone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While global point cloud registration systems have advanced significantly in
all aspects, many studies have focused on specific components, such as feature
extraction, graph-theoretic pruning, or pose solvers. In this paper, we take a
holistic view on the registration problem and develop an open-source and
versatile C++ library for point cloud registration, called
\textit{KISS-Matcher}. KISS-Matcher combines a novel feature detector,
\textit{Faster-PFH}, that improves over the classical fast point feature
histogram (FPFH). Moreover, it adopts a $k$-core-based graph-theoretic pruning
to reduce the time complexity of rejecting outlier correspondences. Finally, it
combines these modules in a complete, user-friendly, and ready-to-use pipeline.
As verified by extensive experiments, KISS-Matcher has superior scalability and
broad applicability, achieving a substantial speed-up compared to
state-of-the-art outlier-robust registration pipelines while preserving
accuracy. Our code will be available at
\href{https://github.com/MIT-SPARK/KISS-Matcher}{\texttt{https://github.com/MIT-SPARK/KISS-Matcher}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynCo: Synthetic Hard Negatives in Contrastive Learning for Better
  Unsupervised Visual Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Giakoumoglou, Tania Stathaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has become a dominant approach in self-supervised visual
representation learning. Hard negatives - samples closely resembling the anchor
- are key to enhancing learned representations' discriminative power. However,
efficiently leveraging hard negatives remains challenging. We introduce SynCo
(sYnthetic Negatives in Contrastive learning), a novel approach that improves
model performance by generating synthetic hard negatives on the representation
space. Building on the MoCo framework, SynCo introduces six strategies for
creating diverse synthetic hard negatives on-the-fly with minimal computational
overhead. SynCo achieves faster training and better representation learning,
reaching 67.9% top-1 accuracy on ImageNet ILSVRC-201 linear evaluation after
200 pretraining epochs, surpassing MoCo's 67.5% using the same ResNet-50
encoder. It also transfers more effectively to detection tasks: on PASCAL VOC,
it outperforms both the supervised baseline and MoCo with 82.6% AP; on COCO, it
sets new benchmarks with 41.0% AP for bounding box detection and 35.7% AP for
instance segmentation. Our synthetic hard negative generation approach
significantly enhances visual representations learned through self-supervised
contrastive learning. Code is available at
https://github.com/giakoumoglou/synco.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Switch EMA: A Free Lunch for Better Flatness and Sharpness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09240v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09240v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Li, Zicheng Liu, Juanxi Tian, Ge Wang, Zedong Wang, Weiyang Jin, Di Wu, Cheng Tan, Tao Lin, Yang Liu, Baigui Sun, Stan Z. Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exponential Moving Average (EMA) is a widely used weight averaging (WA)
regularization to learn flat optima for better generalizations without extra
cost in deep neural network (DNN) optimization. Despite achieving better
flatness, existing WA methods might fall into worse final performances or
require extra test-time computations. This work unveils the full potential of
EMA with a single line of modification, i.e., switching the EMA parameters to
the original model after each epoch, dubbed as Switch EMA (SEMA). From both
theoretical and empirical aspects, we demonstrate that SEMA can help DNNs to
reach generalization optima that better trade-off between flatness and
sharpness. To verify the effectiveness of SEMA, we conduct comparison
experiments with discriminative, generative, and regression tasks on vision and
language datasets, including image classification, self-supervised learning,
object detection and segmentation, image generation, video prediction,
attribute regression, and language modeling. Comprehensive results with popular
optimizers and networks show that SEMA is a free lunch for DNN training by
improving performances and boosting convergence speeds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint V2. Source code and models at
  https://github.com/Westlake-AI/SEMA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Product Importance Sampling via Warp Composition <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18974v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18974v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joey Litalien, Miloš Hašan, Fujun Luan, Krishna Mullia, Iliyan Georgiev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving high efficiency in modern photorealistic rendering hinges on using
Monte Carlo sampling distributions that closely approximate the illumination
integral estimated for every pixel. Samples are typically generated from a set
of simple distributions, each targeting a different factor in the integrand,
which are combined via multiple importance sampling. The resulting mixture
distribution can be far from the actual product of all factors, leading to
sub-optimal variance even for direct-illumination estimation. We present a
learning-based method that uses normalizing flows to efficiently importance
sample illumination product integrals, e.g., the product of environment
lighting and material terms. Our sampler composes a flow head warp with an
emitter tail warp. The small conditional head warp is represented by a neural
spline flow, while the large unconditional tail is discretized per environment
map and its evaluation is instant. If the conditioning is low-dimensional, the
head warp can be also discretized to achieve even better performance. We
demonstrate variance reduction over prior methods on a range of applications
comprising complex geometry, materials and illumination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ACM SIGGRAPH Asia 2024 Conference Papers. Project page:
  https://joeylitalien.github.io/publications/warp</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed
  via Gaussian Splatting <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15125v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15125v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhao Cai, Zihao Xiao, Yixun Liang, Minghan Qin, Yulun Zhang, Xiaokang Yang, Yaoyao Liu, Alan Yuille
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High dynamic range (HDR) novel view synthesis (NVS) aims to create
photorealistic images from novel viewpoints using HDR imaging techniques. The
rendered HDR images capture a wider range of brightness levels containing more
details of the scene than normal low dynamic range (LDR) images. Existing HDR
NVS methods are mainly based on NeRF. They suffer from long training time and
slow inference speed. In this paper, we propose a new framework, High Dynamic
Range Gaussian Splatting (HDR-GS), which can efficiently render novel HDR views
and reconstruct LDR images with a user input exposure time. Specifically, we
design a Dual Dynamic Range (DDR) Gaussian point cloud model that uses
spherical harmonics to fit HDR color and employs an MLP-based tone-mapper to
render LDR color. The HDR and LDR colors are then fed into two Parallel
Differentiable Rasterization (PDR) processes to reconstruct HDR and LDR views.
To establish the data foundation for the research of 3D Gaussian
splatting-based methods in HDR NVS, we recalibrate the camera parameters and
compute the initial positions for Gaussian point clouds. Experiments
demonstrate that our HDR-GS surpasses the state-of-the-art NeRF-based method by
3.84 and 1.91 dB on LDR and HDR NVS while enjoying 1000x inference speed and
only requiring 6.3% training time. Code, models, and recalibrated data will be
publicly available at https://github.com/caiyuanhao1998/HDR-GS
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024; The first 3D Gaussian Splatting-based method for HDR
  imaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Famba-V: Fast Vision Mamba with Cross-Layer Token Fusion <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09808v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09808v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Shen, Zhongwei Wan, Xin Wang, Mi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mamba and Vision Mamba (Vim) models have shown their potential as an
alternative to methods based on Transformer architecture. This work introduces
Fast Mamba for Vision (Famba-V), a cross-layer token fusion technique to
enhance the training efficiency of Vim models. The key idea of Famba-V is to
identify and fuse similar tokens across different Vim layers based on a suit of
cross-layer strategies instead of simply applying token fusion uniformly across
all the layers that existing works propose. We evaluate the performance of
Famba-V on CIFAR-100. Our results show that Famba-V is able to enhance the
training efficiency of Vim models by reducing both training time and peak
memory usage during training. Moreover, the proposed cross-layer strategies
allow Famba-V to deliver superior accuracy-efficiency trade-offs. These results
all together demonstrate Famba-V as a promising efficiency enhancement
technique for Vim models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera ready version of ECCV 2024 Workshop on Computational Aspects
  of Deep Learning (Best Paper Award)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A study on the adequacy of common IQA measures for medical images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19224v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19224v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Breger, Clemens Karner, Ian Selby, Janek Gröhl, Sören Dittmer, Edward Lilley, Judith Babar, Jake Beckford, Thomas R Else, Timothy J Sadler, Shahab Shahipasand, Arthikkaa Thavakumar, Michael Roberts, Carola-Bibiane Schönlieb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image quality assessment (IQA) is standard practice in the development stage
of novel machine learning algorithms that operate on images. The most commonly
used IQA measures have been developed and tested for natural images, but not in
the medical setting. Reported inconsistencies arising in medical images are not
surprising, as they have different properties than natural images. In this
study, we test the applicability of common IQA measures for medical image data
by comparing their assessment to manually rated chest X-ray (5 experts) and
photoacoustic image data (2 experts). Moreover, we include supplementary
studies on grayscale natural images and accelerated brain MRI data. The results
of all experiments show a similar outcome in line with previous findings for
medical images: PSNR and SSIM in the default setting are in the lower range of
the result list and HaarPSI outperforms the other tested measures in the
overall performance. Also among the top performers in our medical experiments
are the full reference measures FSIM, LPIPS and MS-SSIM. Generally, the results
on natural images yield considerably higher correlations, suggesting that
additional employment of tailored IQA measures for medical imaging algorithms
is needed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transferable Tactile <span class="highlight-title">Transformer</span>s for Representation Learning Across
  Diverse Sensors and Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13640v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13640v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialiang Zhao, Yuxiang Ma, Lirui Wang, Edward H. Adelson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents T3: Transferable Tactile Transformers, a framework for
tactile representation learning that scales across multi-sensors and
multi-tasks. T3 is designed to overcome the contemporary issue that
camera-based tactile sensing is extremely heterogeneous, i.e. sensors are built
into different form factors, and existing datasets were collected for disparate
tasks. T3 captures the shared latent information across different sensor-task
pairings by constructing a shared trunk transformer with sensor-specific
encoders and task-specific decoders. The pre-training of T3 utilizes a novel
Foundation Tactile (FoTa) dataset, which is aggregated from several
open-sourced datasets and it contains over 3 million data points gathered from
13 sensors and 11 tasks. FoTa is the largest and most diverse dataset in
tactile sensing to date and it is made publicly available in a unified format.
Across various sensors and tasks, experiments show that T3 pre-trained with
FoTa achieved zero-shot transferability in certain sensor-task pairings, can be
further fine-tuned with small amounts of domain-specific data, and its
performance scales with bigger network sizes. T3 is also effective as a tactile
encoder for long horizon contact-rich manipulation. Results from sub-millimeter
multi-pin electronics insertion tasks show that T3 achieved a task success rate
25% higher than that of policies trained with tactile encoders trained from
scratch, or 53% higher than without tactile sensing. Data, code, and model
checkpoints are open-sourced at https://t3.alanz.info
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 2024 Conference on Robot Learning (CoRL)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GUing: A Mobile GUI Search Engine using a Vision-Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00145v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00145v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialiang Wei, Anne-Lise Courbis, Thomas Lambolais, Binbin Xu, Pierre Louis Bernard, Gérard Dray, Walid Maalej
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphical User Interfaces (GUIs) are central to app development projects. App
developers may use the GUIs of other apps as a means of requirements refinement
and rapid prototyping or as a source of inspiration for designing and improving
their own apps. Recent research has thus suggested retrieving relevant GUI
designs that match a certain text query from screenshot datasets acquired
through crowdsourced or automated exploration of GUIs. However, such
text-to-GUI retrieval approaches only leverage the textual information of the
GUI elements, neglecting visual information such as icons or background images.
In addition, retrieved screenshots are not steered by app developers and lack
app features that require particular input data.
  To overcome these limitations, this paper proposes GUing, a GUI search engine
based on a vision-language model called GUIClip, which we trained specifically
for the problem of designing app GUIs. For this, we first collected from Google
Play app introduction images which display the most representative screenshots
and are often captioned (i.e.~labelled) by app vendors. Then, we developed an
automated pipeline to classify, crop, and extract the captions from these
images. This resulted in a large dataset which we share with this paper:
including 303k app screenshots, out of which 135k have captions. We used this
dataset to train a novel vision-language model, which is, to the best of our
knowledge, the first of its kind for GUI retrieval. We evaluated our approach
on various datasets from related work and in a manual experiment. The results
demonstrate that our model outperforms previous approaches in text-to-GUI
retrieval achieving a Recall@10 of up to 0.69 and a HIT@10 of 0.91. We also
explored the performance of GUIClip for other GUI tasks including GUI
classification and sketch-to-GUI retrieval with encouraging results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Transactions on Software Engineering and Methodology
  (TOSEM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ To Forget or Not? Towards Practical Knowledge Unlearning for Large
  Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01920v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01920v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bozhong Tian, Xiaozhuan Liang, Siyuan Cheng, Qingbin Liu, Mengru Wang, Dianbo Sui, Xi Chen, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) trained on extensive corpora inevitably retain
sensitive data, such as personal privacy information and copyrighted material.
Recent advancements in knowledge unlearning involve updating LLM parameters to
erase specific knowledge. However, current unlearning paradigms are mired in
vague forgetting boundaries, often erasing knowledge indiscriminately. In this
work, we introduce KnowUnDo, a benchmark containing copyrighted content and
user privacy domains to evaluate if the unlearning process inadvertently erases
essential knowledge. Our findings indicate that existing unlearning methods
often suffer from excessive unlearning. To address this, we propose a simple
yet effective method, MemFlex, which utilizes gradient information to precisely
target and unlearn sensitive parameters. Experimental results show that MemFlex
is superior to existing methods in both precise knowledge unlearning and
general knowledge retaining of LLMs. Code and dataset are released at
https://github.com/zjunlp/KnowUnDo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings; Code and dataset are released at
  https://github.com/zjunlp/KnowUnDo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Mechanisms in Large Language Models: A <span class="highlight-title">Survey</span> and Perspective <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15017v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15017v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengru Wang, Yunzhi Yao, Ziwen Xu, Shuofei Qiao, Shumin Deng, Peng Wang, Xiang Chen, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial
for advancing towards trustworthy AGI. This paper reviews knowledge mechanism
analysis from a novel taxonomy including knowledge utilization and evolution.
Knowledge utilization delves into the mechanism of memorization, comprehension
and application, and creation. Knowledge evolution focuses on the dynamic
progression of knowledge within individual and group LLMs. Moreover, we discuss
what knowledge LLMs have learned, the reasons for the fragility of parametric
knowledge, and the potential dark knowledge (hypothesis) that will be
challenging to address. We hope this work can help understand knowledge in LLMs
and provide insights for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings; 39 pages (v3)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenMixup: Open Mixup Toolbox and Benchmark for Visual Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.04851v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.04851v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Li, Zedong Wang, Zicheng Liu, Juanxi Tian, Di Wu, Cheng Tan, Weiyang Jin, Stan Z. Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixup augmentation has emerged as a widely used technique for improving the
generalization ability of deep neural networks (DNNs). However, the lack of
standardized implementations and benchmarks has impeded recent progress,
resulting in poor reproducibility, unfair comparisons, and conflicting
insights. In this paper, we introduce OpenMixup, the first mixup augmentation
codebase, and benchmark for visual representation learning. Specifically, we
train 18 representative mixup baselines from scratch and rigorously evaluate
them across 11 image datasets of varying scales and granularity, ranging from
fine-grained scenarios to complex non-iconic scenes. We also open-source our
modular codebase, including a collection of popular vision backbones,
optimization strategies, and analysis toolkits, which not only supports the
benchmarking but enables broader mixup applications beyond classification, such
as self-supervised learning and regression tasks. Through experiments and
empirical analysis, we gain observations and insights on mixup
performance-efficiency trade-offs, generalization, and optimization behaviors,
and thereby identify preferred choices for different needs. To the best of our
knowledge, OpenMixup has facilitated several recent studies. We believe this
work can further advance reproducible mixup augmentation research and thereby
lay a solid ground for future progress in the community. The source code and
user documents are available at \url{https://github.com/Westlake-AI/openmixup}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint V3. The source code is available at
  https://github.com/Westlake-AI/openmixup</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S4D: Streaming 4D Real-World Reconstruction with Gaussians and 3D
  Control Points 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13036v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13036v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bing He, Yunuo Chen, Guo Lu, Qi Wang, Qunshan Gu, Rong Xie, Li Song, Wenjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic scene reconstruction using Gaussians has recently attracted increased
interest. Mainstream approaches typically employ a global deformation field to
warp a 3D scene in canonical space. However, the inherent low-frequency nature
of implicit neural fields often leads to ineffective representations of complex
motions. Moreover, their structural rigidity can hinder adaptation to scenes
with varying resolutions and durations. To address these challenges, we
introduce a novel approach for streaming 4D real-world reconstruction utilizing
discrete 3D control points. This method physically models local rays and
establishes a motion-decoupling coordinate system. By effectively merging
traditional graphics with learnable pipelines, it provides a robust and
efficient local 6-degrees-of-freedom (6-DoF) motion representation.
Additionally, we have developed a generalized framework that integrates our
control points with Gaussians. Starting from an initial 3D reconstruction, our
workflow decomposes the streaming 4D reconstruction into four independent
submodules: 3D segmentation, 3D control point generation, object-wise motion
manipulation, and residual compensation. Experimental results demonstrate that
our method outperforms existing state-of-the-art 4D Gaussian splatting
techniques on both the Neu3DV and CMU-Panoptic datasets. Notably, the
optimization of our 3D control points is achievable in 100 iterations and
within just 2 seconds per frame on a single NVIDIA 4070 GPU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 9 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion-Agent: A Conversational Framework for Human Motion Generation
  with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17013v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17013v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Wu, Yubo Zhao, Yifan Wang, Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While previous approaches to 3D human motion generation have achieved notable
success, they often rely on extensive training and are limited to specific
tasks. To address these challenges, we introduce Motion-Agent, an efficient
conversational framework designed for general human motion generation, editing,
and understanding. Motion-Agent employs an open-source pre-trained language
model to develop a generative agent, MotionLLM, that bridges the gap between
motion and text. This is accomplished by encoding and quantizing motions into
discrete tokens that align with the language model's vocabulary. With only
1--3\% of the model's parameters fine-tuned using adapters, MotionLLM delivers
performance on par with diffusion models and other transformer-based methods
trained from scratch. By integrating MotionLLM with GPT-4 without additional
training, Motion-Agent is able to generate highly complex motion sequences
through multi-turn conversations, a capability that previous models have
struggled to achieve. Motion-Agent supports a wide range of motion-language
tasks, offering versatile capabilities for generating and customizing human
motion through interactive conversational exchanges. Project page:
https://knoxzhao.github.io/Motion-Agent
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://knoxzhao.github.io/Motion-Agent</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-to-Image Rectified Flow as Plug-and-Play Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03293v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03293v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Yang, Cheng Chen, Xulei Yang, Fayao Liu, Guosheng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale diffusion models have achieved remarkable performance in
generative tasks. Beyond their initial training applications, these models have
proven their ability to function as versatile plug-and-play priors. For
instance, 2D diffusion models can serve as loss functions to optimize 3D
implicit models. Rectified flow, a novel class of generative models, enforces a
linear progression from the source to the target distribution and has
demonstrated superior performance across various domains. Compared to
diffusion-based methods, rectified flow approaches surpass in terms of
generation quality and efficiency, requiring fewer inference steps. In this
work, we present theoretical and experimental evidence demonstrating that
rectified flow based methods offer similar functionalities to diffusion models
- they can also serve as effective priors. Besides the generative capabilities
of diffusion priors, motivated by the unique time-symmetry properties of
rectified flow models, a variant of our method can additionally perform image
inversion. Experimentally, our rectified flow-based priors outperform their
diffusion counterparts - the SDS and VSD losses - in text-to-3D generation. Our
method also displays competitive performance in image inversion and editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended with Stochastic Interpolants. Code:
  https://github.com/yangxiaofeng/rectified_flow_prior</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAGR: Manifold-Aligned Graph Regularization for Continual Action Quality
  Assessment <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04398v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04398v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kanglei Zhou, Liyuan Wang, Xingxing Zhang, Hubert P. H. Shum, Frederick W. B. Li, Jianguo Li, Xiaohui Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Action Quality Assessment (AQA) evaluates diverse skills but models struggle
with non-stationary data. We propose Continual AQA (CAQA) to refine models
using sparse new data. Feature replay preserves memory without storing raw
inputs. However, the misalignment between static old features and the
dynamically changing feature manifold causes severe catastrophic forgetting. To
address this novel problem, we propose Manifold-Aligned Graph Regularization
(MAGR), which first aligns deviated old features to the current feature
manifold, ensuring representation consistency. It then constructs a graph
jointly arranging old and new features aligned with quality scores. Experiments
show MAGR outperforms recent strong baselines with up to 6.56%, 5.66%, 15.64%,
and 9.05% correlation gains on the MTL-AQA, FineDiving, UNLV-Dive, and JDM-MSA
split datasets, respectively. This validates MAGR for continual assessment
challenges arising from non-stationary skill variations. Code is available at
https://github.com/ZhouKanglei/MAGR_CAQA}{https://github.com/ZhouKanglei/MAGR_CAQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2024 as an oral paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MindFormer: Semantic Alignment of Multi-Subject fMRI for Brain Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17720v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17720v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inhwa Han, Jaayeon Lee, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research efforts for visual decoding from fMRI signals have attracted
considerable attention in research community. Still multi-subject fMRI decoding
with one model has been considered intractable due to the drastic variations in
fMRI signals between subjects and even within the same subject across different
trials. To address current limitations in multi-subject brain decoding, here we
introduce a novel semantic alignment method of multi-subject fMRI signals using
so-called MindFormer. This model is specifically designed to generate
fMRI-conditioned feature vectors that can be used for conditioning Stable
Diffusion model for fMRI- to-image generation or large language model (LLM) for
fMRI-to-text generation. More specifically, MindFormer incorporates two key
innovations: 1) a subject specific token that effectively capture individual
differences in fMRI signals while synergistically combines multi subject fMRI
data for training, and 2) a novel feature embedding and training scheme based
on the IP-Adapter to extract semantically meaningful features from fMRI
signals. Our experimental results demonstrate that MindFormer generates
semantically consistent images and text across different subjects. Since our
MindFormer maintains semantic fidelity by fully utilizing the training data
across different subjects by significantly surpassing existing models in
multi-subject brain decoding, this may help deepening our understanding of
neural processing variations among individuals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Unsupervised Semantic Segmentation with Principal Mask
  Proposals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16818v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16818v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Hahn, Nikita Araslanov, Simone Schaub-Meyer, Stefan Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised semantic segmentation aims to automatically partition images
into semantically meaningful regions by identifying global semantic categories
within an image corpus without any form of annotation. Building upon recent
advances in self-supervised representation learning, we focus on how to
leverage these large pre-trained models for the downstream task of unsupervised
segmentation. We present PriMaPs - Principal Mask Proposals - decomposing
images into semantically meaningful masks based on their feature
representation. This allows us to realize unsupervised semantic segmentation by
fitting class prototypes to PriMaPs with a stochastic expectation-maximization
algorithm, PriMaPs-EM. Despite its conceptual simplicity, PriMaPs-EM leads to
competitive results across various pre-trained backbone models, including DINO
and DINOv2, and across different datasets, such as Cityscapes, COCO-Stuff, and
Potsdam-3. Importantly, PriMaPs-EM is able to boost results when applied
orthogonally to current state-of-the-art unsupervised semantic segmentation
pipelines. Code is available at https://github.com/visinf/primaps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in TMLR (September 2024) | OpenReview: see
  https://openreview.net/forum?id=UawaTQzfwy | Project Page: see
  https://visinf.github.io/primaps/ | Code: see
  https://github.com/visinf/primaps</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MaPPER: Multimodal Prior-guided Parameter Efficient Tuning for Referring
  Expression Comprehension <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13609v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13609v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting Liu, Zunnan Xu, Yue Hu, Liangtao Shi, Zhiqiang Wang, Quanjun Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring Expression Comprehension (REC), which aims to ground a local visual
region via natural language, is a task that heavily relies on multimodal
alignment. Most existing methods utilize powerful pre-trained models to
transfer visual/linguistic knowledge by full fine-tuning. However, full
fine-tuning the entire backbone not only breaks the rich prior knowledge
embedded in the pre-training, but also incurs significant computational costs.
Motivated by the recent emergence of Parameter-Efficient Transfer Learning
(PETL) methods, we aim to solve the REC task in an effective and efficient
manner. Directly applying these PETL methods to the REC task is inappropriate,
as they lack the specific-domain abilities for precise local visual perception
and visual-language alignment. Therefore, we propose a novel framework of
Multimodal Prior-guided Parameter Efficient Tuning, namely MaPPER.
Specifically, MaPPER comprises Dynamic Prior Adapters guided by an aligned
prior, and Local Convolution Adapters to extract precise local semantics for
better visual perception. Moreover, the Prior-Guided Text module is proposed to
further utilize the prior for facilitating the cross-modal alignment.
Experimental results on three widely-used benchmarks demonstrate that MaPPER
achieves the best accuracy compared to the full fine-tuning and other PETL
methods with only 1.41% tunable backbone parameters. Our code is available at
https://github.com/liuting20/MaPPER.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning Innovations for Underwater Waste Detection: An In-Depth
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18299v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18299v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaskaran Singh Walia, Pavithra L K, Kesar Mehta, Shivram Harshavardhana, Nandini Tyagi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing the issue of submerged underwater trash is crucial for
safeguarding aquatic ecosystems and preserving marine life. While identifying
debris present on the surface of water bodies is straightforward, assessing the
underwater submerged waste is a challenge due to the image distortions caused
by factors such as light refraction, absorption, suspended particles, color
shifts, and occlusion. This paper conducts a comprehensive review of
state-of-the-art architectures and on the existing datasets to establish a
baseline for submerged waste and trash detection. The primary goal remains to
establish the benchmark of the object localization techniques to be leveraged
by advanced underwater sensors and autonomous underwater vehicles. The ultimate
objective is to explore the underwater environment, to identify, and remove
underwater debris. The absence of benchmarks (dataset or algorithm) in many
researches emphasizes the need for a more robust algorithmic solution. Through
this research, we aim to give performance comparative analysis of various
underwater trash detection algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Tapestry of Consistency in Large Vision-Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14156v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14156v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Zhang, Fei Xiao, Tao Huang, Chun-Kai Fan, Hongyuan Dong, Jiawen Li, Jiacong Wang, Kuan Cheng, Shanghang Zhang, Haoyuan Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) have recently achieved rapid progress,
exhibiting great perception and reasoning abilities concerning visual
information. However, when faced with prompts in different sizes of solution
spaces, LVLMs fail to always give consistent answers regarding the same
knowledge point. This inconsistency of answers between different solution
spaces is prevalent in LVLMs and erodes trust. To this end, we provide a
multi-modal benchmark ConBench, to intuitively analyze how LVLMs perform when
the solution space of a prompt revolves around a knowledge point. Based on the
ConBench tool, we are the first to reveal the tapestry and get the following
findings: (1) In the discriminate realm, the larger the solution space of the
prompt, the lower the accuracy of the answers. (2) Establish the relationship
between the discriminative and generative realms: the accuracy of the
discriminative question type exhibits a strong positive correlation with its
Consistency with the caption. (3) Compared to open-source models, closed-source
models exhibit a pronounced bias advantage in terms of Consistency. Eventually,
we ameliorate the consistency of LVLMs by trigger-based diagnostic refinement,
indirectly improving the performance of their caption. We hope this paper will
accelerate the research community in better evaluating their models and
encourage future advancements in the consistency domain. The project is
available at https://github.com/foundation-multimodal-models/ConBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GaussianBlock: Building Part-Aware Compositional and Editable 3D Scene
  by Primitives and Gaussians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01535v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01535v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyi Jiang, Qihao Zhao, Hossein Rahmani, De Wen Soh, Jun Liu, Na Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, with the development of Neural Radiance Fields and Gaussian
Splatting, 3D reconstruction techniques have achieved remarkably high fidelity.
However, the latent representations learnt by these methods are highly
entangled and lack interpretability. In this paper, we propose a novel
part-aware compositional reconstruction method, called GaussianBlock, that
enables semantically coherent and disentangled representations, allowing for
precise and physical editing akin to building blocks, while simultaneously
maintaining high fidelity. Our GaussianBlock introduces a hybrid representation
that leverages the advantages of both primitives, known for their flexible
actionability and editability, and 3D Gaussians, which excel in reconstruction
quality. Specifically, we achieve semantically coherent primitives through a
novel attention-guided centering loss derived from 2D semantic priors,
complemented by a dynamic splitting and fusion strategy. Furthermore, we
utilize 3D Gaussians that hybridize with primitives to refine structural
details and enhance fidelity. Additionally, a binding inheritance strategy is
employed to strengthen and maintain the connection between the two. Our
reconstructed scenes are evidenced to be disentangled, compositional, and
compact across diverse benchmarks, enabling seamless, direct and precise
editing while maintaining high quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ArMeme: Propagandistic Content in Arabic Memes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Firoj Alam, Abul Hasnat, Fatema Ahmed, Md Arid Hasan, Maram Hasanain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of digital communication, memes have become a significant
medium for cultural and political expression that is often used to mislead
audiences. Identification of such misleading and persuasive multimodal content
has become more important among various stakeholders, including social media
platforms, policymakers, and the broader society as they often cause harm to
individuals, organizations, and/or society. While there has been effort to
develop AI-based automatic systems for resource-rich languages (e.g., English),
it is relatively little to none for medium to low resource languages. In this
study, we focused on developing an Arabic memes dataset with manual annotations
of propagandistic content. We annotated ~6K Arabic memes collected from various
social media platforms, which is a first resource for Arabic multimodal
research. We provide a comprehensive analysis aiming to develop computational
tools for their detection. We will make them publicly available for the
community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>disinformation, misinformation, factuality, harmfulness, fake news,
  propaganda, multimodality, text, images</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Drone Stereo Vision for Radiata Pine Branch Detection and Distance
  Measurement: Utilizing Deep Learning and YOLO Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00503v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00503v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yida Lin, Bing Xue, Mengjie Zhang, Sam Schofield, Richard Green
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research focuses on the development of a drone equipped with pruning
tools and a stereo vision camera to accurately detect and measure the spatial
positions of tree branches. YOLO is employed for branch segmentation, while two
depth estimation approaches, monocular and stereo, are investigated. In
comparison to SGBM, deep learning techniques produce more refined and accurate
depth maps. In the absence of ground-truth data, a fine-tuning process using
deep neural networks is applied to approximate optimal depth values. This
methodology facilitates precise branch detection and distance measurement,
addressing critical challenges in the automation of pruning operations. The
results demonstrate notable advancements in both accuracy and efficiency,
underscoring the potential of deep learning to drive innovation and enhance
automation in the agricultural sector.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal 3D Fusion and In-Situ Learning for Spatially Aware AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyuan Xu, Radha Kumaran, Noah Stier, Kangyou Yu, Tobias Höllerer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Seamless integration of virtual and physical worlds in augmented reality
benefits from the system semantically "understanding" the physical environment.
AR research has long focused on the potential of context awareness,
demonstrating novel capabilities that leverage the semantics in the 3D
environment for various object-level interactions. Meanwhile, the computer
vision community has made leaps in neural vision-language understanding to
enhance environment perception for autonomous tasks. In this work, we introduce
a multimodal 3D object representation that unifies both semantic and linguistic
knowledge with the geometric representation, enabling user-guided machine
learning involving physical objects. We first present a fast multimodal 3D
reconstruction pipeline that brings linguistic understanding to AR by fusing
CLIP vision-language features into the environment and object models. We then
propose "in-situ" machine learning, which, in conjunction with the multimodal
representation, enables new tools and interfaces for users to interact with
physical spaces and objects in a spatially and linguistically meaningful
manner. We demonstrate the usefulness of the proposed system through two
real-world AR applications on Magic Leap 2: a) spatial search in physical
environments with natural language and b) an intelligent inventory system that
tracks object changes over time. We also make our full implementation and demo
data available at (https://github.com/cy-xu/spatially_aware_AI) to encourage
further exploration and research in spatially aware AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, accepted to IEEE ISMAR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Solidarity Amid Hostility: Experiences of Fat People in Online
  Communities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Blakeley H. Payne, Jordan Taylor, Katta Spiel, Casey Fiesler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online communities are important spaces for members of marginalized groups to
organize and support one another. To better understand the experiences of fat
people -- a group whose marginalization often goes unrecognized -- in online
communities, we conducted 12 semi-structured interviews with fat people. Our
participants leveraged online communities to engage in consciousness raising
around fat identity, learning to locate "the problem of being fat" not within
themselves or their own bodies but rather in the oppressive design of the
society around them. Participants were then able to use these communities to
mitigate everyday experiences of anti-fatness, such as navigating hostile
healthcare systems. However, to access these benefits, our participants had to
navigate myriad sociotechnical harms, ranging from harassment to discriminatory
algorithms. In light of these findings, we suggest that researchers and
designers of online communities support selective fat visibility, consider fat
people in the design of content moderation systems, and investigate algorithmic
discrimination toward fat people. More broadly, we call on researchers and
designers to contend with the social and material realities of fat experience,
as opposed to the prevailing paradigm of treating fat people as problems to be
solved in-and-of-themselves. This requires recognizing fat people as a
marginalized social group and actively confronting anti-fatness as it is
embedded in the design of technology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Need Help? Designing Proactive AI Assistants for Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valerie Chen, Alan Zhu, Sebastian Zhao, Hussein Mozannar, David Sontag, Ameet Talwalkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While current chat-based AI assistants primarily operate reactively,
responding only when prompted by users, there is significant potential for
these systems to proactively assist in tasks without explicit invocation,
enabling a mixed-initiative interaction. This work explores the design and
implementation of proactive AI assistants powered by large language models. We
first outline the key design considerations for building effective proactive
assistants. As a case study, we propose a proactive chat-based programming
assistant that automatically provides suggestions and facilitates their
integration into the programmer's code. The programming context provides a
shared workspace enabling the assistant to offer more relevant suggestions. We
conducted a randomized experimental study examining the impact of various
design elements of the proactive assistant on programmer productivity and user
experience. Our findings reveal significant benefits of incorporating proactive
chat assistants into coding environments and uncover important nuances that
influence their usage and effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CardioAI: A Multimodal AI-based System to Support Symptom Monitoring and
  Risk Detection of Cancer Treatment-Induced Cardiotoxicity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyi Wu, Weidan Cao, Shihan Fu, Bingsheng Yao, Ziqi Yang, Changchang Yin, Varun Mishra, Daniel Addison, Ping Zhang, Dakuo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in cancer treatments that prolong patients' lives,
treatment-induced cardiotoxicity remains one severe side effect. The clinical
decision-making of cardiotoxicity is challenging, as non-clinical symptoms can
be missed until life-threatening events occur at a later stage, and clinicians
already have a high workload centered on the treatment, not the side effects.
Our project starts with a participatory design study with 11 clinicians to
understand their practices and needs; then we build a multimodal AI system,
CardioAI, that integrates wearables and LLM-powered voice assistants to monitor
multimodal non-clinical symptoms. Also, the system includes an explainable risk
prediction module that can generate cardiotoxicity risk scores and summaries as
explanations to support clinicians' decision-making. We conducted a heuristic
evaluation with four clinical experts and found that they all believe CardioAI
integrates well into their workflow, reduces their information overload, and
enables them to make more informed decisions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizability analysis of deep learning predictions of human brain
  responses to augmented and semantically novel visual stimuli 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentyn Piskovskyi, Riccardo Chimisso, Sabrina Patania, Tom Foulsham, Giuseppe Vizzari, Dimitri Ognibene
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The purpose of this work is to investigate the soundness and utility of a
neural network-based approach as a framework for exploring the impact of image
enhancement techniques on visual cortex activation. In a preliminary study, we
prepare a set of state-of-the-art brain encoding models, selected among the top
10 methods that participated in The Algonauts Project 2023 Challenge [16]. We
analyze their ability to make valid predictions about the effects of various
image enhancement techniques on neural responses. Given the impossibility of
acquiring the actual data due to the high costs associated with brain imaging
procedures, our investigation builds up on a series of experiments.
Specifically, we analyze the ability of brain encoders to estimate the cerebral
reaction to various augmentations by evaluating the response to augmentations
targeting objects (i.e., faces and words) with known impact on specific areas.
Moreover, we study the predicted activation in response to objects unseen
during training, exploring the impact of semantically out-of-distribution
stimuli. We provide relevant evidence for the generalization ability of the
models forming the proposed framework, which appears to be promising for the
identification of the optimal visual augmentation filter for a given task,
model-driven design strategies as well as for AR and VR applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RespDiff: An End-to-End Multi-scale RNN Diffusion Model for Respiratory
  Waveform Estimation from PPG Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Miao, Zehua Chen, Chang Li, Danilo Mandic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Respiratory rate (RR) is a critical health indicator often monitored under
inconvenient scenarios, limiting its practicality for continuous monitoring.
Photoplethysmography (PPG) sensors, increasingly integrated into wearable
devices, offer a chance to continuously estimate RR in a portable manner. In
this paper, we propose RespDiff, an end-to-end multi-scale RNN diffusion model
for respiratory waveform estimation from PPG signals. RespDiff does not require
hand-crafted features or the exclusion of low-quality signal segments, making
it suitable for real-world scenarios. The model employs multi-scale encoders,
to extract features at different resolutions, and a bidirectional RNN to
process PPG signals and extract respiratory waveform. Additionally, a spectral
loss term is introduced to optimize the model further. Experiments conducted on
the BIDMC dataset demonstrate that RespDiff outperforms notable previous works,
achieving a mean absolute error (MAE) of 1.18 bpm for RR estimation while
others range from 1.66 to 2.15 bpm, showing its potential for robust and
accurate respiratory monitoring in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Co-Learners: Enhancing Cognitive and Social Presence of
  Students in Asynchronous Learning with Generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianjia Wang, Tong Wu, Huayi Liu, Chris Brown, Yan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cognitive presence and social presence are crucial for a comprehensive
learning experience. Despite the flexibility of asynchronous learning
environments to accommodate individual schedules, the inherent constraints of
asynchronous environments make augmenting cognitive and social presence
particularly challenging. Students often face challenges such as a lack of
timely feedback and support, a lack of non-verbal cues in communication, and a
sense of isolation. To address this challenge, this paper introduces Generative
Co-Learners, a system designed to leverage generative AI-powered agents,
simulating co-learners supporting multimodal interactions, to improve cognitive
and social presence in asynchronous learning environments. We conducted a study
involving 12 student participants who used our system to engage with online
programming tutorials to assess the system's effectiveness. The results show
that by implementing features to support textual and visual communication and
simulate an interactive learning environment with generative agents, our system
enhances the cognitive and social presence in the asynchronous learning
environment. These results suggest the potential to use generative AI to
support students learning at scale and transform asynchronous learning into a
more inclusive, engaging, and efficacious educational approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Urban Computing for Climate and Environmental Justice: Early
  Perspectives From Two Research Initiatives <span class="chip">IEEE VIS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carolina Veiga, Ashish Sharma, Daniel de Oliveira, Marcos Lage, Fabio Miranda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The impacts of climate change are intensifying existing vulnerabilities and
disparities within urban communities around the globe, as extreme weather
events, including floods and heatwaves, are becoming more frequent and severe,
disproportionately affecting low-income and underrepresented groups. Tackling
these increasing challenges requires novel approaches that integrate expertise
across multiple domains, including computer science, engineering, climate
science, and public health. Urban computing can play a pivotal role in these
efforts by integrating data from multiple sources to support decision-making
and provide actionable insights into weather patterns, infrastructure
weaknesses, and population vulnerabilities. However, the capacity to leverage
technological advancements varies significantly between the Global South and
Global North. In this paper, we present two multiyear, multidisciplinary
projects situated in Chicago, USA and Niter\'oi, Brazil, highlighting the
opportunities and limitations of urban computing in these diverse contexts.
Reflecting on our experiences, we then discuss the essential requirements, as
well as existing gaps, for visual analytics tools that facilitate the
understanding and mitigation of climate-related risks in urban environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Viz4Climate + Sustainability: IEEE VIS 2024 Workshop
  on Visualization for Climate Action and Sustainability
  (https://svs.gsfc.nasa.gov/events/2024/Viz4ClimateAndSustainability/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IntelliExplain: Enhancing Interactive Code Generation through Natural
  Language Explanations for Non-Professional Programmers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10250v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10250v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yan, Thomas D. Latoza, Ziyu Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chat LLMs such as GPT-3.5-turbo and GPT-4 have shown promise in assisting
humans in coding, particularly by enabling them to conversationally provide
feedback. However, current approaches assume users have expert debugging
skills, limiting accessibility for non-professional programmers. In this paper,
we first explore Chat LLMs' limitations in assisting non-professional
programmers with coding. Through a formative study, we identify two key
elements affecting their experience: the way a Chat LLM explains its generated
code and the structure of human-LLM interaction. We then propose
IntelliExplain, a new conversational code generation framework with enhanced
code explanations and a structured interaction paradigm, which enforces both
better code understanding and a more effective feedback loop. In two
programming tasks (SQL and Python), IntelliExplain yields significantly higher
success rates and reduces task time compared to the vanilla Chat LLM. We also
identify several opportunities that remain in effectively offering a chat-based
programming experience for non-professional programmers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Mechanisms in Large Language Models: A <span class="highlight-title">Survey</span> and Perspective <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15017v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15017v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengru Wang, Yunzhi Yao, Ziwen Xu, Shuofei Qiao, Shumin Deng, Peng Wang, Xiang Chen, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial
for advancing towards trustworthy AGI. This paper reviews knowledge mechanism
analysis from a novel taxonomy including knowledge utilization and evolution.
Knowledge utilization delves into the mechanism of memorization, comprehension
and application, and creation. Knowledge evolution focuses on the dynamic
progression of knowledge within individual and group LLMs. Moreover, we discuss
what knowledge LLMs have learned, the reasons for the fragility of parametric
knowledge, and the potential dark knowledge (hypothesis) that will be
challenging to address. We hope this work can help understand knowledge in LLMs
and provide insights for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings; 39 pages (v3)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data
  Fusion and Multi-Stream CNN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15003v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15003v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oluwaleke Yusuf, Maki Habib, Mohamed Moustafa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hand Gesture Recognition (HGR) enables intuitive human-computer interactions
in various real-world contexts. However, existing frameworks often struggle to
meet the real-time requirements essential for practical HGR applications. This
study introduces a robust, skeleton-based framework for dynamic HGR that
simplifies the recognition of dynamic hand gestures into a static image
classification task, effectively reducing both hardware and computational
demands. Our framework utilizes a data-level fusion technique to encode 3D
skeleton data from dynamic gestures into static RGB spatiotemporal images. It
incorporates a specialized end-to-end Ensemble Tuner (e2eET) Multi-Stream CNN
architecture that optimizes the semantic connections between data
representations while minimizing computational needs. Tested across five
benchmark datasets (SHREC'17, DHG-14/28, FPHA, LMDHG, and CNR), the framework
showed competitive performance with the state-of-the-art. Its capability to
support real-time HGR applications was also demonstrated through deployment on
standard consumer PC hardware, showcasing low latency and minimal resource
usage in real-world settings. The successful deployment of this framework
underscores its potential to enhance real-time applications in fields such as
virtual/augmented reality, ambient intelligence, and assistive technologies,
providing a scalable and efficient solution for dynamic gesture recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages. 7 figures. Code available at
  https://github.com/Outsiders17711/e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A mobile digital device proficiency performance test for cognitive
  clinical research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alan Cronemberger Andrade, Diógenes de Souza Bido, Ana Carolina Bottura de Barros, Walter Richard Boot, Paulo Henrique Ferreira Bertolucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile device proficiency is increasingly important for everyday living,
including to deliver healthcare services. Human-device interactions represent a
potential in cognitive neurology and aging research. Although traditional
pen-and-paper evaluations serve as valuable tools within public health
strategies for population-scale cognitive assessments, digital devices could
amplify cognitive assessment. However, even person-centered studies often fail
to incorporate measures of mobile device proficiency and research with digital
mobile technology frequently neglects these evaluations. Besides that,
cognitive screening, a fundamental part of brain health evaluation and a widely
accepted strategy to identify high-risk individuals vulnerable to cognitive
impairment and dementia, has research using digital devices for older adults in
need for standardization. To address this shortfall, the DigiTAU collaborative
and interdisciplinary project is creating refined methodological parameters for
the investigation of digital biomarkers. With careful consideration of
cognitive design elements, here we describe the open-source and
performance-based Mobile Device Abilities Test (MDAT), a simple, low-cost, and
reproductible open-sourced test framework. This result was achieved with a
cross-sectional study population sample of 101 low and middle-income subjects
aged 20 to 79 years old. Partial least squares structural equation modeling
(PLS-SEM) was used to assess the measurement of the construct. It was possible
to achieve a reliable method with internal consistency, good content validity
related to digital competences, and that does not have much interference with
auto-perceived global functional disability, health self-perception, and motor
dexterity. Limitations for this method are discussed and paths to improve and
establish better standards are highlighted.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Programming Languages <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Linearizability Monitoring for Sets, Stacks, Queues and
  Priority Queues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lee Zheng Han, Umang Mathur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider the problem of automatically monitoring
linearizability. Here, one observes an execution of a concurrent program that
interacts with a concurrent object and determines if the execution witnesses
the violation of linearizability with respect to the sequential specification
of the underlying data structure of the concurrent object. This problem has
been extensively studied in the past for read-write registers, and both tight
upper and lower bounds have been proposed in this case. While this problem has
also been studied for the case of other prominent data structures such as
stacks and queues, we find that these results are either not extensive or in
some cases incorrect. In this paper, we study the problem under the restriction
where values inserted in the data types are distinct (in the execution
observed). We then show that under such a restriction, the linearizability
problem is solvable in polynomial time for these data types. Beyond theoretical
soundness and completeness, the algorithms proposed are empirically proven to
outperform all state-of-the-art linearizability monitors.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Networking and Internet Architecture <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Consistent and Repeatable Testing of mMIMO O-RU across labs: A
  Japan-Singapore Experience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thanh-Tam Nguyen, Mao V. Ngo, Binbin Chen, Mitsuhiro Kuchitsu, Serena Wai, Seitaro Kawai, Kenya Suzuki, Eng Wei Koo, Tony Quek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open Radio Access Networks (RAN) aim to bring a paradigm shift to
telecommunications industry, by enabling an open, intelligent, virtualized, and
multi-vendor interoperable RAN ecosystem. At the center of this movement, O-RAN
ALLIANCE defines the O-RAN architecture and standards, so that companies around
the globe can use these specifications to create innovative and interoperable
solutions. To accelerate the adoption of O-RAN products, rigorous testing of
O-RAN Radio Unit (O-RU) and other O-RAN products plays a key role. O-RAN
ALLIANCE has approved around 20 Open Testing and Integration Centres (OTICs)
globally. OTICs serve as vendor-neutral platforms for providing the testing and
integration services, with the vision that an O-RAN product certified in any
OTIC is accepted in other parts of the world. To demonstrate the viability of
such a certified-once-and-use-everywhere approach, one theme in the O-RAN
Global PlugFest Spring 2024 is to demonstrate consistent and repeatable testing
for the open fronthaul interface across multiple labs. Towards this, Japan OTIC
and Asia Pacific OTIC in Singapore have teamed up together with an O-RU vendor
and Keysight Technology. Our international team successfully completed all test
cases defined by O-RAN ALLIANCE for O-RU conformance testing. In this paper, we
share our journey in achieving this outcome, focusing on the challenges we have
overcome and the lessons we have learned through this process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published version at RitiRAN Workshop - co-located with IEEE VTC Fall
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Consistent and Repeatable Testing of O-RAN Distributed Unit (O-DU)
  across Continents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan V. Ngo, Mao V. Ngo, Binbin Chen, Gabriele Gemmi, Eduardo Baena, Michele Polese, Tommaso Melodia, William Chien, Tony Quek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open Radio Access Networks (O-RAN) are expected to revolutionize the
telecommunications industry with benefits like cost reduction, vendor
diversity, and improved network performance through AI optimization. Supporting
the O-RAN ALLIANCE's mission to achieve more intelligent, open, virtualized and
fully interoperable mobile networks, O-RAN Open Testing and Integration Centers
(OTICs) play a key role in accelerating the adoption of O-RAN specifications
based on rigorous testing and validation. One theme in the recent O-RAN Global
PlugFest Spring 2024 focused on demonstrating consistent and repeatable Open
Fronthaul testing in multiple labs. To respond to this topic, in this paper, we
present a detailed analysis of the testing methodologies and results for O-RAN
Distributed Unit (O-DU) in O-RAN across two OTICs. We identify key differences
in testing setups, share challenges encountered, and propose best practices for
achieving repeatable and consistent testing results. Our findings highlight the
impact of different deployment technologies and testing environments on
performance and conformance testing outcomes, providing valuable insights for
future O-RAN implementations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published version at RitiRAN Workshop - co-located with IEEE VTC Fall
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Research on Enhancing C-V2X Communication via Danger-Aware Vehicular
  Networking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00012v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00012v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lanre Sadeeq, Qasim Ajao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a protocol that optimizes message dissemination in C-V2X
technology, crucial for advancing intelligent transportation systems (ITS)
aimed at enhancing road safety. As vehicle density and velocity rise, the
volume of data requiring communication significantly increases. By considering
the risk levels that vehicles encounter and using inter-vehicle proximity as a
key indicator of potential hazards, the proposed protocol prioritizes
communication, allowing vehicles facing higher risks to transmit their messages
first. Our results show that this prioritization effectively reduces the number
of concurrent transmissions, leading to improved performance metrics such as
packet delivery ratio, throughput, latency, and lower probabilities of channel
congestion and collision.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CONFINE: Preserving Data Secrecy in Decentralized Process Mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valerio Goretti, Davide Basile, Luca Barbaro, Claudio Di Ciccio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the contemporary business landscape, collaboration across multiple
organizations offers a multitude of opportunities, including reduced
operational costs, enhanced performance, and accelerated technological
advancement. The application of process mining techniques in an
inter-organizational setting, exploiting the recorded process event data,
enables the coordination of joint effort and allows for a deeper understanding
of the business. Nevertheless, considerable concerns pertaining to data
confidentiality emerge, as organizations frequently demonstrate a reluctance to
expose sensitive data demanded for process mining, due to concerns related to
privacy and security risks. The presence of conflicting interests among the
parties involved can impede the practice of open data sharing. To address these
challenges, we propose our approach and toolset named CONFINE, which we
developed with the intent of enabling process mining on process event data from
multiple providers while preserving the confidentiality and integrity of the
original records. To ensure that the presented interaction protocol steps are
secure and that the processed information is hidden from both involved and
external actors, our approach is based on a decentralized architecture and
consists of trusted applications running in Trusted Execution Environments
(TEE). In this demo paper, we provide an overview of the core components and
functionalities as well as the specific details of its application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi Armed Bandit Algorithms Based Virtual Machine Allocation Policy
  for Security in Multi-Tenant Distributed Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pravin Patil, Geetanjali Kale, Tanmay Karmarkar, Ruturaj Ghatage
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a secure and dynamic VM allocation strategy for
multi-tenant distributed systems using the Thompson sampling approach. The
method proves more effective and secure compared to epsilon-greedy and upper
confidence bound methods, showing lower regret levels.,Initially, VM allocation
was static, but the unpredictable nature of attacks necessitated a dynamic
approach. Historical VM data was analyzed to understand attack responses, with
rewards granted for unsuccessful attacks and reduced for successful ones,
influencing regret levels.,The paper introduces a Multi Arm Bandit-based VM
allocation policy, utilizing a Weighted Average Ensemble Learning algorithm
trained on known attacks and non-attacks. This ensemble approach outperforms
traditional algorithms like Logistic Regression, SVM, K Nearest Neighbors, and
XGBoost.,For suspicious activity detection, a Stacked Anomaly Detector
algorithm is proposed, trained on known non-attacks. This method surpasses
existing techniques such as Isolation Forest and PCA-based approaches.,Overall,
this paper presents an advanced solution for VM allocation policies, enhancing
cloud-based system security through a combination of dynamic allocation,
ensemble learning, and anomaly detection techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-guided Fuzzing of Distributed Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02307v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02307v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ege Berkay Gulcan, Burcu Kulahcioglu Ozkan, Rupak Majumdar, Srinidhi Nagendra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a coverage-guided testing algorithm for distributed systems
implementations. Our main innovation is the use of an abstract formal model of
the system that is used to define coverage. Such abstract models are frequently
developed in early phases of protocol design and verification but are
infrequently used at testing time. We show that guiding random test generation
using model coverage can be effective in covering interesting points in the
implementation state space. We have implemented a fuzzer for distributed system
implementations and abstract models written in TLA+. Our algorithm shows better
coverage over purely random exploration as well as random exploration guided by
different notions of scheduler coverage and mutation. In particular, we show
consistently higher coverage and detect bugs faster on implementations of
distributed consensus protocols such as those in Etcd-raft and RedisRaft.
Moreover, we discovered 13 previously unknown bugs in their implementations,
four of which could only be detected by model-guided fuzzing.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-05T00:00:00Z">2024-10-05</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Software Engineering <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robotics Meets Software Engineering: A First Look at the Robotics
  Discussions on Stackoverflow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hisham Kidwai, Danika Passler Bates, Sujana Islam Suhi, James Young, Shaiful Chowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots can greatly enhance human capabilities, yet their development presents
a range of challenges. This collaborative study, conducted by a team of
software engineering and robotics researchers, seeks to identify the challenges
encountered by robot developers by analyzing questions posted on StackOverflow.
We created a filtered dataset of 500 robotics-related questions and examined
their characteristics, comparing them with randomly selected questions from the
platform. Our findings indicate that the small size of the robotics community
limits the visibility of these questions, resulting in fewer responses. While
the number of robotics questions has been steadily increasing, they remain less
popular than the average question and answer on StackOverflow. This underscores
the importance of research that focuses on the challenges faced by robotics
practitioners.
  Consequently, we conducted a thematic analysis of the 500 robotics questions
to uncover common inquiry patterns. We identified 11 major themes, with
questions about robot movement being the most frequent. Our analysis of yearly
trends revealed that certain themes, such as Specifications, were prominent
from 2009 to 2014 but have since diminished in relevance. In contrast, themes
like Moving, Actuator, and Remote have consistently dominated discussions over
the years. These findings suggest that challenges in robotics may vary over
time.
  Notably, the majority of robotics questions are framed as How questions,
rather than Why or What questions, revealing the lack of enough resources for
the practitioners. These insights can help guide researchers and educators in
developing effective and timely educational materials for robotics
practitioners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffSpec: Differential Testing with LLMs using Natural Language
  Specifications and Code Artifacts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikitha Rao, Elizabeth Gilbert, Tahina Ramananandro, Nikhil Swamy, Claire Le Goues, Sarah Fakhoury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differential testing can be an effective way to find bugs in software systems
with multiple implementations that conform to the same specification, like
compilers, network protocol parsers, and language runtimes. Specifications for
such systems are often standardized in natural language documents, like
Instruction Set Architecture (ISA) specifications, Wasm specifications or IETF
RFC's. Large Language Models (LLMs) have demonstrated potential in both
generating tests and handling large volumes of natural language text, making
them well-suited for utilizing artifacts like specification documents, bug
reports, and code implementations. In this work, we leverage natural language
and code artifacts to guide LLMs to generate targeted, meaningful tests that
highlight meaningful behavioral differences between implementations, including
those corresponding to bugs. We introduce DiffSpec, a framework for generating
differential tests with LLMs using prompt chaining. We demonstrate the efficacy
of DiffSpec on two different systems, namely, eBPF runtimes and Wasm
validators. Using DiffSpec, we generated 359 differentiating tests, uncovering
at least four distinct and confirmed bugs in eBPF, including a kernel memory
leak, inconsistent behavior in jump instructions, and undefined behavior when
using the stack pointer. We also found 279 differentiating tests in Wasm
validators, that point to at least 2 confirmed and fixed bugs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model Supply Chain: A Research Agenda 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12736v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12736v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenao Wang, Yanjie Zhao, Xinyi Hou, Haoyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Large Language Models (LLMs) has revolutionized
artificial intelligence, introducing unprecedented capabilities in natural
language processing and multimodal content generation. However, the increasing
complexity and scale of these models have given rise to a multifaceted supply
chain that presents unique challenges across infrastructure, foundation models,
and downstream applications. This paper provides a comprehensive research
agenda of the LLM supply chain, offering a structured approach to identify
critical challenges and opportunities through the dual lenses of Software
Engineering (SE) and Security & Privacy (S&P). We begin by establishing a clear
definition of the LLM supply chain, encompassing its components and
dependencies. We then analyze each layer of the supply chain, presenting a
vision for robust and secure LLM development, reviewing the current state of
practices and technologies, and identifying key challenges and research
opportunities. This work aims to bridge the existing research gap in
systematically understanding the multifaceted issues within the LLM supply
chain, offering valuable insights to guide future efforts in this rapidly
evolving domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2030 Software Engineering Workshop, co-located with
  FSE'24. This is an extended and revised version of our paper submitted to the
  TOSEM Special Issue: 2030 Software Engineering Roadmap</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Code to Correctness: Closing the Last Mile of Code Generation with
  Hierarchical Debugging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuling Shi, Songsong Wang, Chengcheng Wan, Xiaodong Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models have made significant strides in code generation,
the pass rate of the generated code is bottlenecked on subtle errors, often
requiring human intervention to pass tests, especially for complex problems.
Existing LLM-based debugging systems treat generated programs as monolithic
units, failing to address bugs at multiple levels of granularity, from
low-level syntax errors to high-level algorithmic flaws. In this paper, we
introduce Multi-Granularity Debugger (MGDebugger), a hierarchical code debugger
by isolating, identifying, and resolving bugs at various levels of granularity.
MGDebugger decomposes problematic code into a hierarchical tree structure of
subfunctions, with each level representing a particular granularity of error.
During debugging, it analyzes each subfunction and iteratively resolves bugs in
a bottom-up manner. To effectively test each subfunction, we propose an
LLM-simulated Python executor, which traces code execution and tracks important
variable states to pinpoint errors accurately. Extensive experiments
demonstrate that MGDebugger outperforms existing debugging systems, achieving
an 18.9% improvement in accuracy over seed generations in HumanEval and a 97.6%
repair success rate in HumanEvalFix. Furthermore, MGDebugger effectively fixes
bugs across different categories and difficulty levels, demonstrating its
robustness and effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and data available at https://github.com/YerbaPage/MGDebugger</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparing large language models and human programmers for generating
  programming code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00894v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00894v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenpin Hou, Zhicheng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We systematically evaluated the performance of seven large language models in
generating programming code using various prompt strategies, programming
languages, and task difficulties. GPT-4 substantially outperforms other large
language models, including Gemini Ultra and Claude 2. The coding performance of
GPT-4 varies considerably with different prompt strategies. In most LeetCode
and GeeksforGeeks coding contests evaluated in this study, GPT-4 employing the
optimal prompt strategy outperforms 85 percent of human participants.
Additionally, GPT-4 demonstrates strong capabilities in translating code
between different programming languages and in learning from past errors. The
computational efficiency of the code generated by GPT-4 is comparable to that
of human programmers. These results suggest that GPT-4 has the potential to
serve as a reliable assistant in programming code generation and software
development.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">18</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open Science Practices by Early Career HCI Researchers: Perceptions,
  Challenges, and Benefits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tatiana Chakravorti, Sanjana Gautam, Priya Silverstein, Sarah M. Rajtmajer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many fields of science, including Human-Computer Interaction (HCI), have
heightened introspection in the wake of concerns around reproducibility and
replicability of published findings. Notably, in recent years the HCI community
has worked to implement policy changes and mainstream open science practices.
Our work investigates early-career HCI researchers' perceptions of open science
and engagement with best practices through 18 semi-structured interviews. Our
findings highlight key barriers to the widespread adoption of data and
materials sharing, and preregistration, namely: lack of clear incentives;
cultural resistance; limited training; time constraints; concerns about
intellectual property; and data privacy issues. We observe that small changes
at major conferences like CHI could meaningfully impact community norms. We
offer recommendations to address these barriers and to promote transparency and
openness in HCI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Visualization JUDGE : Can Multimodal Foundation Models Guide
  Visualization Design Through Visual Perception? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Berger, Shusen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models for vision and language are the basis of AI applications
across numerous sectors of society. The success of these models stems from
their ability to mimic human capabilities, namely visual perception in vision
models, and analytical reasoning in large language models. As visual perception
and analysis are fundamental to data visualization, in this position paper we
ask: how can we harness foundation models to advance progress in visualization
design? Specifically, how can multimodal foundation models (MFMs) guide
visualization design through visual perception? We approach these questions by
investigating the effectiveness of MFMs for perceiving visualization, and
formalizing the overall visualization design and optimization space.
Specifically, we think that MFMs can best be viewed as judges, equipped with
the ability to criticize visualizations, and provide us with actions on how to
improve a visualization. We provide a deeper characterization for text-to-image
generative models, and multi-modal large language models, organized by what
these models provide as output, and how to utilize the output for guiding
design decisions. We hope that our perspective can inspire researchers in
visualization on how to approach MFMs for visualization design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Explanations That Anticipate Human Misconceptions Can
  Improve Human Decision-Making Skills 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zana Buçinca, Siddharth Swaroop, Amanda E. Paluch, Finale Doshi-Velez, Krzysztof Z. Gajos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  People's decision-making abilities often fail to improve or may even erode
when they rely on AI for decision-support, even when the AI provides
informative explanations. We argue this is partly because people intuitively
seek contrastive explanations, which clarify the difference between the AI's
decision and their own reasoning, while most AI systems offer "unilateral"
explanations that justify the AI's decision but do not account for users'
thinking. To align human-AI knowledge on decision tasks, we introduce a
framework for generating human-centered contrastive explanations that explain
the difference between AI's choice and a predicted, likely human choice about
the same task. Results from a large-scale experiment (N = 628) demonstrate that
contrastive explanations significantly enhance users' independent
decision-making skills compared to unilateral explanations, without sacrificing
decision accuracy. Amid rising deskilling concerns, our research demonstrates
that incorporating human reasoning into AI design can foster human skill
development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Be There, Be Together, Be Streamed! AR Scenic Live-Streaming for an
  Interactive and Collective Experience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Huang, Zuyu Xu, Yuanhao Zhang, Chengzhong Liu, Yanwei Zhao, Chuhan Shi, Jason Chen Zhao, Xiaojuan Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scenic Live-Streaming (SLS), capturing real-world scenic sites from fixed
cameras without streamers, combines scene immersion and the social and
real-time characteristics of live-streaming into a unique experience. However,
existing SLS affords limited audience interactions to engage them in a
collective experience compared to many other live-streaming genres. It is also
difficult for SLS to recreate important but intangible constituents of
in-person trip experiences, such as cultural activities. To offer a more
interactive, engaging, and meaningful experience, we propose ARSLS (Augmented
Reality Scenic Live-Streaming). Culturally grounded AR objects with awareness
of the live-streamed environment can be overlaid over camera views to provide
additional interactive features while maintaining consistency with the
live-streamed scene. To explore the design space of this new medium, we
developed an ARSLS prototype for a famous landscape in China. A preliminary
study (N=15) provided initial insights for ARSLS design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 figures, to appear in the adjunct proceedings of ISMAR
  2024 and the ISMAR 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Visual Fidelity in Driving Simulations through Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanjun Bu, Hiroshi Yasuda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have made substantial progress in facilitating image
generation and editing. As the technology matures, we see its potential in the
context of driving simulations to enhance the simulated experience. In this
paper, we explore this potential through the introduction of a novel system
designed to boost visual fidelity. Our system, DRIVE (Diffusion-based Realism
Improvement for Virtual Environments), leverages a diffusion model pipeline to
give a simulated environment a photorealistic view, with the flexibility to be
adapted for other applications. We conducted a preliminary user study to assess
the system's effectiveness in rendering realistic visuals and supporting
participants in performing driving tasks. Our work not only lays the groundwork
for future research on the integration of diffusion models in driving
simulations but also provides practical guidelines and best practices for their
application in this context.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of Surface Co-location and Eye-tracking on Mixed Reality
  Typing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cecilia Schmitz, Joshua Reynolds, Scott Kuhl, Keith Vertanen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accuracy and speed are pivotal when typing. We hypothesized that the lack of
tactile feedback on midair mixed reality keyboards may adversely impact
performance. Our first experiment assessed the potential to provide tactile
feedback to users typing in mixed reality by co-locating the virtual keyboard
on a table or a wall. The keyboard was deterministic (without auto-correct),
relied only on the headset's egocentric cameras for sensing, and included
symbol keys. Users preferred and had the highest entry rate of 12
words-per-minute using a midair keyboard. Error rates were similar in all
conditions. Based on user feedback, our second experiment explored ten-finger
typing. We used a novel eye-tracking technique to mitigate accidental key
presses. The technique halved the number of times backspace was pressed and was
preferred by users. However, participants were faster using only their index
fingers without eye-tracking at 11 words-per-minute.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TeachTune: <span class="highlight-title">Review</span>ing Pedagogical Agents Against Diverse Student Profiles
  with Simulated Students 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyoungwook Jin, Minju Yoo, Jeongeon Park, Yokyung Lee, Xu Wang, Juho Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can empower educators to build pedagogical
conversational agents (PCAs) customized for their students. As students have
different prior knowledge and motivation levels, educators must evaluate the
adaptivity of their PCAs to diverse students. Existing chatbot evaluation
methods (e.g., direct chat and benchmarks) are either manually intensive for
multiple iterations or limited to testing only single-turn interactions. We
present TeachTune, where educators can create simulated students and review
PCAs by observing automated chats between PCAs and simulated students. Our
technical pipeline instructs an LLM-based student to simulate prescribed
knowledge levels and characteristics, helping educators explore diverse
conversation patterns. Our pipeline could produce simulated students whose
behaviors correlate highly to their input knowledge and motivation levels
within 5% and 10% accuracy gaps. Thirty science teachers designed PCAs in a
between-subjects study, and using TeachTune resulted in a lower task load and
higher student profile coverage over a baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gamifying XAI: Enhancing AI Explainability for Non-technical Users
  through LLM-Powered Narrative Gamifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhe You, Jian Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) has become tightly integrated into modern
technology, yet existing exploratory visualizations for explainable AI (XAI)
are primarily designed for users with technical expertise. This leaves everyday
users, who also regularly interact with AI systems, with limited resources to
explore or understand AI technologies they use. We propose a novel framework
that enables non-technical users to collect insights by conversing directly
with visualization elements via LLM-powered narrative gamifications. We
implemented a prototype that utilizes such gamification to facilitate
non-technical users' exploration of AI embedding projections. We conducted a
comparative study with 10 participants to assess our prototype quantitatively
and qualitatively. Our study results indicate that although our prototype
effectively enhances non-technical users' AI/XAI knowledge, and users believe
they learn more through the gamification feature, it remains inconclusive
whether the gamification itself leads to further improvements in understanding.
In addition, opinions among participants regarding the framework's engagement
are mixed: some believe it enhances their exploration of the visualizations,
while others feel it disrupts their workflow.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IdeaSynth: Iterative Research Idea Development Through Evolving and
  Composing Idea Facets with Literature-Grounded Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Pu, K. J. Kevin Feng, Tovi Grossman, Tom Hope, Bhavana Dalvi Mishra, Matt Latzke, Jonathan Bragg, Joseph Chee Chang, Pao Siangliulue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research ideation involves broad exploring and deep refining ideas. Both
require deep engagement with literature. Existing tools focus primarily on idea
broad generation, yet offer little support for iterative specification,
refinement, and evaluation needed to further develop initial ideas. To bridge
this gap, we introduce IdeaSynth, a research idea development system that uses
LLMs to provide literature-grounded feedback for articulating research
problems, solutions, evaluations, and contributions. IdeaSynth represents these
idea facets as nodes on a canvas, and allow researchers to iteratively refine
them by creating and exploring variations and composing them. Our lab study
(N=20) showed that participants, while using IdeaSynth, explored more
alternative ideas and expanded initial ideas with more details compared to a
strong LLM-based baseline. Our deployment study (N=7) demonstrated that
participants effectively used IdeaSynth for real-world research projects at
various ideation stages from developing initial ideas to revising framings of
mature manuscripts, highlighting the possibilities to adopt IdeaSynth in
researcher's workflows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Development of a Mouse for Individuals Without Upper Limbs Using Arduino
  Technology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alfonso Gunsha, Luis Chuquimarca, Pedro Pardo, David Herrera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This project focuses on the design and construction of a prototype mouse
based on the Arduino platform, intended for individuals without upper limbs to
use computers more effectively. The prototype comprises a microcontroller
responsible for processing signals from the MPU-6050 sensor, used as a
reference for cursor position, and foot-operated buttons for right and
left-click functions. Its design enables cursor control through head movements,
providing users with an easy and intuitive way to interact with the computer's
graphical interface. Feasibility testing was conducted through experimental
trials, resulting in ideal accuracy and precision. These trials indicate that
the device is viable for use in individuals without upper limbs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing the Travel Experience for People with Visual Impairments
  through Multimodal Interaction: Navi<span class="highlight-title">GPT</span>, A Real-Time AI-Driven Mobile
  Navigation System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Zhang, Nicholas J. Falletta, Jingyi Xie, Rui Yu, Sooyeon Lee, Syed Masum Billah, John M. Carroll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assistive technologies for people with visual impairments (PVI) have made
significant advancements, particularly with the integration of artificial
intelligence (AI) and real-time sensor technologies. However, current solutions
often require PVI to switch between multiple apps and tools for tasks like
image recognition, navigation, and obstacle detection, which can hinder a
seamless and efficient user experience. In this paper, we present NaviGPT, a
high-fidelity prototype that integrates LiDAR-based obstacle detection,
vibration feedback, and large language model (LLM) responses to provide a
comprehensive and real-time navigation aid for PVI. Unlike existing
applications such as Be My AI and Seeing AI, NaviGPT combines image recognition
and contextual navigation guidance into a single system, offering continuous
feedback on the user's surroundings without the need for app-switching.
Meanwhile, NaviGPT compensates for the response delays of LLM by using location
and sensor data, aiming to provide practical and efficient navigation support
for PVI in dynamic environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures, this work has been accepted by the 2025 ACM
  International Conference on Supporting Group Work (GROUP '25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TR-LLM: Integrating Trajectory Data for Scene-Aware LLM-Based Human
  Action Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kojiro Takeyama, Yimeng Liu, Misha Sra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate prediction of human behavior is crucial for AI systems to
effectively support real-world applications, such as autonomous robots
anticipating and assisting with human tasks. Real-world scenarios frequently
present challenges such as occlusions and incomplete scene observations, which
can compromise predictive accuracy. Thus, traditional video-based methods often
struggle due to limited temporal and spatial perspectives. Large Language
Models (LLMs) offer a promising alternative. Having been trained on a large
text corpus describing human behaviors, LLMs likely encode plausible sequences
of human actions in a home environment. However, LLMs, trained primarily on
text data, lack inherent spatial awareness and real-time environmental
perception. They struggle with understanding physical constraints and spatial
geometry. Therefore, to be effective in a real-world spatial scenario, we
propose a multimodal prediction framework that enhances LLM-based action
prediction by integrating physical constraints derived from human trajectories.
Our experiments demonstrate that combining LLM predictions with trajectory data
significantly improves overall prediction performance. This enhancement is
particularly notable in situations where the LLM receives limited scene
information, highlighting the complementary nature of linguistic knowledge and
physical constraints in understanding and anticipating human behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NarrativeBridge: Enhancing Video Captioning with Causal-Temporal
  Narrative 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06499v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06499v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asmar Nadeem, Faegheh Sardari, Robert Dawes, Syed Sameed Husain, Adrian Hilton, Armin Mustafa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing video captioning benchmarks and models lack coherent representations
of causal-temporal narrative, which is sequences of events linked through cause
and effect, unfolding over time and driven by characters or agents. This lack
of narrative restricts models' ability to generate text descriptions that
capture the causal and temporal dynamics inherent in video content. To address
this gap, we propose NarrativeBridge, an approach comprising of: (1) a novel
Causal-Temporal Narrative (CTN) captions benchmark generated using a large
language model and few-shot prompting, explicitly encoding cause-effect
temporal relationships in video descriptions, evaluated automatically to ensure
caption quality and relevance and validated through human evaluation; and (2) a
dedicated Cause-Effect Network (CEN) architecture with separate encoders for
capturing cause and effect dynamics independently, enabling effective learning
and generation of captions with causal-temporal narrative. Extensive
experiments demonstrate that CEN significantly outperforms state-of-the-art
models, including fine-tuned vision-language models, and is more accurate in
articulating the causal and temporal aspects of video content than the second
best model (GIT): 17.88 and 17.44 CIDEr on the MSVD and MSR-VTT datasets,
respectively. Cross-dataset evaluations further showcase CEN's strong
generalization capabilities. The proposed framework understands and generates
nuanced text descriptions with intricate causal-temporal narrative structures
present in videos, addressing a critical limitation in video captioning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Twenty-Four Years of Empirical Research on Trust in AI: A Bibliometric
  <span class="highlight-title">Review</span> of Trends, Overlooked Issues, and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michaela Benk, Sophie Kerstan, Florian v. Wangenheim, Andrea Ferrario
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trust is widely regarded as a critical component to building artificial
intelligence (AI) systems that people will use and safely rely upon. As
research in this area continues to evolve, it becomes imperative that the
research community synchronizes its empirical efforts and aligns on the path
toward effective knowledge creation. To lay the groundwork toward achieving
this objective, we performed a comprehensive bibliometric analysis,
supplemented with a qualitative content analysis of over two decades of
empirical research measuring trust in AI, comprising 1'156 core articles and
36'306 cited articles across multiple disciplines. Our analysis reveals several
"elephants in the room" pertaining to missing perspectives in global
discussions on trust in AI, a lack of contextualized theoretical models and a
reliance on exploratory methodologies. We highlight strategies for the
empirical research community that are aimed at fostering an in-depth
understanding of trust in AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revised version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eating Speed Measurement Using Wrist-Worn IMU Sensors Towards
  Free-Living Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.05376v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.05376v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunzhuo Wang, T. Sunil Kumar, Walter De Raedt, Guido Camps, Hans Hallez, Bart Vanrumste
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Eating speed is an important indicator that has been widely investigated in
nutritional studies. The relationship between eating speed and several
intake-related problems such as obesity, diabetes, and oral health has received
increased attention from researchers. However, existing studies mainly use
self-reported questionnaires to obtain participants' eating speed, where they
choose options from slow, medium, and fast. Such a non-quantitative method is
highly subjective and coarse at the individual level. This study integrates two
classical tasks in automated food intake monitoring domain: bite detection and
eating episode detection, to advance eating speed measurement in
near-free-living environments automatically and objectively. Specifically, a
temporal convolutional network combined with a multi-head attention module
(TCN-MHA) is developed to detect bites (including eating and drinking gestures)
from IMU data. The predicted bite sequences are then clustered into eating
episodes. Eating speed is calculated by using the time taken to finish the
eating episode to divide the number of bites. To validate the proposed approach
on eating speed measurement, a 7-fold cross validation is applied to the
self-collected fine-annotated full-day-I (FD-I) dataset, and a holdout
experiment is conducted on the full-day-II (FD-II) dataset. The two datasets
are collected from 61 participants with a total duration of 513 h, which are
publicly available. Experimental results show that the proposed approach
achieves a mean absolute percentage error (MAPE) of 0.110 and 0.146 in the FD-I
and FD-II datasets, respectively, showcasing the feasibility of automated
eating speed measurement in near-free-living environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The manuscript has been accepted in IEEE Journal of Biomedical and
  Health Informatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Encode-Store-Retrieve: Augmenting Human Memory through Language-Encoded
  Egocentric Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.05822v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.05822v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junxiao Shen, John Dudley, Per Ola Kristensson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We depend on our own memory to encode, store, and retrieve our experiences.
However, memory lapses can occur. One promising avenue for achieving memory
augmentation is through the use of augmented reality head-mounted displays to
capture and preserve egocentric videos, a practice commonly referred to as
lifelogging. However, a significant challenge arises from the sheer volume of
video data generated through lifelogging, as the current technology lacks the
capability to encode and store such large amounts of data efficiently. Further,
retrieving specific information from extensive video archives requires
substantial computational power, further complicating the task of quickly
accessing desired content. To address these challenges, we propose a memory
augmentation agent that involves leveraging natural language encoding for video
data and storing them in a vector database. This approach harnesses the power
of large vision language models to perform the language encoding process.
Additionally, we propose using large language models to facilitate natural
language querying. Our agent underwent extensive evaluation using the QA-Ego4D
dataset and achieved state-of-the-art results with a BLEU score of 8.3,
outperforming conventional machine learning models that scored between 3.4 and
5.8. Additionally, we conducted a user study in which participants interacted
with the human memory augmentation agent through episodic memory and open-ended
questions. The results of this study show that the agent results in
significantly better recall performance on episodic memory tasks compared to
human participants. The results also highlight the agent's practical
applicability and user acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Panda or not Panda? Understanding Adversarial Attacks with Interactive
  Visualization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13656v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13656v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhe You, Jarvis Tse, Jian Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial machine learning (AML) studies attacks that can fool machine
learning algorithms into generating incorrect outcomes as well as the defenses
against worst-case attacks to strengthen model robustness. Specifically for
image classification, it is challenging to understand adversarial attacks due
to their use of subtle perturbations that are not human-interpretable, as well
as the variability of attack impacts influenced by diverse methodologies,
instance differences, and model architectures. Through a design study with AML
learners and teachers, we introduce AdvEx, a multi-level interactive
visualization system that comprehensively presents the properties and impacts
of evasion attacks on different image classifiers for novice AML learners. We
quantitatively and qualitatively assessed AdvEx in a two-part evaluation
including user studies and expert interviews. Our results show that AdvEx is
not only highly effective as a visualization tool for understanding AML
mechanisms, but also provides an engaging and enjoyable learning experience,
thus demonstrating its overall benefits for AML learners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Successfully Guiding Humans with Imperfect Instructions by Highlighting
  Potential Errors and Suggesting Corrections <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingjun Zhao, Khanh Nguyen, Hal Daumé III
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models will inevitably err in situations with which they are
unfamiliar. However, by effectively communicating uncertainties, they can still
guide humans toward making sound decisions in those contexts. We demonstrate
this idea by developing HEAR, a system that can successfully guide humans in
simulated residential environments despite generating potentially inaccurate
instructions. Diverging from systems that provide users with only the
instructions they generate, HEAR warns users of potential errors in its
instructions and suggests corrections. This rich uncertainty information
effectively prevents misguidance and reduces the search space for users.
Evaluation with 80 users shows that HEAR achieves a 13% increase in success
rate and a 29% reduction in final location error distance compared to only
presenting instructions to users. Interestingly, we find that offering users
possibilities to explore, HEAR motivates them to make more attempts at the
task, ultimately leading to a higher success rate. To our best knowledge, this
work is the first to show the practical benefits of uncertainty communication
in a long-horizon sequential decision-making problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Programming Languages <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Code to Correctness: Closing the Last Mile of Code Generation with
  Hierarchical Debugging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuling Shi, Songsong Wang, Chengcheng Wan, Xiaodong Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models have made significant strides in code generation,
the pass rate of the generated code is bottlenecked on subtle errors, often
requiring human intervention to pass tests, especially for complex problems.
Existing LLM-based debugging systems treat generated programs as monolithic
units, failing to address bugs at multiple levels of granularity, from
low-level syntax errors to high-level algorithmic flaws. In this paper, we
introduce Multi-Granularity Debugger (MGDebugger), a hierarchical code debugger
by isolating, identifying, and resolving bugs at various levels of granularity.
MGDebugger decomposes problematic code into a hierarchical tree structure of
subfunctions, with each level representing a particular granularity of error.
During debugging, it analyzes each subfunction and iteratively resolves bugs in
a bottom-up manner. To effectively test each subfunction, we propose an
LLM-simulated Python executor, which traces code execution and tracks important
variable states to pinpoint errors accurately. Extensive experiments
demonstrate that MGDebugger outperforms existing debugging systems, achieving
an 18.9% improvement in accuracy over seed generations in HumanEval and a 97.6%
repair success rate in HumanEvalFix. Furthermore, MGDebugger effectively fixes
bugs across different categories and difficulty levels, demonstrating its
robustness and effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and data available at https://github.com/YerbaPage/MGDebugger</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparing large language models and human programmers for generating
  programming code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00894v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00894v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenpin Hou, Zhicheng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We systematically evaluated the performance of seven large language models in
generating programming code using various prompt strategies, programming
languages, and task difficulties. GPT-4 substantially outperforms other large
language models, including Gemini Ultra and Claude 2. The coding performance of
GPT-4 varies considerably with different prompt strategies. In most LeetCode
and GeeksforGeeks coding contests evaluated in this study, GPT-4 employing the
optimal prompt strategy outperforms 85 percent of human participants.
Additionally, GPT-4 demonstrates strong capabilities in translating code
between different programming languages and in learning from past errors. The
computational efficiency of the code generated by GPT-4 is comparable to that
of human programmers. These results suggest that GPT-4 has the potential to
serve as a reliable assistant in programming code generation and software
development.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Networking and Internet Architecture <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Task-Oriented Communication Framework for Real-Time Collaborative
  Vision Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengru Fang, Jingjing Wang, Yanan Ma, Yihang Tao, Yiqin Deng, Xianhao Chen, Yuguang Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cooperative perception enhances sensing in multi-robot and vehicular networks
by aggregating information from multiple agents, improving perception accuracy
and range. However, mobility and non-rigid sensor mounts introduce extrinsic
calibration errors, necessitating online calibration, which is complicated by
limited overlap in sensing regions. Maintaining fresh information is crucial
for timely and accurate sensing. To address calibration errors and ensure both
perception accuracy and transmission timeliness, we propose a Robust
Task-Oriented Communication framework (R-TOCOM) that optimizes calibration and
feature transmission in both deployment and streaming phases. First, we
formulate an Age of Perceived Targets (AoPT) minimization problem to capture
information freshness. Then, in the deployment phase, we introduce a
channel-aware self-calibration technique based on re-identification (Re-ID).
This technique adaptively compresses key-point features according to channel
capacities, effectively addressing calibration issues via spatial and temporal
cross-camera correlations. In the streaming phase, we tackle the trade-off
between bandwidth and inference accuracy by integrating an Information
Bottleneck (IB)-based encoding method that adjusts video compression rates
based on task relevance, thereby reducing communication overhead and latency.
To mitigate performance degradation from packet loss, we introduce a priority
network that filters corrupted features. Extensive studies demonstrate our
framework outperforms five baselines, improving multiple object detection
accuracy (MODA) by 25.49% and reducing communication costs by 51.36% under
severe channel condition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring 5G Network Performance: Comparison of Inner and Outer City
  Areas in Phetchaburi Province 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phisit Pornpongtechavanich, Therdpong Daengsi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of 5G technology has transformed various aspects of life,
including tourism, by enabling people worldwide to communicate and travel with
ease. Traveling to different places and countries is now seamless, removing
language barriers and facilitating easy access to information on culture,
accommodation, and tourist attractions. Additionally, access to applications
that facilitate quicker language translation further enhances the travel
experience. Phetchaburi Province holds significant importance as a global
tourist destination. UNESCO has recognized Phetchaburi as a member of the
UNESCO Creative Cities Network (UCCN), comprising one of 49 cities worldwide
acknowledged for their creative city initiatives. Phetchaburi Province stands
as the 5th city in Thailand to receive this designation. This research
investigated 5G performance in Phetchaburi Province, both the inner and outer
city, focusing on download and upload speeds. The results indicate that there
is widespread 5G coverage throughout Phetchaburi Province, including urban and
rural areas, especially for the 5G network with a good performance provided by
one of the mobile network operators. In addition, the statistical analysis
reveals differences in 5G performances between the inner city and the outer
city of Phetchaburi Province, particularly for download speeds (p-value <
0.001).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Security Testbed for Preempting Attacks against Supercomputing
  Infrastructure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09602v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09602v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phuong Cao, Zbigniew Kalbarczyk, Ravishankar Iyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Securing HPC has a unique threat model. Untrusted, malicious code exploiting
the concentrated computing power may exert an outsized impact on the shared,
open-networked environment in HPC, unlike well-isolated VM tenants in public
clouds. Therefore, preempting attacks targeting supercomputing systems before
damage remains the top security priority. The main challenge is that noisy
attack attempts and unreliable alerts often mask \emph{real attacks}, causing
permanent damages such as system integrity violations and data breaches. This
paper describes a security testbed embedded in live traffic of a supercomputer
at the National Center for Supercomputing Applications (NCSA). The objective is
to demonstrate attack \textit{preemption}, i.e., stopping system compromise and
data breaches at petascale supercomputers. Deployment of our testbed at NCSA
enables the following key contributions:
  1) Insights from characterizing unique \textit{attack patterns} found in real
security logs of more than 200 security incidents curated in the past two
decades at NCSA.
  2) Deployment of an attack visualization tool to illustrate the challenges of
identifying real attacks in HPC environments and to support security operators
in interactive attack analyses.
  3) Demonstrate the utility of the testbed by running novel models, such as
Factor-Graph-based models, to preempt a real-world ransomware family.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Third Annual Workshop on Cyber Security in
  High-Performance Computing (S-HPC 24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Optimization Framework for Channel Simulation-Based Base
  Station Placement and Transmission Power Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20778v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20778v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koya Sato, Katsuya Suto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study proposes an adaptive experimental design framework for a
channel-simulation-based base station (BS) design that supports the joint
optimization of transmission power and placement. We consider a system in which
multiple transmitters provide wireless services over a shared frequency band.
Our objective is to maximize the average throughput within an area of interest.
System operators can design the system configurations prior to deployment by
iterating them through channel simulations and updating the parameters.
However, accurate channel simulations are computationally expensive; therefore,
it is preferable to configure the system using a limited number of simulation
iterations. We develop a solver for the problem based on Bayesian optimization
(BO), a black-box optimization method. The numerical results demonstrate that
our proposed framework can achieve 18-22% higher throughput performance than
conventional placement and power optimization strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 7 figures, 1 table. This work has been published in IEEE
  Networking Letters under CC BY-NC-ND 4.0 (DOI: 10.1109/LNET.2024.3469175)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MindFlayer: Efficient Asynchronous Parallel SGD in the Presence of
  Heterogeneous and Random Worker Compute Times 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artavazd Maranjyan, Omar Shaikh Omar, Peter Richtárik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of minimizing the expectation of smooth nonconvex
functions with the help of several parallel workers whose role is to compute
stochastic gradients. In particular, we focus on the challenging situation
where the workers' compute times are arbitrarily heterogeneous and random. In
the simpler regime characterized by arbitrarily heterogeneous but deterministic
compute times, Tyurin and Richt\'arik (NeurIPS 2023) recently designed the
first theoretically optimal asynchronous SGD method, called Rennala SGD, in
terms of a novel complexity notion called time complexity. The starting point
of our work is the observation that Rennala SGD can have arbitrarily bad
performance in the presence of random compute times -- a setting it was not
designed to handle. To advance our understanding of stochastic optimization in
this challenging regime, we propose a new asynchronous SGD method, for which we
coin the name MindFlayer SGD. Our theory and empirical results demonstrate the
superiority of MindFlayer SGD over existing baselines, including Rennala SGD,
in cases when the noise is heavy tailed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Slim-ABC: An Optimized Atomic Broadcast Protocol 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nasit S Sony, Xianzhong Ding, Mukesh Singhal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Byzantine Agreement (BA) problem is a fundamental challenge in
distributed systems, focusing on achieving reaching an agreement among parties,
some of which may behave maliciously. With the rise of cryptocurrencies, there
has been significant interest in developing atomic broadcast protocols, which
facilitate agreement on a subset of parties' requests. However, these protocols
often come with high communication complexity ($O(ln^2 + \lambda n^3 \log n)$,
where $l$ is the bit length of the input, $n$ is the number of parties, and
$\lambda$ represents the security parameter bit length). This can lead to
inefficiency, especially when the requests across parties exhibit little
variation, resulting in unnecessary resource consumption. In this paper, we
introduce Slim-ABC, a novel atomic broadcast protocol that eliminates the
$O(ln^2 + \lambda n^3 \log n)$ term associated with traditional atomic
broadcast protocols. While Slim-ABC reduces the number of accepted requests, it
significantly mitigates resource wastage, making it more efficient. The
protocol leverages the asynchronous common subset and provable-broadcast
mechanisms to achieve a communication complexity of $O(ln^2 + \lambda n^2)$.
Despite the trade-off in accepted requests, Slim-ABC maintains robust security
by allowing only a fraction ($f+1$) of parties to broadcast requests. We
present an extensive efficiency analysis of Slim-ABC, evaluating its
performance across key metrics such as message complexity, communication
complexity, and time complexity. Additionally, we provide a rigorous security
analysis, demonstrating that Slim-ABC satisfies the \textit{agreement},
\textit{validity}, and \textit{totality} properties of the asynchronous common
subset protocol.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancements in Robotics Process Automation: A Novel Model with Enhanced
  Empirical Validation and Theoretical Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gokul Pandy, Vivekananda Jayaram, Manjunatha Sughaturu Krishnappa, Balaji Shesharao Ingole, Koushik Kumar Ganeeb, Shenson Joseph
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotics Process Automation is revolutionizing business operations by
significantly enhancing efficiency, productivity, and operational excellence
across various industries. This manuscript delivers a comprehensive review of
recent advancements in RPA technologies and proposes a novel model designed to
elevate RPA capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages. European Journal of Computer Science and Information
  Technology 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lazy Qubit Reordering for Accelerating Parallel State-Vector-based
  Quantum Circuit Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Teranishi, Shoma Hiraoka, Wataru Mizukami, Masao Okita, Fumihiko Ino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes two quantum operation scheduling methods for accelerating
parallel state-vector-based quantum circuit simulation using multiple graphics
processing units (GPUs). The proposed methods reduce all-to-all communication
caused by qubit reordering (QR), which can dominate the overhead of parallel
simulation. Our approach eliminates redundant QRs by introducing intentional
delays in QR communications such that multiple QRs can be aggregated into a
single QR. The delays are carefully introduced based on the principles of
time-space tiling, or a cache optimization technique for classical computers,
which we use to arrange the execution order of quantum operations. Moreover, we
present an extended scheduling method for the hierarchical interconnection of
GPU cluster systems to avoid slow inter-node communication. We develop these
methods tailored for two primary procedures in variational quantum eigensolver
(VQE) simulation: quantum state update (QSU) and expectation value computation
(EVC). Experimental validation on 32-GPU executions demonstrates acceleration
in QSU and EVC -- up to 54$\times$ and 606$\times$, respectively -- compared to
existing methods. Moreover, our extended scheduling method further reduced
communication time by up to 15\% in a two-layered interconnected cluster
system. Our approach is useful for any quantum circuit simulations, including
QSU and/or EVC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 180 Days After EIP-4844: Will Blob Sharing Solve Dilemma for Small
  Rollups? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suhyeon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of blobs through EIP-4844 has significantly reduced the Data
Availability (DA) costs for rollups on Ethereum. However, due to the fixed size
of blobs at 128 KB, rollups with low data throughput face a dilemma: they
either use blobs inefficiently or decrease the frequency of DA submissions.
Blob sharing, where multiple rollups share a single blob, has been proposed as
a solution to this problem. This paper examines the effectiveness of blob
sharing based on real-world data collected approximately six months after the
implementation of EIP-4844. By simulating cost changes using a simple blob
sharing format, we demonstrate that blob sharing can substantially improve the
costs and DA service quality for small rollups, effectively resolving their
dilemma. Notably, we observed cost reductions in USD exceeding 90% for most of
the rollups when they cooperate, attributable to the smoothing effect of the
blob base fee achieved through blob sharing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dispersion on Time-Varying Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashish Saxena, Tanvir Kaur, Kaushik Mondal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dispersion involves the coordination of $k \leq n$ agents on a graph of
size $n$ to reach a configuration where at each node at most one agent can be
present. It is a well-studied problem. Also, this problem is studied on dynamic
graphs with $n$ nodes where at each discrete time step the graph is a connected
sub-graph of the complete graph $K_n$. An optimal algorithm is provided
assuming global communication and 1-hop visibility of the agents. How this
problem pans out on Time-Varying Graphs (TVG) is an open question in the
literature. In this work we study this problem on TVG where at each discrete
time step the graph is a connected sub-graph of an underlying graph $G$ (known
as a footprint) consisting of $n$ nodes. We have the following results even if
only one edge from $G$ is missing in the connected sub-graph at any time step
and all agents start from a rooted initial configuration. Even with unlimited
memory at each agent and 1-hop visibility, it is impossible to solve dispersion
for $n$ co-located agents on a TVG in the local communication model.
Furthermore, even with unlimited memory at each agent but without 1-hop
visibility, it is impossible to achieve dispersion for $n$ co-located agents in
the global communication model. From the positive side, the existing algorithm
for dispersion on dynamic graphs with the assumptions of global communication
and 1-hop visibility works on TVGs as well. This fact and the impossibility
results push us to come up with a modified definition of the dispersion problem
on TVGs, as one needs to start with more than $n$ agents if the objective is to
drop the strong assumptions of global communication and 1-hop visibility. Then,
we provide an algorithm to solve the modified dispersion problem on TVG
starting with $n+1$ agents with $O(\log n)$ memory per agent while dropping
both the assumptions of global communication and 1-hop visibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Starting Point for Dynamic Community Detection with Leiden Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11658v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11658v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhajit Sahu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world graphs often evolve over time, making community or cluster
detection a crucial task. In this technical report, we extend three dynamic
approaches - Naive-dynamic (ND), Delta-screening (DS), and Dynamic Frontier
(DF) - to our multicore implementation of the Leiden algorithm, known for its
high-quality community detection. Our experiments, conducted on a server with a
64-core AMD EPYC-7742 processor, show that ND, DS, and DF Leiden achieve
average speedups of 1.37x, 1.47x, and 1.98x on large graphs with random batch
updates, compared to the Static Leiden algorithm - while scaling at a rate of
1.6x for every doubling of threads. To our knowledge, this is the first attempt
to apply dynamic approaches to the Leiden algorithm. We hope these early
results pave the way for further development of dynamic approaches for evolving
graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 10 figures, 2 tables. arXiv admin note: substantial text
  overlap with arXiv:2404.19634</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-04T00:00:00Z">2024-10-04</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Software Engineering <span class="chip" style="font-size: 60%">24</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Survey</span> on Code Generation for Low resource and Domain Specific
  Programming Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sathvik Joel, Jie JW Wu, Fatemeh H. Fard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown impressive capabilities in code
generation for popular programming languages. However, their performance on
Low-Resource Programming Languages (LRPLs) and Domain-Specific Languages (DSLs)
remains a significant challenge, affecting millions of developers-3.5 million
users in Rust alone-who cannot fully utilize LLM capabilities. LRPLs and DSLs
encounter unique obstacles, including data scarcity and, for DSLs, specialized
syntax that is poorly represented in general-purpose datasets.
  Addressing these challenges is crucial, as LRPLs and DSLs enhance development
efficiency in specialized domains, such as finance and science. While several
surveys discuss LLMs in software engineering, none focus specifically on the
challenges and opportunities associated with LRPLs and DSLs. Our survey fills
this gap by systematically reviewing the current state, methodologies, and
challenges in leveraging LLMs for code generation in these languages. We
filtered 111 papers from over 27,000 published studies between 2020 and 2024 to
evaluate the capabilities and limitations of LLMs in LRPLs and DSLs. We report
the LLMs used, benchmarks, and metrics for evaluation, strategies for enhancing
performance, and methods for dataset collection and curation.
  We identified four main evaluation techniques and several metrics for
assessing code generation in LRPLs and DSLs. Our analysis categorizes
improvement methods into six groups and summarizes novel architectures proposed
by researchers. Despite various techniques and metrics, a standard approach and
benchmark dataset for evaluating code generation in LRPLs and DSLs are lacking.
This survey serves as a resource for researchers and practitioners at the
intersection of LLMs, software engineering, and specialized programming
languages, laying the groundwork for future advancements in code generation for
LRPLs and DSLs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SWE-bench Multimodal: Do AI Systems Generalize to Visual Software
  Domains? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Yang, Carlos E. Jimenez, Alex L. Zhang, Kilian Lieret, Joyce Yang, Xindi Wu, Ori Press, Niklas Muennighoff, Gabriel Synnaeve, Karthik R. Narasimhan, Diyi Yang, Sida I. Wang, Ofir Press
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous systems for software engineering are now capable of fixing bugs
and developing features. These systems are commonly evaluated on SWE-bench
(Jimenez et al., 2024a), which assesses their ability to solve software issues
from GitHub repositories. However, SWE-bench uses only Python repositories,
with problem statements presented predominantly as text and lacking visual
elements such as images. This limited coverage motivates our inquiry into how
existing systems might perform on unrepresented software engineering domains
(e.g., front-end, game development, DevOps), which use different programming
languages and paradigms. Therefore, we propose SWE-bench Multimodal (SWE-bench
M), to evaluate systems on their ability to fix bugs in visual, user-facing
JavaScript software. SWE-bench M features 617 task instances collected from 17
JavaScript libraries used for web interface design, diagramming, data
visualization, syntax highlighting, and interactive mapping. Each SWE-bench M
task instance contains at least one image in its problem statement or unit
tests. Our analysis finds that top-performing SWE-bench systems struggle with
SWE-bench M, revealing limitations in visual problem-solving and cross-language
generalization. Lastly, we show that SWE-agent's flexible language-agnostic
features enable it to substantially outperform alternatives on SWE-bench M,
resolving 12% of task instances compared to 6% for the next best system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Code Preference via Synthetic Evolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Liu, Thanh Nguyen, Mingyue Shang, Hantian Ding, Xiaopeng Li, Yu Yu, Varun Kumar, Zijian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently demonstrated remarkable coding
capabilities. However, assessing code generation based on well-formed
properties and aligning it with developer preferences remains challenging. In
this paper, we explore two key questions under the new challenge of code
preference learning: (i) How do we train models to predict meaningful
preferences for code? and (ii) How do human and LLM preferences align with
verifiable code properties and developer code tastes? To this end, we propose
CodeFavor, a framework for training pairwise code preference models from
synthetic evolution data, including code commits and code critiques. To
evaluate code preferences, we introduce CodePrefBench, a benchmark comprising
1364 rigorously curated code preference tasks to cover three verifiable
properties-correctness, efficiency, and security-along with human preference.
Our evaluation shows that CodeFavor holistically improves the accuracy of
model-based code preferences by up to 28.8%. Meanwhile, CodeFavor models can
match the performance of models with 6-9x more parameters while being 34x more
cost-effective. We also rigorously validate the design choices in CodeFavor via
a comprehensive set of controlled experiments. Furthermore, we discover the
prohibitive costs and limitations of human-based code preference: despite
spending 23.4 person-minutes on each task, 15.1-40.3% of tasks remain unsolved.
Compared to model-based preference, human preference tends to be more accurate
under the objective of code correctness, while being sub-optimal for
non-functional objectives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MeDeT: Medical Device Digital Twins Creation with Few-shot Meta-learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hassan Sartaj, Shaukat Ali, Julie Marie Gjøby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Testing healthcare Internet of Things (IoT) applications at system and
integration levels necessitates integrating numerous medical devices of various
types. Challenges of incorporating medical devices are: (i) their continuous
evolution, making it infeasible to include all device variants, and (ii)
rigorous testing at scale requires multiple devices and their variants, which
is time-intensive, costly, and impractical. Our collaborator, Oslo City's
health department, faced these challenges in developing automated test
infrastructure, which our research aims to address. In this context, we propose
a meta-learning-based approach (MeDeT) to generate digital twins (DTs) of
medical devices and adapt DTs to evolving devices. We evaluate MeDeT in
OsloCity's context using five widely-used medical devices integrated with a
real-world healthcare IoT application. Our evaluation assesses MeDeT's ability
to generate and adapt DTs across various devices and versions using different
few-shot methods, the fidelity of these DTs, the scalability of operating 1000
DTs concurrently, and the associated time costs. Results show that MeDeT can
generate DTs with over 96% fidelity, adapt DTs to different devices and newer
versions with reduced time cost (around one minute), and operate 1000 DTs in a
scalable manner while maintaining the fidelity level, thus serving in place of
physical devices for testing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-model Approach for Video Data Retrieval in Autonomous Vehicle
  Development 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesper Knapp, Klas Moberg, Yuchuan Jin, Simin Sun, Miroslaw Staron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving software generates enormous amounts of data every second,
which software development organizations save for future analysis and testing
in the form of logs. However, given the vast size of this data, locating
specific scenarios within a collection of vehicle logs can be challenging.
Writing the correct SQL queries to find these scenarios requires engineers to
have a strong background in SQL and the specific databases in question, further
complicating the search process. This paper presents and evaluates a pipeline
that allows searching for specific scenarios in log collections using natural
language descriptions instead of SQL. The generated descriptions were evaluated
by engineers working with vehicle logs at the Zenseact on a scale from 1 to 5.
Our approach achieved a mean score of 3.3, demonstrating the potential of using
a multi-model architecture to improve the software development workflow. We
also present an interface that can visualize the query process and visualize
the results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative AI in the Software Engineering Domain: Tensions of
  Occupational Identity and Patterns of Identity Protection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anuschka Schmitt, Krzysztof Z. Gajos, Osnat Mokryn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The adoption of generative Artificial Intelligence (GAI) in organizational
settings calls into question workers' roles, and relatedly, the implications
for their long-term skill development and domain expertise. In our qualitative
study in the software engineering domain, we build on the theoretical lenses of
occupational identity and self-determination theory to understand how and why
software engineers make sense of GAI for their work. We find that engineers'
sense-making is contingent on domain expertise, as juniors and seniors felt
their needs for competence, autonomy, and relatedness to be differently
impacted by GAI. We shed light on the importance of the individual's role in
preserving tacit domain knowledge as engineers engaged in sense-making that
protected their occupational identity. We illustrate how organizations play an
active role in shaping workers' sense-making process and propose design
guidelines on how organizations and system designers can facilitate the impact
of technological change on workers' occupational identity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-Aware Environment Simulation of Medical Devices Digital
  Twins 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hassan Sartaj, Shaukat Ali, Julie Marie Gjøby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Smart medical devices are an integral component of the healthcare Internet of
Things (IoT), providing patients with various healthcare services through an
IoT-based application. Ensuring the dependability of such applications through
system and integration-level testing mandates the physical integration of
numerous medical devices, which is costly and impractical. In this context,
digital twins of medical devices play an essential role in facilitating testing
automation. Testing with digital twins without accounting for uncertain
environmental factors of medical devices leaves many functionalities of
IoT-based healthcare applications untested. In addition, digital twins
operating without environmental factors remain out of sync and uncalibrated
with their corresponding devices functioning in the real environment. To deal
with these challenges, in this paper, we propose a model-based approach (EnvDT)
for modeling and simulating the environment of medical devices' digital twins
under uncertainties. We empirically evaluate the EnvDT using three medicine
dispensers, Karie, Medido, and Pilly connected to a real-world IoT-based
healthcare application. Our evaluation targets analyzing the coverage of
environment models and the diversity of uncertain scenarios generated for
digital twins. Results show that EnvDT achieves approximately 61% coverage of
environment models and generates diverse uncertain scenarios (with a
near-maximum diversity value of 0.62) during multiple environmental
simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Applying the FAIR Principles to Computational Workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sean R. Wilkinson, Meznah Aloqalaa, Khalid Belhajjame, Michael R. Crusoe, Bruno de Paula Kinoshita, Luiz Gadelha, Daniel Garijo, Ove Johan Ragnar Gustafsson, Nick Juty, Sehrish Kanwal, Farah Zaib Khan, Johannes Köster, Karsten Peters-von Gehlen, Line Pouchard, Randy K. Rannow, Stian Soiland-Reyes, Nicola Soranzo, Shoaib Sufi, Ziheng Sun, Baiba Vilne, Merridee A. Wouters, Denis Yuen, Carole Goble
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent trends within computational and data sciences show an increasing
recognition and adoption of computational workflows as tools for productivity,
reproducibility, and democratized access to platforms and processing know-how.
As digital objects to be shared, discovered, and reused, computational
workflows benefit from the FAIR principles, which stand for Findable,
Accessible, Interoperable, and Reusable. The Workflows Community Initiative's
FAIR Workflows Working Group (WCI-FW), a global and open community of
researchers and developers working with computational workflows across
disciplines and domains, has systematically addressed the application of both
FAIR data and software principles to computational workflows. We present our
recommendations with commentary that reflects our discussions and justifies our
choices and adaptations. Like the software and data principles on which they
are based, these are offered to workflow users and authors, workflow management
system developers, and providers of workflow services as guide rails for
adoption and fodder for discussion. Workflows are becoming more prevalent as
documented, automated instruments for data analysis, data collection, AI-based
predictions, and simulations. The FAIR recommendations for workflows that we
propose in this paper will maximize their value as research assets and
facilitate their adoption by the wider community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 1 figure, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approaching Code Search for Python as a Translation Retrieval Problem
  with Dual Encoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Monoshiz Mahbub Khan, Zhe Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code search is vital in the maintenance and extension of software systems.
Past works have used separate language models for the natural language and
programming language artifacts on models with multiple encoders and different
loss functions. Similarly, this work approaches code search for Python as a
translation retrieval problem while the natural language queries and the
programming language are treated as two types of languages. By using dual
encoders, these two types of language sequences are projected onto a shared
embedding space, in which the distance reflects the similarity between a given
pair of query and code. However, in contrast to previous work, this approach
uses a unified language model, and a dual encoder structure with a cosine
similarity loss function. A unified language model helps the model take
advantage of the considerable overlap of words between the artifacts, making
the learning much easier. On the other hand, the dual encoders trained with
cosine similarity loss helps the model learn the underlining patterns of which
terms are important for predicting linked pairs of artifacts. Evaluation shows
the proposed model achieves performance better than state-of-the-art code
search models. In addition, this model is much less expensive in terms of time
and complexity, offering a cheaper, faster, and better alternative.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Equivalent Representations of Code By A Self-Reflection
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Li, Ge Li, Lecheng Wang, Hao Zhu, Zhi Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Equivalent Representations (ERs) of code are textual representations that
preserve the same semantics as the code itself, e.g., natural language comments
and pseudocode. ERs play a critical role in software development and
maintenance. However, how to automatically generate ERs of code remains an open
challenge. In this paper, we propose a self-reflection approach to generating
ERs of code. It enables two Large Language Models (LLMs) to work mutually and
produce an ER through a reflection process. Depending on whether constraints on
ERs are applied, our approach generates ERs in both open and constrained
settings. We conduct a empirical study to generate ERs in two settings and
obtain eight findings. (1) Generating ERs in the open setting. In the open
setting, we allow LLMs to represent code without any constraints, analyzing the
resulting ERs and uncovering five key findings. These findings shed light on
how LLMs comprehend syntactic structures, APIs, and numerical computations in
code. (2) Generating ERs in the constrained setting. In the constrained
setting, we impose constraints on ERs, such as natural language comments,
pseudocode, and flowcharts. This allows our approach to address a range of
software engineering tasks. Based on our experiments, we have three findings
demonstrating that our approach can effectively generate ERs that adhere to
specific constraints, thus supporting various software engineering tasks. (3)
Future directions. We also discuss potential future research directions, such
as deriving intermediate languages for code generation, exploring LLM-friendly
requirement descriptions, and further supporting software engineering tasks. We
believe that this paper will spark discussions in research communities and
inspire many follow-up studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Showing LLM-Generated Code Selectively Based on Confidence of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Li, Yuqi Zhu, Yongmin Li, Ge Li, Zhi Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown impressive abilities in code
generation, but they may generate erroneous programs. Reading a program takes
ten times longer than writing it. Showing these erroneous programs to
developers will waste developers' energies and introduce security risks to
software.
  To address the above limitations, we propose HonestCoder, a novel LLM-based
code generation approach. HonestCoder selectively shows the generated programs
to developers based on LLMs' confidence. The confidence provides valuable
insights into the correctness of generated programs. To achieve this goal, we
propose a novel approach to estimate LLMs' confidence in code generation. It
estimates confidence by measuring the multi-modal similarity between
LLMs-generated programs.
  We collect and release a multilingual benchmark named TruthCodeBench, which
consists of 2,265 samples and covers two popular programming languages (i.e.,
Python and Java). We apply HonestCoder to four popular LLMs (e.g.,
DeepSeek-Coder and Code Llama) and evaluate it on TruthCodeBench. Based on the
experiments, we obtain the following insights. (1) HonestCoder can effectively
estimate LLMs' confidence and accurately determine the correctness of generated
programs. For example, HonestCoder outperforms the state-of-the-art baseline by
27.79% in AUROC and 63.74% in AUCPR. (2) HonestCoder can decrease the number of
erroneous programs shown to developers. Compared to eight baselines, it can
show more correct programs and fewer erroneous programs to developers. (3)
Compared to showing code indiscriminately, HonestCoder only adds slight time
overhead (approximately 0.4 seconds per requirement). (4) We discuss future
directions to facilitate the application of LLMs in software development. We
hope this work can motivate broad discussions about measuring the reliability
of LLMs' outputs in performing code-related tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M2AR: A Web-based Modeling Environment for the Augmented Reality
  Workflow Modeling Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Muff, Hans-Georg Fill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces M2AR, a new web-based, two- and three-dimensional
modeling environment that enables the modeling and execution of augmented
reality applications without requiring programming knowledge. The platform is
based on a 3D JavaScript library and the mixed reality immersive web standard
WebXR. For a first demonstration of its feasibility, the previously introduced
Augmented Reality Workflow Modeling Language (ARWFML) has been successfully
implemented using this environment. The usefulness of the new modeling
environment is demonstrated by showing use cases of the ARWFML on M2AR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning test generators for cyber-physical systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jarkko Peltomäki, Ivan Porres
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Black-box runtime verification methods for cyber-physical systems can be used
to discover errors in systems whose inputs and outputs are expressed as signals
over time and their correctness requirements are specified in a temporal logic.
Existing methods, such as requirement falsification, often focus on finding a
single input that is a counterexample to system correctness. In this paper, we
study how to create test generators that can produce multiple and diverse
counterexamples for a single requirement. Several counterexamples expose system
failures in varying input conditions and support the root cause analysis of the
faults.
  We present the WOGAN algorithm to create such test generators automatically.
The algorithm works by training iteratively a Wasserstein generative
adversarial network that models the target distribution of the uniform
distribution on the set of counterexamples. WOGAN is an algorithm that trains
generative models that act as test generators for runtime verification. The
training is performed online without the need for a previous model or dataset.
We also propose criteria to evaluate such test generators.
  We evaluate the trained generators on several well-known problems including
the ARCH-COMP falsification benchmarks. Our experimental results indicate that
generators trained by the WOGAN algorithm are as effective as state-of-the-art
requirement falsification algorithms while producing tests that are as diverse
as a sample from uniform random sampling. We conclude that WOGAN is a viable
method to produce test generators automatically and that these test generators
can generate multiple and diverse counterexamples for the runtime verification
of cyber-physical systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 4 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Potential of Citizen Platforms for Requirements Engineering of Large
  Socio-Technical Software Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jukka Ruohonen, Kalle Hjerppe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Participatory citizen platforms are innovative solutions to digitally better
engage citizens in policy-making and deliberative democracy in general.
Although these platforms have been used also in an engineering context, thus
far, there is no existing work for connecting the platforms to requirements
engineering. The present paper fills this notable gap. In addition to
discussing the platforms in conjunction with requirements engineering, the
paper elaborates potential advantages and disadvantages, thus paving the way
for a future pilot study in a software engineering context. With these
engineering tenets, the paper also contributes to the research of large
socio-technical software systems in a public sector context, including their
implementation and governance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to REFSQ</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Specification Slicing for VDM-SL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomohiro Oda, Han-Myung Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The executable specification is one of the powerful tools in lightweight
formal software development. VDM-SL allows the explicit and executable
definition of operations that reference and update internal state through
imperative statements. While the extensive executable subset of VDM-SL enables
validation and testing in the specification phase, it also brings difficulties
in reading and debugging as in imperative programming. In this paper, we define
specification slicing for VDM-SL based on program slicing, a technique used for
debugging and maintaining program source code in implementation languages. We
then present and discuss its applications. The slicer for VDM-SL is implemented
on ViennaTalk and can be used on browsers and debuggers describing the VDM-SL
specification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, submitted to the 22nd Overture Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Horizon-Length Prediction: Advancing Fill-in-the-Middle Capabilities for
  Code Generation with Lookahead Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifeng Ding, Hantian Ding, Shiqi Wang, Qing Sun, Varun Kumar, Zijian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fill-in-the-Middle (FIM) has become integral to code language models,
enabling generation of missing code given both left and right contexts.
However, the current FIM training paradigm, which reorders original training
sequences and then performs regular next-token prediction (NTP), often leads to
models struggling to generate content that aligns smoothly with the surrounding
context. Crucially, while existing works rely on rule-based post-processing to
circumvent this weakness, such methods are not practically usable in
open-domain code completion tasks as they depend on restrictive,
dataset-specific assumptions (e.g., generating the same number of lines as in
the ground truth). Moreover, model performance on FIM tasks deteriorates
significantly without these unrealistic assumptions.
  We hypothesize that NTP alone is insufficient for models to learn effective
planning conditioned on the distant right context, a critical factor for
successful code infilling. To overcome this, we propose Horizon-Length
Prediction (HLP), a novel training objective that teaches models to predict the
number of remaining middle tokens (i.e., horizon length) at each step. HLP
advances FIM with lookahead planning, enabling models to inherently learn
infilling boundaries for arbitrary left and right contexts without relying on
dataset-specific post-processing. Our evaluation across different models and
sizes shows that HLP significantly improves FIM performance by up to 24%
relatively on diverse benchmarks, across file-level and repository-level, and
without resorting to unrealistic post-processing methods. Furthermore, the
enhanced planning capability gained through HLP boosts model performance on
code reasoning. Importantly, HLP only incurs negligible training overhead and
no additional inference cost, ensuring its practicality for real-world
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning and Machine Learning: Advancing Big Data Analytics and
  Management with Design Patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keyu Chen, Ziqian Bi, Tianyang Wang, Yizhu Wen, Pohsun Feng, Qian Niu, Junyu Liu, Benji Peng, Sen Zhang, Ming Li, Xuanhe Pan, Jiawei Xu, Jinlang Wang, Caitlyn Heqi Yin, Ming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This book, Design Patterns in Machine Learning and Deep Learning: Advancing
Big Data Analytics Management, presents a comprehensive study of essential
design patterns tailored for large-scale machine learning and deep learning
applications. The book explores the application of classical software
engineering patterns, Creational, Structural, Behavioral, and Concurrency
Patterns, to optimize the development, maintenance, and scalability of big data
analytics systems. Through practical examples and detailed Python
implementations, it bridges the gap between traditional object-oriented design
patterns and the unique demands of modern data analytics environments. Key
design patterns such as Singleton, Factory, Observer, and Strategy are analyzed
for their impact on model management, deployment strategies, and team
collaboration, providing invaluable insights into the engineering of efficient,
reusable, and flexible systems. This volume is an essential resource for
developers, researchers, and engineers aiming to enhance their technical
expertise in both machine learning and software design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>138pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interactive GDPR-Compliant Privacy Policy Generation for Software
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pattaraporn Sangaroonsilp, Hoa Khanh Dam, Omar Haggag, John Grundy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software applications are designed to assist users in conducting a wide range
of tasks or interactions. They have become prevalent and play an integral part
in people's lives in this digital era. To use those software applications,
users are sometimes requested to provide their personal information. As privacy
has become a significant concern and many data protection regulations exist
worldwide, software applications must provide users with a privacy policy
detailing how their personal information is collected and processed. We propose
an approach that generates a comprehensive and compliant privacy policy with
respect to the General Data Protection Regulation (GDPR) for diverse software
applications. To support this, we first built a library of privacy clauses
based on existing privacy policy analysis. We then developed an interactive
rule-based system that prompts software developers with a series of questions
and uses their answers to generate a customised privacy policy for a given
software application. We evaluated privacy policies generated by our approach
in terms of readability, completeness and coverage and compared them to privacy
policies generated by three existing privacy policy generators and a Generative
AI-based tool. Our evaluation results show that the privacy policy generated by
our approach is the most complete and comprehensive.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Insight into Security Code <span class="highlight-title">Review</span> with LLMs: Capabilities, Obstacles
  and Influential Factors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16310v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16310v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Yu, Peng Liang, Yujia Fu, Amjed Tahir, Mojtaba Shahin, Chong Wang, Yangxiao Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Security code review is a time-consuming and labor-intensive process
typically requiring integration with automated security defect detection tools.
However, existing security analysis tools struggle with poor generalization,
high false positive rates, and coarse detection granularity. Large Language
Models (LLMs) have been considered promising candidates for addressing those
challenges. In this study, we conducted an empirical study to explore the
potential of LLMs in detecting security defects during code review.
Specifically, we evaluated the performance of six LLMs under five different
prompts and compared them with state-of-theart static analysis tools. We also
performed linguistic and regression analyses for the best-performing LLM to
identify quality problems in its responses and factors influencing its
performance. Our findings show that: (1) existing pre-trained LLMs have limited
capability in security code review but? significantly outperform the
state-of-the-art static analysis tools. (2) GPT-4 performs best among all LLMs
when provided with a CWE list for reference. (3) GPT-4 frequently generates
responses that are verbose or not compliant with the task requirements given in
the prompts. (4) GPT-4 is more adept at identifying security defects in code
files with fewer tokens, containing functional logic, or written by developers
with less involvement in the project.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 5 images, 7 tables, Manuscript submitted to a journal
  (2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenHands: An Open Platform for AI Software Developers as Generalist
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16741v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16741v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, Graham Neubig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software is one of the most powerful tools that we humans have at our
disposal; it allows a skilled programmer to interact with the world in complex
and profound ways. At the same time, thanks to improvements in large language
models (LLMs), there has also been a rapid development in AI agents that
interact with and affect change in their surrounding environments. In this
paper, we introduce OpenHands (f.k.a. OpenDevin), a platform for the
development of powerful and flexible AI agents that interact with the world in
similar ways to those of a human developer: by writing code, interacting with a
command line, and browsing the web. We describe how the platform allows for the
implementation of new agents, safe interaction with sandboxed environments for
code execution, coordination between multiple agents, and incorporation of
evaluation benchmarks. Based on our currently incorporated benchmarks, we
perform an evaluation of agents over 15 challenging tasks, including software
engineering (e.g., SWE-BENCH) and web browsing (e.g., WEBARENA), among others.
Released under the permissive MIT license, OpenHands is a community project
spanning academia and industry with more than 2.1K contributions from over 188
contributors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/All-Hands-AI/OpenHands</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A higher-order transformation approach to the formalization and analysis
  of BPMN using graph transformation systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.05243v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.05243v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Kräuter, Adrian Rutle, Harald König, Yngve Lamo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Business Process Modeling Notation (BPMN) is a widely used standard
notation for defining intra- and inter-organizational workflows. However, the
informal description of the BPMN execution semantics leads to different
interpretations of BPMN elements and difficulties in checking behavioral
properties. In this article, we propose a formalization of the execution
semantics of BPMN that, compared to existing approaches, covers more BPMN
elements while also facilitating property checking. Our approach is based on a
higher-order transformation from BPMN models to graph transformation systems.
To show the capabilities of our approach, we implemented it as an open-source
web-based tool.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RePlay: a Recommendation Framework for Experimentation and Production
  Use 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07272v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07272v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexey Vasilev, Anna Volodkevich, Denis Kulandin, Tatiana Bysheva, Anton Klenitskiy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using a single tool to build and compare recommender systems significantly
reduces the time to market for new models. In addition, the comparison results
when using such tools look more consistent. This is why many different tools
and libraries for researchers in the field of recommendations have recently
appeared. Unfortunately, most of these frameworks are aimed primarily at
researchers and require modification for use in production due to the inability
to work on large datasets or an inappropriate architecture. In this demo, we
present our open-source toolkit RePlay - a framework containing an end-to-end
pipeline for building recommender systems, which is ready for production use.
RePlay also allows you to use a suitable stack for the pipeline on each stage:
Pandas, Polars, or Spark. This allows the library to scale computations and
deploy to a cluster. Thus, RePlay allows data scientists to easily move from
research mode to production mode using the same interfaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Augmentation for Code Translation with Comparable Corpora and
  Multiple References <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqing Xie, Atharva Naik, Daniel Fried, Carolyn Rose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One major challenge of translating code between programming languages is that
parallel training data is often limited. To overcome this challenge, we present
two data augmentation techniques, one that builds comparable corpora (i.e.,
code pairs with similar functionality), and another that augments existing
parallel data with multiple reference translations. Specifically, we build and
analyze multiple types of comparable corpora, including programs generated from
natural language documentation using a code generation model. Furthermore, to
reduce overfitting to a single reference translation, we automatically generate
additional translation references for available parallel data and filter the
translations by unit tests, which increases variation in target translations.
Experiments show that our data augmentation techniques significantly improve
CodeT5 for translation between Java, Python, and C++ by an average of 7.5%
Computational Accuracy (CA@1), which verifies the correctness of translations
by execution. The code is available at https://github.com/Veronicium/CMTrans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 Findings (with minor updates on the flowcharts)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Metamorphic Testing of Image Captioning Systems via Image-Level
  Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11791v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11791v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyuan Xie, Xingpeng Li, Songqiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Image Captioning (IC) technique is widely used to describe images in
natural language. Recently, some IC system testing methods have been proposed.
However, these methods still rely on pre-annotated information and hence cannot
really alleviate the oracle problem in testing. Besides, their method
artificially manipulates objects, which may generate unreal images as test
cases and thus lead to less meaningful testing results. Thirdly, existing
methods have various requirements on the eligibility of source test cases, and
hence cannot fully utilize the given images to perform testing. To tackle these
issues, in this paper, we propose REIC to perform metamorphic testing for IC
systems with some image-level reduction transformations like image cropping and
stretching. Instead of relying on the pre-annotated information, REIC uses a
localization method to align objects in the caption with corresponding objects
in the image, and checks whether each object is correctly described or deleted
in the caption after transformation. With the image-level reduction
transformations, REIC does not artificially manipulate any objects and hence
can avoid generating unreal follow-up images. Besides, it eliminates the
requirement on the eligibility of source test cases in the metamorphic
transformation process, as well as decreases the ambiguity and boosts the
diversity among the follow-up test cases, which consequently enables testing to
be performed on any test image and reveals more distinct valid violations. We
employ REIC to test five popular IC systems. The results demonstrate that REIC
can sufficiently leverage the provided test images to generate follow-up cases
of good reality, and effectively detect a great number of distinct violations,
without the need for any pre-annotated information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Software Engineering (TSE) in
  September 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">32</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Understanding the Experiences of People in Late Adulthood with
  Embedded Information Displays in the Home <span class="chip">IEEE VIS '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zack While, Henry Wheeler-Klainberg, Tanja Blascheck, Petra Isenberg, Ali Sarvghad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedded information displays (EIDs) are becoming increasingly ubiquitous on
home appliances and devices such as microwaves, coffee machines, fridges, or
digital thermostats. These displays are often multi-purpose, functioning as
interfaces for selecting device settings, communicating operating status using
simple visualizations, and displaying notifications. However, their usability
for people in the late adulthood (PLA) development stage is not
well-understood. We report on two focus groups with PLA (n = 11, ages 76-94)
from a local retirement community. Participants were shown images of everyday
home electronics and appliances, answering questions about their experiences
using the EIDs. Using open coding, we qualitatively analyzed their comments to
distill key themes regarding how EIDs can negatively affect PLA's ability to
take in information (e.g., poor labels) and interact with these devices (e.g.,
unintuitive steps) alongside strategies employed to work around these issues.
We argue that understanding the equitable design and communication of devices'
functions, operating status, and messages is important for future information
display designers. We hope this work stimulates further investigation into more
equitable EID design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, accepted to the 1st Workshop on Accessible
  Visualization at IEEE VIS '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demystifying Technology for Policymaking: Exploring the Rideshare
  Context and Data Initiative Opportunities to Advance Tech Policymaking
  Efforts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angie Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the face of rapidly advancing technologies, evidence of harms they can
exacerbate, and insufficient policy to ensure accountability from tech
companies, what are HCI opportunities for advancing policymaking of technology?
In this paper, we explore challenges and opportunities for tech policymaking
through a case study of app-based rideshare driving. We begin with background
on rideshare platforms and how they operate. Next, we review literature on
algorithmic management about how rideshare drivers actually experience platform
features -- often to the detriment of their well-being -- and ways they
respond. In light of this, researchers and advocates have called for increased
worker protections, thus we turn to rideshare policy and regulation efforts in
the U.S. Here, we differentiate the political strategies of platforms with
those of drivers to illustrate the conflicting narratives policymakers face
when trying to oversee gig work platforms. We reflect that past methods
surfacing drivers' experiences may be insufficient for policymaker needs when
developing oversight. To address this gap and our original inquiry -- what are
HCI opportunities for advancing tech policymaking -- we briefly explore two
paths forward for holding tech companies accountable in the rideshare context:
(1) data transparency initiatives to enable collective auditing by workers and
(2) legal frameworks for holding platforms accountable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KidLM: Advancing Language Models for Children -- Early Insights and
  Future Directions <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mir Tafseer Nayeem, Davood Rafiei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies highlight the potential of large language models in creating
educational tools for children, yet significant challenges remain in
maintaining key child-specific properties such as linguistic nuances, cognitive
needs, and safety standards. In this paper, we explore foundational steps
toward the development of child-specific language models, emphasizing the
necessity of high-quality pre-training data. We introduce a novel user-centric
data collection pipeline that involves gathering and validating a corpus
specifically written for and sometimes by children. Additionally, we propose a
new training objective, Stratified Masking, which dynamically adjusts masking
probabilities based on our domain-specific child language data, enabling models
to prioritize vocabulary and concepts more suitable for children. Experimental
evaluations demonstrate that our model excels in understanding lower
grade-level text, maintains safety by avoiding stereotypes, and captures
children's unique preferences. Furthermore, we provide actionable insights for
future research and development in child-specific language modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (long, main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JumpStarter: Getting Started on Personal Goals with AI-Powered Context
  Curation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sitong Wang, Xuanming Zhang, Jenny Ma, Alyssa Hwang, Lydia B. Chilton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Everyone aspires to achieve personal goals. However, getting started is often
complex and daunting, especially for large projects. AI has the potential to
create plans and help jumpstart progress, but it often lacks sufficient
personal context to be useful. We introduce JumpStarter, a system that uses
AI-powered context curation to create action plans and draft personalized
working solutions. JumpStarter assists users by posing questions to elicit
relevant context, breaking down goals into manageable steps, and selecting
appropriate context to draft working solutions for each step. A technical
evaluation indicates that context curation results in plans and working
solutions of higher quality. A user study demonstrates that compared to
ChatGPT, JumpStarter significantly reduces users' mental load and increases
their efficiency in kickstarting personal projects. We discuss the design
implications of AI-powered context curation to facilitate the use of generative
AI in complex problem-solving.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Tool to Facilitate Web-Browsing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03866v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03866v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Kelly, Jonatan Fontanez, Tali Sharot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Search engine results often misalign with users' goals due to opaque
algorithms, leading to unhelpful or detrimental information consumption. To
address this, we developed a Google Chrome plugin that provides "content
labels" for webpages in Google search results, assessing Actionability (guiding
actions), Knowledge (enhancing understanding), and Emotion. Using natural
language processing and machine learning, the plugin predicts these properties
from webpage text based on models trained on participants' ratings, effectively
reflecting user perceptions. The implications include enhanced user control
over information consumption and promotion of healthier engagement with online
content, potentially improving decision-making and well-being.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aligning LLMs with Individual Preferences via Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shujin Wu, May Fung, Cheng Qian, Jeonghwan Kim, Dilek Hakkani-Tur, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) demonstrate increasingly advanced
capabilities, aligning their behaviors with human values and preferences
becomes crucial for their wide adoption. While previous research focuses on
general alignment to principles such as helpfulness, harmlessness, and honesty,
the need to account for individual and diverse preferences has been largely
overlooked, potentially undermining customized human experiences. To address
this gap, we train LLMs that can ''interact to align'', essentially cultivating
the meta-skill of LLMs to implicitly infer the unspoken personalized
preferences of the current user through multi-turn conversations, and then
dynamically align their following behaviors and responses to these inferred
preferences. Our approach involves establishing a diverse pool of 3,310
distinct user personas by initially creating seed examples, which are then
expanded through iterative self-generation and filtering. Guided by distinct
user personas, we leverage multi-LLM collaboration to develop a multi-turn
preference dataset containing 3K+ multi-turn conversations in tree structures.
Finally, we apply supervised fine-tuning and reinforcement learning to enhance
LLMs using this dataset. For evaluation, we establish the ALOE (ALign With
CustOmized PrEferences) benchmark, consisting of 100 carefully selected
examples and well-designed metrics to measure the customized alignment
performance during conversations. Experimental results demonstrate the
effectiveness of our method in enabling dynamic, personalized alignment via
interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code and dataset are made public at
  https://github.com/ShujinWu-0814/ALOE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Cook, Tim Rocktäschel, Jakob Foerster, Dennis Aumiller, Alex Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the widespread adoption and usage of Large Language Models (LLMs), it
is crucial to have flexible and interpretable evaluations of their
instruction-following ability. Preference judgments between model outputs have
become the de facto evaluation standard, despite distilling complex,
multi-faceted preferences into a single ranking. Furthermore, as human
annotation is slow and costly, LLMs are increasingly used to make these
judgments, at the expense of reliability and interpretability. In this work, we
propose TICK (Targeted Instruct-evaluation with ChecKlists), a fully automated,
interpretable evaluation protocol that structures evaluations with
LLM-generated, instruction-specific checklists. We first show that, given an
instruction, LLMs can reliably produce high-quality, tailored evaluation
checklists that decompose the instruction into a series of YES/NO questions.
Each question asks whether a candidate response meets a specific requirement of
the instruction. We demonstrate that using TICK leads to a significant increase
(46.4% $\to$ 52.2%) in the frequency of exact agreements between LLM judgements
and human preferences, as compared to having an LLM directly score an output.
We then show that STICK (Self-TICK) can be used to improve generation quality
across multiple benchmarks via self-refinement and Best-of-N selection. STICK
self-refinement on LiveBench reasoning tasks leads to an absolute gain of
$+$7.8%, whilst Best-of-N selection with STICK attains $+$6.3% absolute
improvement on the real-world instruction dataset, WildBench. In light of this,
structured, multi-faceted self-improvement is shown to be a promising way to
further advance LLM capabilities. Finally, by providing LLM-generated
checklists to human evaluators tasked with directly scoring LLM responses to
WildBench instructions, we notably increase inter-annotator agreement (0.194
$\to$ 0.256).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative AI in the Software Engineering Domain: Tensions of
  Occupational Identity and Patterns of Identity Protection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anuschka Schmitt, Krzysztof Z. Gajos, Osnat Mokryn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The adoption of generative Artificial Intelligence (GAI) in organizational
settings calls into question workers' roles, and relatedly, the implications
for their long-term skill development and domain expertise. In our qualitative
study in the software engineering domain, we build on the theoretical lenses of
occupational identity and self-determination theory to understand how and why
software engineers make sense of GAI for their work. We find that engineers'
sense-making is contingent on domain expertise, as juniors and seniors felt
their needs for competence, autonomy, and relatedness to be differently
impacted by GAI. We shed light on the importance of the individual's role in
preserving tacit domain knowledge as engineers engaged in sense-making that
protected their occupational identity. We illustrate how organizations play an
active role in shaping workers' sense-making process and propose design
guidelines on how organizations and system designers can facilitate the impact
of technological change on workers' occupational identity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artificial Human Lecturers: Initial Findings From Asia's First AI
  Lecturers in Class to Promote Innovation in Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ching Christie Pang, Yawei Zhao, Zhizhuo Yin, Jia Sun, Reza Hadi Mogavi, Pan Hui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, artificial intelligence (AI) has become increasingly
integrated into education, reshaping traditional learning environments. Despite
this, there has been limited investigation into fully operational artificial
human lecturers. To the best of our knowledge, our paper presents the world's
first study examining their deployment in a real-world educational setting.
Specifically, we investigate the use of "digital teachers," AI-powered virtual
lecturers, in a postgraduate course at the Hong Kong University of Science and
Technology (HKUST). Our study explores how features such as appearance,
non-verbal cues, voice, and verbal expression impact students' learning
experiences. Findings suggest that students highly value naturalness,
authenticity, and interactivity in digital teachers, highlighting areas for
improvement, such as increased responsiveness, personalized avatars, and
integration with larger learning platforms. We conclude that digital teachers
have significant potential to enhance education by providing a more flexible,
engaging, personalized, and accessible learning experience for students.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 6 figures (10 sub-figures), 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Toxicity Classifiers and Large Language Models Respond to Ableism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahika Phutane, Ananya Seelam, Aditya Vashistha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  People with disabilities (PwD) regularly encounter ableist hate and
microaggressions online. While online platforms use machine learning models to
moderate online harm, there is little research investigating how these models
interact with ableism. In this paper, we curated a dataset of 100 social media
comments targeted towards PwD, and recruited 160 participants to rate and
explain how toxic and ableist these comments were. We then prompted
state-of-the art toxicity classifiers (TCs) and large language models (LLMs) to
rate and explain the harm. Our analysis revealed that TCs and LLMs rated
toxicity significantly lower than PwD, but LLMs rated ableism generally on par
with PwD. However, ableism explanations by LLMs overlooked emotional harm, and
lacked specificity and acknowledgement of context, important facets of PwD
explanations. Going forward, we discuss challenges in designing
disability-aware toxicity classifiers, and advocate for the shift from ableism
detection to ableism interpretation and explanation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Digital Twin for Human-Centric and Integrated Lighting Asset
  Management in Public Libraries: From Corrective to Predictive Maintenance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Lin, Jingchun Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lighting asset management in public libraries has traditionally been
reactive, focusing on corrective maintenance, addressing issues only when
failures occur. Although standards now encourage preventive measures, such as
incorporating a maintenance factor, the broader goal of human centric,
sustainable lighting systems requires a shift toward predictive maintenance
strategies. This study introduces an enhanced digital twin model designed for
the proactive management of lighting assets in public libraries. By integrating
descriptive, diagnostic, predictive, and prescriptive analytics, the model
enables a comprehensive, multilevel view of asset health. The proposed
framework supports both preventive and predictive maintenance strategies,
allowing for early detection of issues and the timely resolution of potential
failures. In addition to the specific application for lighting systems, the
design is adaptable for other building assets, providing a scalable solution
for integrated asset management in various public spaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> Spatio-Temporal Graph Mask-Passing Attention Network for
  Perceptual Importance Prediction of Multi-point Tactility 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dazhong He, Qian Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While visual and auditory information are prevalent in modern multimedia
systems, haptic interaction, e.g., tactile and kinesthetic interaction,
provides a unique form of human perception. However, multimedia technology for
contact interaction is less mature than non-contact multimedia technologies and
requires further development. Specialized haptic media technologies, requiring
low latency and bitrates, are essential to enable haptic interaction,
necessitating haptic information compression. Existing vibrotactile signal
compression methods, based on the perceptual model, do not consider the
characteristics of fused tactile perception at multiple spatially distributed
interaction points. In fact, differences in tactile perceptual importance are
not limited to conventional frequency and time domains, but also encompass
differences in the spatial locations on the skin unique to tactile perception.
For the most frequently used tactile information, vibrotactile texture
perception, we have developed a model to predict its perceptual importance at
multiple points, based on self-supervised learning and Spatio-Temporal Graph
Neural Network. Current experimental results indicate that this model can
effectively predict the perceptual importance of various points in multi-point
tactile perception scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at Eurohaptics 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Narrative Player: Reviving Data Narratives with Visuals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekai Shao, Leixian Shen, Haotian Li, Yi Shan, Huamin Qu, Yun Wang, Siming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-rich documents are commonly found across various fields such as
business, finance, and science. However, a general limitation of these
documents for reading is their reliance on text to convey data and facts.
Visual representation of text aids in providing a satisfactory reading
experience in comprehension and engagement. However, existing work emphasizes
presenting the insights of local text context, rather than fully conveying data
stories within the whole paragraphs and engaging readers. To provide readers
with satisfactory data stories, this paper presents Narrative Player, a novel
method that automatically revives data narratives with consistent and
contextualized visuals. Specifically, it accepts a paragraph and corresponding
data table as input and leverages LLMs to characterize the clauses and extract
contextualized data facts. Subsequently, the facts are transformed into a
coherent visualization sequence with a carefully designed optimization-based
approach. Animations are also assigned between adjacent visualizations to
enable seamless transitions. Finally, the visualization sequence, transition
animations, and audio narration generated by text-to-speech technologies are
rendered into a data video. The evaluation results showed that the
automatic-generated data videos were well-received by participants and experts
for enhancing reading.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ScriptViz: A Visualization Tool to Aid Scriptwriting based on a Large
  Movie Database 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anyi Rao, Jean-Peïc Chou, Maneesh Agrawala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scriptwriters usually rely on their mental visualization to create a vivid
story by using their imagination to see, feel, and experience the scenes they
are writing. Besides mental visualization, they often refer to existing images
or scenes in movies and analyze the visual elements to create a certain mood or
atmosphere. In this paper, we develop ScriptViz to provide external
visualization based on a large movie database for the screenwriting process. It
retrieves reference visuals on the fly based on scripts' text and dialogue from
a large movie database. The tool provides two types of control on visual
elements that enable writers to 1) see exactly what they want with fixed visual
elements and 2) see variances in uncertain elements. User evaluation among 15
scriptwriters shows that ScriptViz is able to present scriptwriters with
consistent yet diverse visual possibilities, aligning closely with their
scripts and helping their creation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the 37th Annual ACM Symposium on User Interface Software
  and Technology (UIST'24). Webpage:
  https://virtualfilmstudio.github.io/projects/scriptviz</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M2AR: A Web-based Modeling Environment for the Augmented Reality
  Workflow Modeling Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Muff, Hans-Georg Fill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces M2AR, a new web-based, two- and three-dimensional
modeling environment that enables the modeling and execution of augmented
reality applications without requiring programming knowledge. The platform is
based on a 3D JavaScript library and the mixed reality immersive web standard
WebXR. For a first demonstration of its feasibility, the previously introduced
Augmented Reality Workflow Modeling Language (ARWFML) has been successfully
implemented using this environment. The usefulness of the new modeling
environment is demonstrated by showing use cases of the ARWFML on M2AR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StoryNavi: On-Demand Narrative-Driven Reconstruction of Video Play With
  Generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alston Lantian Xu, Tianwei Ma, Tianmeng Liu, Can Liu, Alvaro Cassinelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manually navigating lengthy videos to seek information or answer questions
can be a tedious and time-consuming task for users. We introduce StoryNavi, a
novel system powered by VLLMs for generating customised video play experiences
by retrieving materials from original videos. It directly answers users' query
by constructing non-linear sequence with identified relevant clips to form a
cohesive narrative. StoryNavi offers two modes of playback of the constructed
video plays: 1) video-centric, which plays original audio and skips irrelevant
segments, and 2) narrative-centric, narration guides the experience, and the
original audio is muted. Our technical evaluation showed adequate retrieval
performance compared to human retrieval. Our user evaluation shows that
maintaining narrative coherence significantly enhances user engagement when
viewing disjointed video segments. However, factors like video genre, content,
and the query itself may lead to varying user preferences for the playback
mode.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis and Detection of Differences in Spoken User Behaviors between
  Autonomous and Wizard-of-Oz Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikey Elmers, Koji Inoue, Divesh Lala, Keiko Ochi, Tatsuya Kawahara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examined users' behavioral differences in a large corpus of
Japanese human-robot interactions, comparing interactions between a
tele-operated robot and an autonomous dialogue system. We analyzed user spoken
behaviors in both attentive listening and job interview dialogue scenarios.
Results revealed significant differences in metrics such as speech length,
speaking rate, fillers, backchannels, disfluencies, and laughter between
operator-controlled and autonomous conditions. Furthermore, we developed
predictive models to distinguish between operator and autonomous system
conditions. Our models demonstrated higher accuracy and precision compared to
the baseline model, with several models also achieving a higher F1 score than
the baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and will be presented at the 27th conference of the Oriental
  COCOSDA (O-COCOSDA 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Decision Subjects' Engagement with and Perceived Fairness
  of AI Models When Opportunities of Qualification Improvement Exist 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meric Altug Gemalmaz, Ming Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore how an AI model's decision fairness affects people's engagement
with and perceived fairness of the model if they are subject to its decisions,
but could repeatedly and strategically respond to these decisions. Two types of
strategic responses are considered -- people could determine whether to
continue interacting with the model, and whether to invest in themselves to
improve their chance of future favorable decisions from the model. Via three
human-subject experiments, we found that in decision subjects' strategic,
repeated interactions with an AI model, the model's decision fairness does not
change their willingness to interact with the model or to improve themselves,
even when the model exhibits unfairness on salient protected attributes.
However, decision subjects still perceive the AI model to be less fair when it
systematically biases against their group, especially if the difficulty of
improving one's qualification for the favorable decision is larger for the
lowly-qualified people.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Playwright: Authoring Data Videos with Annotated Narration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leixian Shen, Haotian Li, Yun Wang, Tianqi Luo, Yuyu Luo, Huamin Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating data videos that effectively narrate stories with animated visuals
requires substantial effort and expertise. A promising research trend is
leveraging the easy-to-use natural language (NL) interaction to automatically
synthesize data video components from narrative content like text narrations,
or NL commands that specify user-required designs. Nevertheless, previous
research has overlooked the integration of narrative content and specific
design authoring commands, leading to generated results that lack customization
or fail to seamlessly fit into the narrative context. To address these issues,
we introduce a novel paradigm for creating data videos, which seamlessly
integrates users' authoring and narrative intents in a unified format called
annotated narration, allowing users to incorporate NL commands for design
authoring as inline annotations within the narration text. Informed by a
formative study on users' preference for annotated narration, we develop a
prototype system named Data Playwright that embodies this paradigm for
effective creation of data videos. Within Data Playwright, users can write
annotated narration based on uploaded visualizations. The system's interpreter
automatically understands users' inputs and synthesizes data videos with
narration-animation interplay, powered by large language models. Finally, users
can preview and fine-tune the video. A user study demonstrated that
participants can effectively create data videos with Data Playwright by
effortlessly articulating their desired outcomes through annotated narration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures, accepted by IEEE TVCG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "It Explains What I am Currently Going Through Perfectly to a Tee":
  Understanding User Perceptions on LLM-Enhanced Narrative Interventions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16732v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16732v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ananya Bhattacharjee, Sarah Yi Xu, Pranav Rao, Yuchen Zeng, Jonah Meyerhoff, Syed Ishtiaque Ahmed, David C Mohr, Michael Liut, Alex Mariakakis, Rachel Kornfield, Joseph Jay Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stories about overcoming personal struggles can effectively illustrate the
application of psychological theories in real life, yet they may fail to
resonate with individuals' experiences. In this work, we employ large language
models (LLMs) to create tailored narratives that acknowledge and address unique
challenging thoughts and situations faced by individuals. Our study, involving
346 young adults across two settings, demonstrates that LLM-enhanced stories
were perceived to be better than human-written ones in conveying key takeaways,
promoting reflection, and reducing belief in negative thoughts. These stories
were not only seen as more relatable but also similarly authentic to
human-written ones, highlighting the potential of LLMs in helping young adults
manage their struggles. The findings of this work provide crucial design
considerations for future narrative-based digital mental health interventions,
such as the need to maintain relatability without veering into implausibility
and refining the wording and tone of AI-enhanced content.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dark Mode or Light Mode? Exploring the Impact of Contrast Polarity on
  Visualization Performance Between Age Groups <span class="chip">IEEE VIS '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zack While, Ali Sarvghad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examines the impact of positive and negative contrast polarities
(i.e., light and dark modes) on the performance of younger adults and people in
their late adulthood (PLA). In a crowdsourced study with 134 participants (69
below age 60, 66 aged 60 and above), we assessed their accuracy and time
performing analysis tasks across three common visualization types (Bar, Line,
Scatterplot) and two contrast polarities (positive and negative). We observed
that, across both age groups, the polarity that led to better performance and
the resulting amount of improvement varied on an individual basis, with each
polarity benefiting comparable proportions of participants. However, the
contrast polarity that led to better performance did not always match their
preferred polarity. Additionally, we observed that the choice of contrast
polarity can have an impact on time similar to that of the choice of
visualization type, resulting in an average percent difference of around 36%.
These findings indicate that, overall, the effects of contrast polarity on
visual analysis performance do not noticeably change with age. Furthermore,
they underscore the importance of making visualizations available in both
contrast polarities to better-support a broad audience with differing needs.
Supplementary materials for this work can be found at https://osf.io/539a4/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures, accepted as a short paper to IEEE VIS '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Review</span> on the Role of Virtual Reality in Reducing Mental Health Diseases
  Specifically Stress, Anxiety, and Depression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18918v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18918v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadia Saeed, Khan Bahadar Khan, Muhammad Abul Hassan, Abdul Qayyum, Saba Salahuddin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: Virtual Reality (VR) is a technological interface that allows
users to interact with a simulated environment. VR has been used extensively
for mental health and clinical research. Mental health disorders are globally
burdening health problems in the world. According to the Psychological
Interventions Implementation Manual published by WHO on 6th March 2024, around
one in eight people in the world lived with a mental disorder. This literature
review is synthesized to find out the effects of VR therapy on stress, anxiety
and depression. Method: We used Google Scholar database using keywords of VR,
stress, anxiety and depression. Publication from last ten years (2014 to 1024)
are considered. Researches only in the English language are included. All the
papers and articles with the keyword VR missing were rejected. Result: Google
Scholar yielded 17,700 results from our keywords. Nine studies met our search
criteria that are included in this review. Out of nine, five studies
encountered mental stress and gave effective results in reducing it by VR
therapy. The other four targeted mood disorders, Social anxiety disorders,
depression, loss of happiness and sleep deprivation. They also showed immense
potential in reducing mental illness while using VR. Conclusion: Findings are
in favor of the effectiveness of VR in reducing stress, anxiety and depression.
Still, it is insufficient evidence to consider VR as solely independent
treatment over the traditional medication. In future, the limitations can be
overcome to relying on VR and using it in hospitals as a reliable source of
cure for mental illness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tool-Assisted Learning of Computational Reductions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tristan Kneisel, Elias Radtke, Marko Schmellenkamp, Fabian Vehlken, Thomas Zeume
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational reductions are an important and powerful concept in computer
science. However, they are difficult for many students to grasp. In this paper,
we outline a concept for how the learning of reductions can be supported by
educational support systems. We present an implementation of the concept within
such a system, concrete web-based and interactive learning material for
reductions, and report on our experiences using the material in a large
introductory course on theoretical computer science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fixed a typo compared to last version; 6 pages + references,
  including one page of screenshots; accepted at SIGCSE TS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wrapper Boxes: Faithful Attribution of Model Predictions to Training
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08644v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08644v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiheng Su, Junyi Jessy Li, Matthew Lease
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can we preserve the accuracy of neural models while also providing faithful
explanations of model decisions to training data? We propose a "wrapper box''
pipeline: training a neural model as usual and then using its learned feature
representation in classic, interpretable models to perform prediction. Across
seven language models of varying sizes, including four large language models
(LLMs), two datasets at different scales, three classic models, and four
evaluation metrics, we first show that the predictive performance of wrapper
classic models is largely comparable to the original neural models.
  Because classic models are transparent, each model decision is determined by
a known set of training examples that can be directly shown to users. Our
pipeline thus preserves the predictive performance of neural language models
while faithfully attributing classic model decisions to training data. Among
other use cases, such attribution enables model decisions to be contested based
on responsible training instances. Compared to prior work, our approach
achieves higher coverage and correctness in identifying which training data to
remove to change a model decision. To reproduce findings, our source code is
online at: https://github.com/SamSoup/WrapperBox.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unraveling the Truth: Do VLMs really Understand Charts? A Deep Dive into
  Consistency and Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11229v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11229v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srija Mukhopadhyay, Adnan Qidwai, Aparna Garimella, Pritika Ramu, Vivek Gupta, Dan Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chart question answering (CQA) is a crucial area of Visual Language
Understanding. However, the robustness and consistency of current Visual
Language Models (VLMs) in this field remain under-explored. This paper
evaluates state-of-the-art VLMs on comprehensive datasets, developed
specifically for this study, encompassing diverse question categories and chart
formats. We investigate two key aspects: 1) the models' ability to handle
varying levels of chart and question complexity, and 2) their robustness across
different visual representations of the same underlying data. Our analysis
reveals significant performance variations based on question and chart types,
highlighting both strengths and weaknesses of current models. Additionally, we
identify areas for improvement and propose future research directions to build
more robust and reliable CQA systems. This study sheds light on the limitations
of current models and paves the way for future advancements in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 9 Tables, 5 figures, 22 examples</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Why Would You Suggest That? Human Trust in Language Model Responses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02018v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02018v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manasi Sharma, Ho Chit Siu, Rohan Paleja, Jaime D. Peña
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of Large Language Models (LLMs) has revealed a growing need for
human-AI collaboration, especially in creative decision-making scenarios where
trust and reliance are paramount. Through human studies and model evaluations
on the open-ended News Headline Generation task from the LaMP benchmark, we
analyze how the framing and presence of explanations affect user trust and
model performance. Overall, we provide evidence that adding an explanation in
the model response to justify its reasoning significantly increases
self-reported user trust in the model when the user has the opportunity to
compare various responses. Position and faithfulness of these explanations are
also important factors. However, these gains disappear when users are shown
responses independently, suggesting that humans trust all model responses,
including deceptive ones, equitably when they are shown in isolation. Our
findings urge future research to delve deeper into the nuanced evaluation of
trust in human-machine teaming systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Decoding and Reconstruction via EEG Embeddings with Guided
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07721v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07721v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyang Li, Chen Wei, Shiying Li, Jiachen Zou, Haoyang Qin, Quanying Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How to decode human vision through neural signals has attracted a
long-standing interest in neuroscience and machine learning. Modern contrastive
learning and generative models improved the performance of visual decoding and
reconstruction based on functional Magnetic Resonance Imaging (fMRI). However,
the high cost and low temporal resolution of fMRI limit their applications in
brain-computer interfaces (BCIs), prompting a high need for visual decoding
based on electroencephalography (EEG). In this study, we present an end-to-end
EEG-based visual reconstruction zero-shot framework, consisting of a tailored
brain encoder, called the Adaptive Thinking Mapper (ATM), which projects neural
signals from different sources into the shared subspace as the clip embedding,
and a two-stage multi-pipe EEG-to-image generation strategy. In stage one, EEG
is embedded to align the high-level clip embedding, and then the prior
diffusion model refines EEG embedding into image priors. A blurry image also
decoded from EEG for maintaining the low-level feature. In stage two, we input
both the high-level clip embedding, the blurry image and caption from EEG
latent to a pre-trained diffusion model. Furthermore, we analyzed the impacts
of different time windows and brain regions on decoding and reconstruction. The
versatility of our framework is demonstrated in the magnetoencephalogram (MEG)
data modality. The experimental results indicate that our EEG-based visual
zero-shot framework achieves SOTA performance in classification, retrieval and
reconstruction, highlighting the portability, low cost, and high temporal
resolution of EEG, enabling a wide range of BCI applications. Our code is
available at https://github.com/ncclab-sustech/EEG_Image_decode.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Longitudinal Analysis and Quantitative Assessment of Child Development
  through Mobile Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06919v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06919v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Carlos Ruiz-Garcia, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Julian Fierrez, Javier Ortega-Garcia, Jaime Herreros-Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article provides a comprehensive overview of recent research in the area
of Child-Computer Interaction (CCI). The main contributions of the present
article are two-fold. First, we present a novel longitudinal CCI database named
ChildCIdbLong, which comprises over 600 children aged 18 months to 8 years old,
acquired continuously over 4 academic years (2019-2023). As a result,
ChildCIdbLong comprises over 12K test acquisitions over a tablet device.
Different tests are considered in ChildCIdbLong, requiring different touch and
stylus gestures, enabling the evaluation of praxical and cognitive skills such
as attentional, visuo-spatial, and executive, among others. In addition to the
ChildCIdbLong database, we propose a novel quantitative metric called Test
Quality (Q), designed to measure the motor and cognitive development of
children through their interaction with a tablet device. In order to provide a
better comprehension of the proposed Q metric, popular percentile-based growth
representations are introduced for each test, providing a two-dimensional space
to compare children's development with respect to the typical age skills of the
population. The results achieved in the present article highlight the potential
of the novel ChildCIdbLong database in conjunction with the proposed Q metric
to measure the motor and cognitive development of children as they grow up. The
proposed framework could be very useful as an automatic tool to support child
experts (e.g., paediatricians, educators, or neurologists) for early detection
of potential physical/cognitive impairments during children's development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures, 7 tables, 46 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Renovo: Sensor-Based Visual Assistive Technology for Physiotherapists in
  the Rehabilitation of Stroke Patients with Upper Limb Motor Impairments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.03631v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.03631v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Ridwan Kabir, Mohammad Ishrak Abedin, Mohaimin Ehsan, Mohammad Anas Jawad, Hasan Mahmud, Md. Kamrul Hasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stroke patients with upper limb motor impairments are re-acclimated to their
corresponding motor functionalities through therapeutic interventions.
Physiotherapists typically assess these functionalities using various
qualitative protocols. However, such assessments are often biased and prone to
errors, reducing rehabilitation efficacy. Therefore, real-time visualization
and quantitative analysis of performance metrics, such as range of motion,
repetition rate, velocity, etc., are crucial for accurate progress assessment.
This study introduces Renovo, a working prototype of a wearable motion
sensor-based assistive technology that assists physiotherapists with real-time
visualization of these metrics. We also propose a novel mathematical framework
for generating quantitative performance scores without relying on any machine
learning model. We present the results of a three-week pilot study involving 16
stroke patients with upper limb disabilities, evaluated across three successive
sessions at one-week intervals by both Renovo and physiotherapists (N=5).
Results suggest that while the expertise of a physiotherapist is irreplaceable,
Renovo can assist in the decision-making process by providing valuable
quantitative information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 14 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Repairs in a Block World: A New Benchmark for Handling User Corrections
  with Multi-Modal Language Models <span class="chip">EMNLP'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Chiyah-Garcia, Alessandro Suglia, Arash Eshghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In dialogue, the addressee may initially misunderstand the speaker and
respond erroneously, often prompting the speaker to correct the
misunderstanding in the next turn with a Third Position Repair (TPR). The
ability to process and respond appropriately to such repair sequences is thus
crucial in conversational AI systems. In this paper, we first collect, analyse,
and publicly release BlockWorld-Repairs: a dataset of multi-modal TPR sequences
in an instruction-following manipulation task that is, by design, rife with
referential ambiguity. We employ this dataset to evaluate several
state-of-the-art Vision and Language Models (VLM) across multiple settings,
focusing on their capability to process and accurately respond to TPRs and thus
recover from miscommunication. We find that, compared to humans, all models
significantly underperform in this task. We then show that VLMs can benefit
from specialised losses targeting relevant tokens during fine-tuning, achieving
better performance and generalising better to new scenarios. Our results
suggest that these models are not yet ready to be deployed in multi-modal
collaborative settings where repairs are common, and highlight the need to
design training regimes and objectives that facilitate learning from
interaction. Our code and data are available at
www.github.com/JChiyah/blockworld-repairs
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP'24 Main (Upcoming). Data and code at
  www.github.com/JChiyah/blockworld-repairs - for Bibtex see
  https://raw.githubusercontent.com/JChiyah/blockworld-repairs/refs/heads/main/citation.bib</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Educational Question Generation of Children Storybooks via Question Type
  Distribution Learning and Event-Centric Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.14187v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.14187v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenjie Zhao, Yufang Hou, Dakuo Wang, Mo Yu, Chengzhong Liu, Xiaojuan Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating educational questions of fairytales or storybooks is vital for
improving children's literacy ability. However, it is challenging to generate
questions that capture the interesting aspects of a fairytale story with
educational meaningfulness. In this paper, we propose a novel question
generation method that first learns the question type distribution of an input
story paragraph, and then summarizes salient events which can be used to
generate high-cognitive-demand questions. To train the event-centric
summarizer, we finetune a pre-trained transformer-based sequence-to-sequence
model using silver samples composed by educational question-answer pairs. On a
newly proposed educational question answering dataset FairytaleQA, we show good
performance of our method on both automatic and human evaluation metrics. Our
work indicates the necessity of decomposing question type distribution learning
and event-centric summary generation for educational question generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Readability and Faithfulness of Concept-based Explanations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18533v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18533v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Li, Haoran Jin, Ruixuan Huang, Zhihao Xu, Defu Lian, Zijia Lin, Di Zhang, Xiting Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing popularity of general-purpose Large Language Models (LLMs),
comes a need for more global explanations of model behaviors. Concept-based
explanations arise as a promising avenue for explaining high-level patterns
learned by LLMs. Yet their evaluation poses unique challenges, especially due
to their non-local nature and high dimensional representation in a model's
hidden space. Current methods approach concepts from different perspectives,
lacking a unified formalization. This makes evaluating the core measures of
concepts, namely faithfulness or readability, challenging. To bridge the gap,
we introduce a formal definition of concepts generalizing to diverse
concept-based explanations' settings. Based on this, we quantify the
faithfulness of a concept explanation via perturbation. We ensure adequate
perturbation in the high-dimensional space for different concepts via an
optimization problem. Readability is approximated via an automatic and
deterministic measure, quantifying the coherence of patterns that maximally
activate a concept while aligning with human understanding. Finally, based on
measurement theory, we apply a meta-evaluation method for evaluating these
measures, generalizable to other types of explanations or tasks as well.
Extensive experimental analysis has been conducted to inform the selection of
explanation evaluation measures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024; code:
  https://github.com/hr-jin/Concept-Explanation-Evaluation</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Programming Languages <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Equivalent Representations of Code By A Self-Reflection
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Li, Ge Li, Lecheng Wang, Hao Zhu, Zhi Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Equivalent Representations (ERs) of code are textual representations that
preserve the same semantics as the code itself, e.g., natural language comments
and pseudocode. ERs play a critical role in software development and
maintenance. However, how to automatically generate ERs of code remains an open
challenge. In this paper, we propose a self-reflection approach to generating
ERs of code. It enables two Large Language Models (LLMs) to work mutually and
produce an ER through a reflection process. Depending on whether constraints on
ERs are applied, our approach generates ERs in both open and constrained
settings. We conduct a empirical study to generate ERs in two settings and
obtain eight findings. (1) Generating ERs in the open setting. In the open
setting, we allow LLMs to represent code without any constraints, analyzing the
resulting ERs and uncovering five key findings. These findings shed light on
how LLMs comprehend syntactic structures, APIs, and numerical computations in
code. (2) Generating ERs in the constrained setting. In the constrained
setting, we impose constraints on ERs, such as natural language comments,
pseudocode, and flowcharts. This allows our approach to address a range of
software engineering tasks. Based on our experiments, we have three findings
demonstrating that our approach can effectively generate ERs that adhere to
specific constraints, thus supporting various software engineering tasks. (3)
Future directions. We also discuss potential future research directions, such
as deriving intermediate languages for code generation, exploring LLM-friendly
requirement descriptions, and further supporting software engineering tasks. We
believe that this paper will spark discussions in research communities and
inspire many follow-up studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fair Asynchronous Session Subtyping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2101.08181v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2101.08181v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mario Bravetti, Julien Lange, Gianluigi Zavattaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Session types are widely used as abstractions of asynchronous message passing
systems. Refinement for such abstractions is crucial as it allows improvements
of a given component without compromising its compatibility with the rest of
the system. In the context of session types, the most general notion of
refinement is asynchronous session subtyping, which allows message emissions to
be anticipated w.r.t. a bounded amount of message consumptions. In this paper
we investigate the possibility to anticipate emissions w.r.t. an unbounded
amount of consumptions: to this aim we propose to consider fair compliance over
asynchronous session types and fair refinement as the relation that preserves
it. This allows us to propose a novel variant of session subtyping that
leverages the notion of controllability from service contract theory and that
is a sound characterisation of fair refinement. In addition, we show that both
fair refinement and our novel subtyping are undecidable. We also present a
sound algorithm which deals with examples that feature potentially unbounded
buffering. Finally, we present an implementation of our algorithm and an
empirical evaluation of it on synthetic benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Operating Systems <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CyberCortex.AI: An AI-based Operating System for Autonomous Robotics and
  Complex Automation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01241v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01241v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sorin Grigorescu, Mihai Zaha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The underlying framework for controlling autonomous robots and complex
automation applications are Operating Systems (OS) capable of scheduling
perception-and-control tasks, as well as providing real-time data communication
to other robotic peers and remote cloud computers. In this paper, we introduce
CyberCortex AI, a robotics OS designed to enable heterogeneous AI-based
robotics and complex automation applications. CyberCortex AI is a decentralized
distributed OS which enables robots to talk to each other, as well as to High
Performance Computers (HPC) in the cloud. Sensory and control data from the
robots is streamed towards HPC systems with the purpose of training AI
algorithms, which are afterwards deployed on the robots. Each functionality of
a robot (e.g. sensory data acquisition, path planning, motion control, etc.) is
executed within a so-called DataBlock of Filters shared through the internet,
where each filter is computed either locally on the robot itself, or remotely
on a different robotic system. The data is stored and accessed via a so-called
Temporal Addressable Memory (TAM), which acts as a gateway between each
filter's input and output. CyberCortex AI has two main components: i) the
CyberCortex AI inference system, which is a real-time implementation of the
DataBlock running on the robots' embedded hardware, and ii) the CyberCortex AI
dojo, which runs on an HPC computer in the cloud, and it is used to design,
train and deploy AI algorithms. We present a quantitative and qualitative
performance analysis of the proposed approach using two collaborative robotics
applications: i) a forest fires prevention system based on an Unitree A1 legged
robot and an Anafi Parrot 4K drone, as well as ii) an autonomous driving system
which uses CyberCortex AI for collaborative perception and motion control.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Networking and Internet Architecture <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AraSync: Precision Time Synchronization in Rural Wireless Living Lab 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Nadim, Taimoor Ul Islam, Salil Reddy, Tianyi Zhang, Zhibo Meng, Reshal Afzal, Sarath Babu, Arsalan Ahmad, Daji Qiao, Anish Arora, Hongwei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time synchronization is a critical component in network operation and
management, and it is also required by Ultra-Reliable, Low-Latency
Communications (URLLC) in next-generation wireless systems such as those of 5G,
6G, and Open RAN. In this context, we design and implement AraSync as an
end-to-end time synchronization system in the ARA wireless living lab to enable
advanced wireless experiments and applications involving stringent time
constraints. We make use of Precision Time Protocol (PTP) at different levels
to achieve synchronization accuracy in the order of nanoseconds. Along with
fiber networks, AraSync enables time synchronization across the AraHaul
wireless x-haul network consisting of long-range, high-capacity mmWave and
microwave links. In this paper, we present the detailed design and
implementation of AraSync, including its hardware and software components and
the PTP network topology. Further, we experimentally characterize the
performance of AraSync from spatial and temporal dimensions. Our measurement
and analysis of the clock offset and mean path delay show the impact of the
wireless channel and weather conditions on the PTP synchronization accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 10 figures, accepted in ACM WiNTECH 2024 (The 18th ACM
  Workshop on Wireless Network Testbeds, Experimental evaluation &
  Characterization 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Reinforcement Learning for Delay-Optimized Task Offloading in
  Vehicular Fog Computin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Parsa Toopchinezhad, Mahmood Ahmadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The imminent rise of autonomous vehicles (AVs) is revolutionizing the future
of transport. The Vehicular Fog Computing (VFC) paradigm has emerged to
alleviate the load of compute-intensive and delay-sensitive AV programs via
task offloading to nearby vehicles. Effective VFC requires an intelligent and
dy?namic offloading algorithm. As a result, this paper adapts Deep
Reinforcement Learning (DRL) for VFC offloading. First, a simulation
environment utilizing realistic hardware and task specifications, in addition
to a novel vehicular movement model based on grid-planned cities, is created.
Afterward, a DRL-based algorithm is trained and tested on the environment with
the goal of minimizing global task delay. The DRL model displays impressive
results, outperforming other greedy and conventional methods. The findings
further demonstrate the effectiveness of the DRL model in minimizing queue
congestion, especially when compared to traditional cloud computing methods
that struggle to handle the demands of a large fleet of vehicles. This is
corroborated by queuing theory, highlighting the self-scalability of the
VFC-based DRL approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tarzan: Passively-Learned Real-Time Rate Control for Video Conferencing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neil Agarwal, Rui Pan, Francis Y. Yan, Ravi Netravali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rate control algorithms are at the heart of video conferencing platforms,
determining target bitrates that match dynamic network characteristics for high
quality. Recent data-driven strategies have shown promise for this challenging
task, but the performance degradation they introduce during training has been a
nonstarter for many production services, precluding adoption. This paper aims
to bolster the practicality of data-driven rate control by presenting an
alternative avenue for experiential learning: leveraging purely existing
telemetry logs produced by the incumbent algorithm in production. We observe
that these logs contain effective decisions, although often at the wrong times
or in the wrong order. To realize this approach despite the inherent
uncertainty that log-based learning brings (i.e., lack of feedback for new
decisions), our system, Tarzan, combines a variety of robust learning
techniques (i.e., conservatively reasoning about alternate behavior to minimize
risk and using a richer model formulation to account for environmental noise).
Across diverse networks (emulated and real-world), Tarzan outperforms the
widely deployed GCC algorithm, increasing average video bitrates by 15-39%
while reducing freeze rates by 60-100%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Centralized-Distributed Resource Allocation Based on Deep
  Reinforcement Learning for Cooperative D2D Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yu, Xiaoqing Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Device-to-device (D2D) technology enables direct communication between
adjacent devices within cellular networks. Due to its high data rate, low
latency, and performance improvement in spectrum and energy efficiency, it has
been widely investigated and applied as a critical technology in 5G New Radio
(NR). In addition to conventional overlay and underlay D2D communications,
cooperative D2D communication, which can achieve a win-win situation between
cellular users (CUs) and D2D users (DUs) through cooperative relaying
technique, has attracted extensive attention from academic and industrial
circles in the past decade. This paper delves into optimizing joint spectrum
allocation, power control, and link-matching between multiple CUs and DUs for
cooperative D2D communications, using weighted sum energy efficiency (WSEE) as
the performance metric to address the challenges of green communication and
sustainable development. This integer programming problem can be decomposed
into a classic weighted bipartite graph matching and a series of nonconvex
spectrum allocation and power control problems between potentially matched
cellular and D2D link pairs. To address this issue, we propose a hybrid
centralized-distributed scheme based on deep reinforcement learning (DRL) and
the Kuhn-Munkres (KM) algorithm. Leveraging the latter, the CUs and DUs
autonomously optimize spectrum allocation and power control by only utilizing
local information. Then, the base station (BS) determines the link matching.
Simulation results reveal that it achieves near-optimal performance and
significantly enhances the network convergence speed with low signaling
overheads. In addition, we also propose and utilize cooperative link sets for
corresponding D2D links to accelerate the proposed scheme and reduce signaling
exchange further.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages,9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reparo: Loss-Resilient Generative Codec for Video Conferencing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14135v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14135v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhong Li, Vibhaalakshmi Sivaraman, Pantea Karimi, Lijie Fan, Mohammad Alizadeh, Dina Katabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Packet loss during video conferencing often results in poor quality and video
freezing. Retransmitting lost packets is often impractical due to the need for
real-time playback, and using Forward Error Correction (FEC) for packet
recovery is challenging due to the unpredictable and bursty nature of Internet
losses. Excessive redundancy leads to inefficiency and wasted bandwidth, while
insufficient redundancy results in undecodable frames, causing video freezes
and quality degradation in subsequent frames.
  We introduce Reparo -- a loss-resilient video conferencing framework based on
generative deep learning models to address these issues. Our approach generates
missing information when a frame or part of a frame is lost. This generation is
conditioned on the data received thus far, considering the model's
understanding of how people and objects appear and interact within the visual
realm. Experimental results, using publicly available video conferencing
datasets, demonstrate that Reparo outperforms state-of-the-art FEC-based video
conferencing solutions in terms of both video quality (measured through PSNR,
SSIM, and LPIPS) and the occurrence of video freezes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Challenges in Network Intrusion Detection Systems: Research
  Insights and Future Prospects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18736v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18736v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabrine Ennaji, Fabio De Gaspari, Dorjan Hitaj, Alicia K Bidi, Luigi V. Mancini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning has brought significant advances in cybersecurity,
particularly in the development of Intrusion Detection Systems (IDS). These
improvements are mainly attributed to the ability of machine learning
algorithms to identify complex relationships between features and effectively
generalize to unseen data. Deep neural networks, in particular, contributed to
this progress by enabling the analysis of large amounts of training data,
significantly enhancing detection performance. However, machine learning models
remain vulnerable to adversarial attacks, where carefully crafted input data
can mislead the model into making incorrect predictions. While adversarial
threats in unstructured data, such as images and text, have been extensively
studied, their impact on structured data like network traffic is less explored.
This survey aims to address this gap by providing a comprehensive review of
machine learning-based Network Intrusion Detection Systems (NIDS) and
thoroughly analyzing their susceptibility to adversarial attacks. We critically
examine existing research in NIDS, highlighting key trends, strengths, and
limitations, while identifying areas that require further exploration.
Additionally, we discuss emerging challenges in the field and offer insights
for the development of more robust and resilient NIDS. In summary, this paper
enhances the understanding of adversarial attacks and defenses in NIDS and
guide future research in improving the robustness of machine learning models in
cybersecurity applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedStein: Enhancing Multi-Domain Federated Learning Through James-Stein
  Estimator <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunny Gupta, Nikita Jangid, Amit Sethi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) facilitates data privacy by enabling collaborative
in-situ training across decentralized clients. Despite its inherent advantages,
FL faces significant challenges of performance and convergence when dealing
with data that is not independently and identically distributed (non-i.i.d.).
While previous research has primarily addressed the issue of skewed label
distribution across clients, this study focuses on the less explored challenge
of multi-domain FL, where client data originates from distinct domains with
varying feature distributions. We introduce a novel method designed to address
these challenges FedStein: Enhancing Multi-Domain Federated Learning Through
the James-Stein Estimator. FedStein uniquely shares only the James-Stein (JS)
estimates of batch normalization (BN) statistics across clients, while
maintaining local BN parameters. The non-BN layer parameters are exchanged via
standard FL techniques. Extensive experiments conducted across three datasets
and multiple models demonstrate that FedStein surpasses existing methods such
as FedAvg and FedBN, with accuracy improvements exceeding 14% in certain
domains leading to enhanced domain generalization. The code is available at
https://github.com/sunnyinAI/FedStein
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures. Accepted at International Workshop on Federated
  Foundation Models In Conjunction with NeurIPS 2024 (FL@FM-NeurIPS'24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Influence-oriented Personalized Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Tan, Guodong Long, Jing Jiang, Chengqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional federated learning (FL) methods often rely on fixed weighting for
parameter aggregation, neglecting the mutual influence by others. Hence, their
effectiveness in heterogeneous data contexts is limited. To address this
problem, we propose an influence-oriented federated learning framework, namely
FedC^2I, which quantitatively measures Client-level and Class-level Influence
to realize adaptive parameter aggregation for each client. Our core idea is to
explicitly model the inter-client influence within an FL system via the
well-crafted influence vector and influence matrix. The influence vector
quantifies client-level influence, enables clients to selectively acquire
knowledge from others, and guides the aggregation of feature representation
layers. Meanwhile, the influence matrix captures class-level influence in a
more fine-grained manner to achieve personalized classifier aggregation. We
evaluate the performance of FedC^2I against existing federated learning methods
under non-IID settings and the results demonstrate the superiority of our
method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memory-distributed level set-based inverse homogenisation of
  three-dimensional piezoelectric materials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary J. Wegert, Anthony P. Roberts, Vivien J. Challis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we use level set-based topology optimisation to design
three-dimensional periodic piezoelectric materials with enhanced properties.
Our methodology is fully memory-distributed and written in Julia using the
package GridapTopOpt. We compare and assess several existing iterative solvers
with respect to their weak scalability and find that an approximate Schur
complement preconditioned GMRES method demonstrates the best performance and
scalability for solving the piezoelectric homogenisation equations. We use the
developed techniques to computationally design high-resolution piezoelectric
metamaterials with enhanced stiffness and piezoelectric properties that yield
new insights into material design for sensor, hydrophone, and actuator
applications. We suggest two robust structures with simple geometric features
that exhibit enhanced piezoelectric properties several times larger than those
of the base material. We find that level set-based topology optimisation is
well suited to problems involving piezoelectricity and has the advantage of
avoiding large regions of intermediate density material.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedCert: Federated Accuracy Certification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Hieu Nguyen, Huu Tien Nguyen, Trung Thanh Nguyen, Manh Duong Nguyen, Trong Nghia Hoang, Truong Thao Nguyen, Phi Le Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has emerged as a powerful paradigm for training
machine learning models in a decentralized manner, preserving data privacy by
keeping local data on clients. However, evaluating the robustness of these
models against data perturbations on clients remains a significant challenge.
Previous studies have assessed the effectiveness of models in centralized
training based on certified accuracy, which guarantees that a certain
percentage of the model's predictions will remain correct even if the input
data is perturbed. However, the challenge of extending these evaluations to FL
remains unresolved due to the unknown client's local data. To tackle this
challenge, this study proposed a method named FedCert to take the first step
toward evaluating the robustness of FL systems. The proposed method is designed
to approximate the certified accuracy of a global model based on the certified
accuracy and class distribution of each client. Additionally, considering the
Non-Independent and Identically Distributed (Non-IID) nature of data in
real-world scenarios, we introduce the client grouping algorithm to ensure
reliable certified accuracy during the aggregation step of the approximation
algorithm. Through theoretical analysis, we demonstrate the effectiveness of
FedCert in assessing the robustness and reliability of FL systems. Moreover,
experimental results on the CIFAR-10 and CIFAR-100 datasets under various
scenarios show that FedCert consistently reduces the estimation error compared
to baseline methods. This study offers a solution for evaluating the robustness
of FL systems and lays the groundwork for future research to enhance the
dependability of decentralized learning. The source code is available at
https://github.com/thanhhff/FedCert/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 22nd International Symposium on Network Computing and
  Applications (NCA 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generic Multicast (Extended Version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        José Augusto Bolina, Pierre Sutra, Douglas Antunes Rocha, Lasaro Camargos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Communication primitives play a central role in modern computing. They offer
a panel of reliability and ordering guarantees for messages, enabling the
implementation of complex distributed interactions. In particular, atomic
broadcast is a pivotal abstraction for implementing fault-tolerant distributed
services. This primitive allows disseminating messages across the system in a
total order. There are two group communication primitives closely related to
atomic broadcast. Atomic multicast permits targeting a subset of participants,
possibly stricter than the whole system. Generic broadcast leverages the
semantics of messages to order them only where necessary (that is when they
conflict). In this paper, we propose to combine all these primitives into a
single, more general one, called generic multicast. We formally specify the
guarantees offered by generic multicast and present efficient algorithms.
Compared to prior works, our solutions offer appealing properties in terms of
time and space complexity. In particular, when a run is conflict-free, that is
no two messages conflict, a message is delivered after at most three message
delays.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TOB-SVD: Total-Order Broadcast with Single-Vote Decisions in the Sleepy
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco D'Amato, Roberto Saltini, Thanh-Hai Tran, Luca Zanolini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past years, distributed consensus research has extended its focus
towards addressing challenges in large-scale, permissionless systems, such as
blockchains. This shift is characterized by the need to accommodate dynamic
participation, contrasting the traditional approach of a static set of
continuously online participants. Works like Bitcoin and the sleepy model have
set the stage for this evolving framework.
  Notable contributions from Momose and Ren (CCS 2022) and subsequent works
have introduced Total-Order Broadcast protocols leveraging Graded Agreement
primitives and supporting dynamic participation. However, these approaches
often require multiple phases of voting per decision, creating a potential
bottleneck for real-world large-scale systems.
  Addressing this, our paper introduces TOB-SVD, a novel Total-Order Broadcast
protocol in the sleepy model, which is resilient to up to 1/2 of adversarial
participants. TOB-SVD requires only a single phase of voting per decision in
the best case and achieves lower expected latency compared to existing
approaches offering the same optimal adversarial resilience. This work paves
the way to more practical Total-Order Broadcast protocols to be implemented in
real-world systems where a large number of participants are involved
simultaneously and their participation level might fluctuate over time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Local certification of forbidden subgraphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12148v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12148v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Bousquet, Linda Cook, Laurent Feuilloley, Théo Pierron, Sébastien Zeitoun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting specific structures in a network has been a very active theme of
research in distributed computing for at least a decade. In this paper, we
start the study of subgraph detection from the perspective of local
certification. Remember that a local certification is a distributed mechanism
enabling the nodes of a network to check the correctness of the current
configuration, thanks to small pieces of information called certificates. Our
main question is: For a given graph $H$, what is the minimum certificate size
that allows checking that the network does not contain $H$ as a (possibly
induced) subgraph?
  We show a variety of lower and upper bounds, uncovering an interesting
interplay between the optimal certificate size, the size of the forbidden
subgraph, and the locality of the verification. Along the way we introduce
several new technical tools, in particular what we call the \emph{layered map},
which is not specific to forbidden subgraphs and that we expect to be useful
for certifying many other properties.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scattered Mixture-of-Experts Implementation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08245v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08245v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shawn Tan, Yikang Shen, Rameswar Panda, Aaron Courville
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ScatterMoE, an implementation of Sparse Mixture-of-Experts (SMoE)
on GPUs. ScatterMoE builds upon existing implementations, and overcoming some
of the limitations to improve inference and training speed, and memory
footprint. This implementation achieves this by avoiding padding and making
excessive copies of the input. We introduce ParallelLinear, the main component
we use to build our implementation and the various kernels used to speed up the
operation. We benchmark our implementation against Megablocks, and show that it
enables a higher throughput and lower memory footprint. We also show how
ParallelLinear enables extension of the Mixture-of-Experts concept by
demonstrating with an implementation of Mixture of Attention.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-03T00:00:00Z">2024-10-03</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Software Engineering <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does the Order of Fine-tuning Matter and Why? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihong Chen, Jiawei Li, Hyunjae Suh, Lianghao Jiang, Zheng Zhou, Jingze Chen, Jiri Gesi, Iftekhar Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To improve the performance on a target task, researchers have fine-tuned
language models with an intermediate task before the target task of interest.
However, previous works have focused on the pre-trained language models and
downstream tasks in Natural Language Processing (NLP) and considered only one
intermediate task. The effect of fine-tuning multiple intermediate tasks and
their ordering on target task performance has not been fully explored in
Software Engineering. In this study, we perform the first empirical study on
analyzing the impact of task ordering on target task performance. Experimental
results show that there is an impact of task ordering on target task
performance by up to 6% of performance gain and up to 4% of performance loss.
To explain such an impact, we consider a variety of potential factors,
including the characteristics of dataset (syntactic similarity and semantic
similarity analysis, dataset size), model (probing task and attention
analysis), and task (task affinity analysis). Our study provides Software
Engineering researchers and practitioners with insights into the effect of task
orderings and how to select the one that is cost-effective while achieving the
best performance gain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain-Specific Retrieval-Augmented Generation Using Vector Stores,
  Knowledge Graphs, and Tensor Factorization <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan C. Barron, Ves Grantcharov, Selma Wanna, Maksim E. Eren, Manish Bhattarai, Nicholas Solovyev, George Tompkins, Charles Nicholas, Kim Ø. Rasmussen, Cynthia Matuszek, Boian S. Alexandrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are pre-trained on large-scale corpora and excel
in numerous general natural language processing (NLP) tasks, such as question
answering (QA). Despite their advanced language capabilities, when it comes to
domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations,
knowledge cut-offs, and lack of knowledge attributions. Additionally, fine
tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and
time consuming process. The retrieval-augmented generation (RAG) process has
recently emerged as a method capable of optimization of LLM responses, by
referencing them to a predetermined ontology. It was shown that using a
Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into
account relevant sub-graphs that preserve the information in a structured
manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM
framework, that integrates RAG with KG and a vector store (VS) that store
factual domain specific information. Importantly, to avoid hallucinations in
the KG, we build these highly domain-specific KGs and VSs without the use of
LLMs, but via NLP, data mining, and nonnegative tensor factorization with
automatic model selection. Pairing our RAG with a domain-specific: (i) KG
(containing structured information), and (ii) VS (containing unstructured
information) enables the development of domain-specific chat-bots that
attribute the source of information, mitigate hallucinations, lessen the need
for fine-tuning, and excel in highly domain-specific question answering tasks.
We pair SMART-SLIC with chain-of-thought prompting agents. The framework is
designed to be generalizable to adapt to any specific or specialized domain. In
this paper, we demonstrate the question answering capabilities of our framework
on a corpus of scientific publications on malware analysis and anomaly
detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages 7 figures, 1 table, 1 cypher code Accepted to ICMLA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preparing for Super-Reactivity: Early Fault-Detection in the Development
  of Exceedingly Complex Reactive Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Harel, Assaf Marron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the term Super-Reactive Systems to refer to reactive systems
whose construction and behavior are complex, constantly changing and evolving,
and heavily interwoven with other systems and the physical world. Finding
hidden faults in such systems early in planning and development is critical for
human safety, the environment, society and the economy. However, the complexity
of the system and its interactions and the absence of adequate technical
details pose a great obstacle. We propose an architecture for models and tools
to overcome such barriers and enable simulation, systematic analysis, and fault
detection and handling, early in the development of super-reactive systems. The
approach is facilitated by the inference and abstraction capabilities and the
power and knowledge afforded by large language models and associated AI tools.
It is based on: (i) deferred, just-in-time interpretation of model elements
that are stored in natural language form, and (ii) early capture of tacit
interdependencies among seemingly orthogonal requirements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ It is Giving Major Satisfaction: Why Fairness Matters for Developers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emeralda Sesari, Federica Sarro, Ayushi Rastogi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software practitioners often face unfairness in their work, such as unequal
recognition of contributions, gender bias, and unclear criteria for performance
reviews. While the link between fairness and job satisfaction has been
established in other fields, its relevance to software professionals remains
underexplored. This study aims to examine how fairness perceptions relate to
job satisfaction among software practitioners, focusing on both general trends
and demographic-specific differences. We conducted an online survey of 108
software practitioners, followed by ordinal logistic regression to analyze the
relationship between fairness perceptions and job satisfaction in software
engineering contexts, with moderation analysis examining how this relationship
varies across demographic groups.
  Our findings indicate that all four fairness dimensions, distributive,
procedural, interpersonal, and informational, significantly affect both overall
job satisfaction and satisfaction with job security. Among these, interpersonal
fairness has the biggest impact, being more than twice as influential on
overall job satisfaction. The relationship between fairness perceptions and job
satisfaction is notably stronger for female, ethnically underrepresented, less
experienced practitioners, and those with work limitations. Fairness in
authorship emerged as an important factor for job satisfaction collectively,
while fairness in policy implementation, high-demand situations, and working
hours particularly impacted specific demographic groups. This study highlights
the unique role of fairness in software engineering, offering strategies for
organizations to promote fair practices and targeted approaches specific for
certain demographic groups.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demonstration Attack against In-Context Learning for Code Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Ge, Weisong Sun, Yihang Lou, Chunrong Fang, Yiran Zhang, Yiming Li, Xiaofang Zhang, Yang Liu, Zhihong Zhao, Zhenyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have revolutionized code
intelligence by improving programming productivity and alleviating challenges
faced by software developers. To further improve the performance of LLMs on
specific code intelligence tasks and reduce training costs, researchers reveal
a new capability of LLMs: in-context learning (ICL). ICL allows LLMs to learn
from a few demonstrations within a specific context, achieving impressive
results without parameter updating. However, the rise of ICL introduces new
security vulnerabilities in the code intelligence field. In this paper, we
explore a novel security scenario based on the ICL paradigm, where attackers
act as third-party ICL agencies and provide users with bad ICL content to
mislead LLMs outputs in code intelligence tasks. Our study demonstrates the
feasibility and risks of such a scenario, revealing how attackers can leverage
malicious demonstrations to construct bad ICL content and induce LLMs to
produce incorrect outputs, posing significant threats to system security. We
propose a novel method to construct bad ICL content called DICE, which is
composed of two stages: Demonstration Selection and Bad ICL Construction,
constructing targeted bad ICL content based on the user query and transferable
across different query inputs. Ultimately, our findings emphasize the critical
importance of securing ICL mechanisms to protect code intelligence systems from
adversarial manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CodeJudge: Evaluating Code Generation with Large Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixi Tong, Tianyi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown promising performance in code
generation. However, how to reliably evaluate code generated by LLMs remains an
unresolved problem. This paper presents CodeJudge, a code evaluation framework
that leverages LLMs to evaluate the semantic correctness of generated code
without the need for test cases. We investigate different ways to guide the LLM
in performing "slow thinking" to arrive at an in-depth and reliable evaluation.
We experimented with four LLMs as evaluators on four code generation datasets
and five programming languages. The results show that CodeJudge significantly
outperformed existing methods in most settings. Furthermore, compared with a
SOTA GPT-3.5-based code evaluation method, CodeJudge achieved better results
even when using a much smaller model, Llama-3-8B-Instruct. Our code and
datasets are available on GitHub https://github.com/VichyTong/CodeJudge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (Main, Long Paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated Bug Generation in the era of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02407v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02407v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Reza Ibrahimzada, Yang Chen, Ryan Rong, Reyhaneh Jabbarvand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bugs are essential in software engineering; many research studies in the past
decades have been proposed to detect, localize, and repair bugs in software
systems. Effectiveness evaluation of such techniques requires complex bugs,
i.e., those that are hard to detect through testing and hard to repair through
debugging. From the classic software engineering point of view, a
hard-to-repair bug differs from the correct code in multiple locations, making
it hard to localize and repair. Hard-to-detect bugs, on the other hand,
manifest themselves under specific test inputs and reachability conditions.
These two objectives, i.e., generating hard-to-detect and hard-to-repair bugs,
are mostly aligned; a bug generation technique can change multiple statements
to be covered only under a specific set of inputs. However, these two
objectives are conflicting for learning-based techniques: A bug should have a
similar code representation to the correct code in the training data to
challenge a bug prediction model to distinguish them. The hard-to-repair bug
definition remains the same but with a caveat: the more a bug differs from the
original code, the more distant their representations are and easier to be
detected. We propose BugFarm, to transform arbitrary code into multiple complex
bugs. BugFarm leverages LLMs to mutate code in multiple locations
(hard-to-repair). To ensure that multiple modifications do not notably change
the code representation, BugFarm analyzes the attention of the underlying model
and instructs LLMs to only change the least attended locations
(hard-to-detect). Our comprehensive evaluation of 435k+ bugs from over 1.9M
mutants generated by BUGFARM and two alternative approaches demonstrates our
superiority in generating bugs that are hard to detect by learning-based bug
prediction approaches and hard-to-repair by state-of-the-art learning-based
program repair technique.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAG-Enhanced Commit Message Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05514v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05514v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linghao Zhang, Hongyi Zhang, Chong Wang, Peng Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Commit message is one of the most important textual information in software
development and maintenance. However, it is time-consuming to write commit
messages manually. Commit Message Generation (CMG) has become a research
hotspot. Recently, several pre-trained language models (PLMs) and large
language models (LLMs) with code capabilities have been introduced,
demonstrating impressive performance on code-related tasks. Meanwhile, prior
studies have explored the utilization of retrieval techniques for CMG, but it
is still unclear what effects would emerge from combining advanced retrieval
techniques with various generation models. This paper proposed REACT, a
REtrieval-Augmented framework for CommiT message generation. It integrates
advanced retrieval techniques with different PLMs and LLMs, to enhance the
performance of these models on the CMG task. Specifically, a hybrid retriever
is designed and used to retrieve the most relevant code diff and commit message
pair as an exemplar. Then, the retrieved pair is utilized to guide and enhance
the CMG task by PLMs and LLMs through fine-tuning and in-context learning. The
experimental results show that REACT significantly enhances these models'
performance on the CMG task, improving the BLEU score of CodeT5 by up to 55%,
boosting Llama 3's BLEU score by 102%, and substantially surpassing all
baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 5 images, 6 tables, Manuscript submitted to a journal
  (2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Automated Code Vulnerability Repair using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03741v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03741v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David de-Fitero-Dominguez, Eva Garcia-Lopez, Antonio Garcia-Cabot, Jose-Javier Martinez-Herraiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research addresses the complex challenge of automated repair of code
vulnerabilities, vital for enhancing digital security in an increasingly
technology-driven world. The study introduces a novel and efficient format for
the representation of code modification, using advanced Large Language Models
(LLMs) such as Code Llama and Mistral. These models, fine-tuned on datasets
featuring C code vulnerabilities, significantly improve the accuracy and
adaptability of automated code repair techniques. A key finding is the enhanced
repair accuracy of these models when compared to previous methods such as
VulRepair, which underscores their practical utility and efficiency. The
research also offers a critical assessment of current evaluation metrics, such
as perfect predictions, and their limitations in reflecting the true
capabilities of automated repair models in real-world scenarios. Following
this, it underscores the importance of using test datasets devoid of train
samples, emphasizing the need for dataset integrity to enhance the
effectiveness of LLMs in code repair tasks. The significance of this work is
its contribution to digital security, setting new standards for automated code
vulnerability repair and paving the way for future advancements in the fields
of cybersecurity and artificial intelligence. The study does not only highlight
the potential of LLMs in enhancing code security but also fosters further
exploration and research in these crucial areas.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RGD: Multi-LLM Based Agent Debugger via Refinement and Generation
  Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01242v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01242v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolin Jin, Zechao Sun, Huaming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown incredible potential in code
generation tasks, and recent research in prompt engineering have enhanced LLMs'
understanding of textual information. However, ensuring the accuracy of
generated code often requires extensive testing and validation by programmers.
While LLMs can typically generate code based on task descriptions, their
accuracy remains limited, especially for complex tasks that require a deeper
understanding of both the problem statement and the code generation process.
This limitation is primarily due to the LLMs' need to simultaneously comprehend
text and generate syntactically and semantically correct code, without having
the capability to automatically refine the code. In real-world software
development, programmers rarely produce flawless code in a single attempt based
on the task description alone, they rely on iterative feedback and debugging to
refine their programs. Inspired by this process, we introduce a novel
architecture of LLM-based agents for code generation and automatic debugging:
Refinement and Guidance Debugging (RGD). The RGD framework is a multi-LLM-based
agent debugger that leverages three distinct LLM agents-Guide Agent, Debug
Agent, and Feedback Agent. RGD decomposes the code generation task into
multiple steps, ensuring a clearer workflow and enabling iterative code
refinement based on self-reflection and feedback. Experimental results
demonstrate that RGD exhibits remarkable code generation capabilities,
achieving state-of-the-art performance with a 9.8% improvement on the HumanEval
dataset and a 16.2% improvement on the MBPP dataset compared to the
state-of-the-art approaches and traditional direct prompting approaches. We
highlight the effectiveness of the RGD framework in enhancing LLMs' ability to
generate and refine code autonomously.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Constructed Context Decompilation with Fined-grained Alignment
  Enhancement <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunlong Feng, Dechuan Teng, Yang Xu, Honglin Mu, Xiao Xu, Libo Qin, Qingfu Zhu, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decompilation transforms compiled code back into a high-level programming
language for analysis when source code is unavailable. Previous work has
primarily focused on enhancing decompilation performance by increasing the
scale of model parameters or training data for pre-training. Based on the
characteristics of the decompilation task, we propose two methods: (1) Without
fine-tuning, the Self-Constructed Context Decompilation (sc$^2$dec) method
recompiles the LLM's decompilation results to construct pairs for in-context
learning, helping the model improve decompilation performance. (2) Fine-grained
Alignment Enhancement (FAE), which meticulously aligns assembly code with
source code at the statement level by leveraging debugging information, is
employed during the fine-tuning phase to achieve further improvements in
decompilation. By integrating these two methods, we achieved a Re-Executability
performance improvement of approximately 3.90% on the Decompile-Eval benchmark,
establishing a new state-of-the-art performance of 52.41%. The code, data, and
models are available at https://github.com/AlongWY/sccdec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing <span class="highlight-title">Pre-Train</span>ed Language Models for Vulnerability Detection via
  Semantic-Preserving Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00249v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00249v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiliang Qi, Jiahao Cao, Darsh Poddar, Sophia Li, Xinda Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development and widespread use of advanced network systems,
software vulnerabilities pose a significant threat to secure communications and
networking. Learning-based vulnerability detection systems, particularly those
leveraging pre-trained language models, have demonstrated significant potential
in promptly identifying vulnerabilities in communication networks and reducing
the risk of exploitation. However, the shortage of accurately labeled
vulnerability datasets hinders further progress in this field. Failing to
represent real-world vulnerability data variety and preserve vulnerability
semantics, existing augmentation approaches provide limited or even
counterproductive contributions to model training. In this paper, we propose a
data augmentation technique aimed at enhancing the performance of pre-trained
language models for vulnerability detection. Given the vulnerability dataset,
our method performs natural semantic-preserving program transformation to
generate a large volume of new samples with enriched data diversity and
variety. By incorporating our augmented dataset in fine-tuning a series of
representative code pre-trained models (i.e., CodeBERT, GraphCodeBERT,
UnixCoder, and PDBERT), up to 10.1% increase in accuracy and 23.6% increase in
F1 can be achieved in the vulnerability detection task. Comparison results also
show that our proposed method can substantially outperform other prominent
vulnerability augmentation approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EAI International Conference on Security and Privacy in
  Communication Networks (SecureComm 2024)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">20</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CounterQuill: Investigating the Potential of Human-AI Collaboration in
  Online Counterspeech Writing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Ding, Kaike Ping, Uma Sushmitha Gunturi, Buse Carik, Sophia Stil, Lance T Wilhelm, Taufiq Daryanto, James Hawdon, Sang Won Lee, Eugenia H Rho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online hate speech has become increasingly prevalent on social media
platforms, causing harm to individuals and society. While efforts have been
made to combat this issue through content moderation, the potential of
user-driven counterspeech as an alternative solution remains underexplored.
Existing counterspeech methods often face challenges such as fear of
retaliation and skill-related barriers. To address these challenges, we
introduce CounterQuill, an AI-mediated system that assists users in composing
effective and empathetic counterspeech. CounterQuill provides a three-step
process: (1) a learning session to help users understand hate speech and
counterspeech; (2) a brainstorming session that guides users in identifying key
elements of hate speech and exploring counterspeech strategies; and (3) a
co-writing session that enables users to draft and refine their counterspeech
with CounterQuill. We conducted a within-subjects user study with 20
participants to evaluate CounterQuill in comparison to ChatGPT. Results show
that CounterQuill's guidance and collaborative writing process provided users a
stronger sense of ownership over their co-authored counterspeech. Users
perceived CounterQuill as a writing partner and thus were more willing to post
the co-written counterspeech online compared to the one written with ChatGPT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncovering the New Accessibility Crisis in Scholarly PDFs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anukriti Kumar, Lucy Lu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most scholarly works are distributed online in PDF format, which can present
significant accessibility challenges for blind and low-vision readers. To
characterize the scope of this issue, we perform a large-scale analysis of 20K
open- and closed-access scholarly PDFs published between 2014-2023 sampled
across broad fields of study. We assess the accessibility compliance of these
documents based on six criteria: Default Language, Appropriate Nesting, Tagged
PDF, Table Headers, Tab Order, and Alt-Text; selected based on prior work and
the SIGACCESS Guide for Accessible PDFs. To ensure robustness, we corroborate
our findings through automated accessibility checking, manual evaluation of alt
text, comparative assessments with an alternate accessibility checker, and
manual assessments with screen readers. Our findings reveal that less than 3.2%
of tested PDFs satisfy all criteria, while a large majority (74.9%) fail to
meet any criteria at all. Worse yet, we observe a concerning drop in PDF
accessibility since 2019, largely among open-access papers, suggesting that
efforts to improve document accessibility have not taken hold and are on a
backslide. While investigating factors contributing to this drop, we identify
key associations between fields of study, creation platforms used, models of
publishing, and PDF accessibility compliance, suggesting that publisher and
author choices significantly influence document accessibility. This paper
highlights a new crisis in scholarly document accessibility and the need for a
multi-faceted approach to address the problem, involving the development of
better tools, enhanced author education, and systemic changes in academic
publishing practices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ People are poorly equipped to detect AI-powered voice clones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarah Barrington, Hany Farid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As generative AI continues its ballistic trajectory, everything from text to
audio, image, and video generation continues to improve in mimicking
human-generated content. Through a series of perceptual studies, we report on
the realism of AI-generated voices in terms of identity matching and
naturalness. We find human participants cannot reliably identify short
recordings (less than 20 seconds) of AI-generated voices. Specifically,
participants mistook the identity of an AI-voice for its real counterpart 80%
of the time, and correctly identified a voice as AI-generated only 60% of the
time. In all cases, performance is independent of the demographics of the
speaker or listener.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Label-Free Subjective Player Experience Modelling via Let's Play Videos <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dave Goel, Athar Mahmoudi-Nejad, Matthew Guzdial
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Player Experience Modelling (PEM) is the study of AI techniques applied to
modelling a player's experience within a video game. PEM development can be
labour-intensive, requiring expert hand-authoring or specialized data
collection. In this work, we propose a novel PEM development approach,
approximating player experience from gameplay video. We evaluate this approach
predicting affect in the game Angry Birds via a human subject study. We
validate that our PEM can strongly correlate with self-reported and sensor
measures of affect, demonstrating the potential of this approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures, AAAI Conference on Artificial Intelligence and
  Interactive Digital Entertainment</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AiBAT: Artificial Intelligence/Instructions for Build, Assembly, and
  Test 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Nuernberger, Anny Liu, Heather Stefanini, Richard Otis, Amanda Towler, R. Peter Dillon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instructions for Build, Assembly, and Test (IBAT) refers to the process used
whenever any operation is conducted on hardware, including tests, assembly, and
maintenance. Currently, the generation of IBAT documents is time-intensive, as
users must manually reference and transfer information from engineering
diagrams and parts lists into IBAT instructions. With advances in machine
learning and computer vision, however, it is possible to have an artificial
intelligence (AI) model perform the partial filling of the IBAT template,
freeing up engineer time for more highly skilled tasks. AiBAT is a novel system
for assisting users in authoring IBATs. It works by first analyzing assembly
drawing documents, extracting information and parsing it, and then filling in
IBAT templates with the extracted information. Such assisted authoring has
potential to save time and reduce cost. This paper presents an overview of the
AiBAT system, including promising preliminary results and discussion on future
work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-rays: Exploring Bias in the Gaze of AI Through a Multimodal
  Interactive Installation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyao Gao, Yiwen Zhang, Ling Li, Theodoros Papatheodorou, Wei Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data surveillance has become more covert and pervasive with AI algorithms,
which can result in biased social classifications. Appearance offers intuitive
identity signals, but what does it mean to let AI observe and speculate on
them? We introduce AI-rays, an interactive installation where AI generates
speculative identities from participants' appearance which are expressed
through synthesized personal items placed in participants' bags. It uses
speculative X-ray visions to contrast reality with AI-generated assumptions,
metaphorically highlighting AI's scrutiny and biases. AI-rays promotes
discussions on modern surveillance and the future of human-machine reality
through a playful, immersive experience exploring AI biases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Siggraph Asia 2024 Art Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pseudo-Automation: How Labor-Offsetting Technologies Reconfigure Roles
  and Relationships in Frontline Retail Work 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pegah Moradi, Karen Levy, Cristobal Cheyre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-service machines are a form of pseudo-automation; rather than actually
automate tasks, they offset them to unpaid customers. Typically implemented for
customer convenience and to reduce labor costs, self-service is often
criticized for worsening customer service and increasing loss and theft for
retailers. Though millions of frontline service workers continue to interact
with these technologies on a day-to-day basis, little is known about how these
machines change the nature of frontline labor. Through interviews with current
and former cashiers who work with self-checkout technologies, we investigate
how technology that offsets labor from an employee to a customer can
reconfigure frontline work. We find three changes to cashiering tasks as a
result of self-checkout: (1) Working at self-checkout involved parallel demands
from multiple customers, (2) self-checkout work was more problem-oriented
(including monitoring and policing customers), and (3) traditional checkout
began to become more demanding as easier transactions were filtered to
self-checkout. As their interactions with customers became more focused on
problem solving and rule enforcement, cashiers were often positioned as
adversaries to customers at self-checkout. To cope with perceived
adversarialism, cashiers engaged in a form of relational patchwork, using
techniques like scapegoating the self-checkout machine and providing excessive
customer service in order to maintain positive customer interactions in the
face of potential conflict. Our findings highlight how even under
pseudo-automation, workers must engage in relational work to manage and mend
negative human-to-human interactions so that machines can be properly
implemented in context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Forthcoming in the Proceedings of the 2025 Conference on Computer
  Supported Cooperative Work and Social Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards the Pedagogical Steering of Large Language Models for Tutoring:
  A Case Study with Modeling Productive Failure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romain Puech, Jakub Macina, Julia Chatain, Mrinmaya Sachan, Manu Kapur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One-to-one tutoring is one of the most efficient methods of teaching.
Following the rise in popularity of Large Language Models (LLMs), there have
been efforts to use them to create conversational tutoring systems, which can
make the benefits of one-to-one tutoring accessible to everyone. However,
current LLMs are primarily trained to be helpful assistants and thus lack
crucial pedagogical skills. For example, they often quickly reveal the solution
to the student and fail to plan for a richer multi-turn pedagogical
interaction. To use LLMs in pedagogical scenarios, they need to be steered
towards using effective teaching strategies: a problem we introduce as
Pedagogical Steering and believe to be crucial for the efficient use of LLMs as
tutors. We address this problem by formalizing a concept of tutoring strategy,
and introducing StratL, an algorithm to model a strategy and use prompting to
steer the LLM to follow this strategy. As a case study, we create a prototype
tutor for high school math following Productive Failure (PF), an advanced and
effective learning design. To validate our approach in a real-world setting, we
run a field study with 17 high school students in Singapore. We quantitatively
show that StratL succeeds in steering the LLM to follow a Productive Failure
tutoring strategy. We also thoroughly investigate the existence of spillover
effects on desirable properties of the LLM, like its ability to generate
human-like answers. Based on these results, we highlight the challenges in
Pedagogical Steering and suggest opportunities for further improvements. We
further encourage follow-up research by releasing a dataset of Productive
Failure problems and the code of our prototype and algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 9 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aggregation of Constrained Crowd Opinions for Urban Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akanksha Das, Jyoti Patel, Malay Bhattacharyya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collective decision making is often a customary action taken in government
crowdsourcing. Through ensemble of opinions (popularly known as judgment
analysis), governments can satisfy majority of the people who provided
opinions. This has various real-world applications like urban planning or
participatory budgeting that require setting up {\em facilities} based on the
opinions of citizens. Recently, there is an emerging interest in performing
judgment analysis on opinions that are constrained. We consider a new dimension
of this problem that accommodate background constraints in the problem of
judgment analysis, which ensures the collection of more responsible opinions.
The background constraints refer to the restrictions (with respect to the
existing infrastructure) to be taken care of while performing the consensus of
opinions. In this paper, we address the said kind of problems with efficient
unsupervised approaches of learning suitably modified to cater to the
constraints of urban planning. We demonstrate the effectiveness of this
approach in various scenarios where the opinions are taken for setting up ATM
counters and sewage lines. Our main contributions encompass a novel approach of
collecting data for smart city planning (in the presence of constraints),
development of methods for opinion aggregation in various formats. As a whole,
we present a new dimension of judgment analysis by adding background
constraints to the problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning
  in Social VR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengxu Pan, Alexandra Kitson, Hongyu Wan, Mirjana Prpa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many people struggle with learning a new language, with traditional tools
falling short in providing contextualized learning tailored to each learner's
needs. The recent development of large language models (LLMs) and embodied
conversational agents (ECAs) in social virtual reality (VR) provide new
opportunities to practice language learning in a contextualized and
naturalistic way that takes into account the learner's language level and
needs. To explore this opportunity, we developed ELLMA-T, an ECA that leverages
an LLM (GPT-4) and situated learning framework for supporting learning English
language in social VR (VRChat). Drawing on qualitative interviews (N=12), we
reveal the potential of ELLMA-T to generate realistic, believable and
context-specific role plays for agent-learner interaction in VR, and LLM's
capability to provide initial language assessment and continuous feedback to
learners. We provide five design implications for the future development of
LLM-based language agents in social VR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Source Data Selection for Brain-Computer Interfaces based on Simple
  Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frida Heskebeck, Carolina Bergeling, Bo Bernhardsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper demonstrates that simple features available during the calibration
of a brain-computer interface can be utilized for source data selection to
improve the performance of the brain-computer interface for a new target user
through transfer learning. To support this, a public motor imagery dataset is
used for analysis, and a method called the Transfer Performance Predictor
method is presented. The simple features are based on the covariance matrices
of the data and the Riemannian distance between them. The Transfer Performance
Predictor method outperforms other source data selection methods as it selects
source data that gives a better transfer learning performance for the target
users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Capacitive Touch Images Enhance Mobile Keyboard Decoding? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piyawat Lertvittayakumjorn, Shanqing Cai, Billy Dou, Cedric Ho, Shumin Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capacitive touch sensors capture the two-dimensional spatial profile
(referred to as a touch heatmap) of a finger's contact with a mobile
touchscreen. However, the research and design of touchscreen mobile keyboards
-- one of the most speed and accuracy demanding touch interfaces -- has focused
on the location of the touch centroid derived from the touch image heatmap as
the input, discarding the rest of the raw spatial signals. In this paper, we
investigate whether touch heatmaps can be leveraged to further improve the tap
decoding accuracy for mobile touchscreen keyboards. Specifically, we developed
and evaluated machine-learning models that interpret user taps by using the
centroids and/or the heatmaps as their input and studied the contribution of
the heatmaps to model performance. The results show that adding the heatmap
into the input feature set led to 21.4% relative reduction of character error
rates on average, compared to using the centroid alone. Furthermore, we
conducted a live user study with the centroid-based and heatmap-based decoders
built into Pixel 6 Pro devices and observed lower error rate, faster typing
speed, and higher self-reported satisfaction score based on the heatmap-based
decoder than the centroid-based decoder. These findings underline the promise
of utilizing touch heatmaps for improving typing experience in mobile
keyboards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to UIST 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Capturing complex hand movements and object interactions using machine
  learning-powered stretchable smart textile gloves 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arvin Tashakori, Zenan Jiang, Amir Servati, Saeid Soltanian, Harishkumar Narayana, Katherine Le, Caroline Nakayama, Chieh-ling Yang, Z. Jane Wang, Janice J. Eng, Peyman Servati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate real-time tracking of dexterous hand movements and interactions has
numerous applications in human-computer interaction, metaverse, robotics, and
tele-health. Capturing realistic hand movements is challenging because of the
large number of articulations and degrees of freedom. Here, we report accurate
and dynamic tracking of articulated hand and finger movements using
stretchable, washable smart gloves with embedded helical sensor yarns and
inertial measurement units. The sensor yarns have a high dynamic range,
responding to low 0.005 % to high 155 % strains, and show stability during
extensive use and washing cycles. We use multi-stage machine learning to report
average joint angle estimation root mean square errors of 1.21 and 1.45 degrees
for intra- and inter-subjects cross-validation, respectively, matching accuracy
of costly motion capture cameras without occlusion or field of view
limitations. We report a data augmentation technique that enhances robustness
to noise and variations of sensors. We demonstrate accurate tracking of
dexterous hand movements during object interactions, opening new avenues of
applications including accurate typing on a mock paper keyboard, recognition of
complex dynamic and static gestures adapted from American Sign Language and
object identification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond correlation: The impact of human uncertainty in measuring the
  effectiveness of automatic evaluation and LLM-as-a-judge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aparna Elangovan, Jongwoo Ko, Lei Xu, Mahsa Elyasi, Ling Liu, Sravan Bodapati, Dan Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The effectiveness of automatic evaluation of generative models is typically
measured by comparing it to human evaluation using correlation metrics.
However, metrics like Krippendorff's $\alpha$ and Randolph's $\kappa$,
originally designed to measure the reliability of human labeling, make
assumptions about human behavior and the labeling process. In this paper, we
show how *relying on a single aggregate correlation score* can obscure
fundamental differences between human behavior and automatic evaluation
methods, including LLM-as-a-Judge. Specifically, we demonstrate that when the
proportion of samples with variation or uncertainty in human labels (gathered
during human evaluation) is relatively high, machine labels (generated by
automatic evaluation methods) may superficially appear to have similar or
better correlation with the human majority label compared to human-to-human
(HH) correlation. This can create the misleading impression that automatic
evaluation is accurate enough to approximate the human majority label. However,
as the proportion of samples with consistent human labels increases, the
correlation between machine labels and human majority labels declines, falling
below HH correlation. Based on these findings, we first propose stratifying
results by human label uncertainty to provide a more robust analysis of
automatic evaluation performance. Second, recognizing that uncertainty and
variation are inherent in perception-based human evaluations, such as those
involving attitudes or preferences, we introduce a new metric - *binned
Jensen-Shannon Divergence for perception* for such scenarios to better measure
the effectiveness of automatic evaluations. Third, we present visualization
techniques -- *perception charts*, to compare the strengths and limitations of
automatic evaluation and to contextualize correlation measures appropriately
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-Based Risk Model for Improved Driver Support in Interactive
  Driving Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Puphal, Benedict Flade, Matti Krüger, Ryohei Hirano, Akihito Kimata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of human-based driver support. Nowadays,
driver support systems help users to operate safely in many driving situations.
Nevertheless, these systems do not fully use the rich information that is
available from sensing the human driver. In this paper, we therefore present a
human-based risk model that uses driver information for improved driver
support. In contrast to state of the art, our proposed risk model combines a)
the current driver perception based on driver errors, such as the driver
overlooking another vehicle (i.e., notice error), and b) driver
personalization, such as the driver being defensive or confident. In extensive
simulations of multiple interactive driving scenarios, we show that our novel
human-based risk model achieves earlier warning times and reduced warning
errors compared to a baseline risk model not using human driver information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ E2H: A Two-Stage Non-Invasive Neural Signal Driven Humanoid Robotic
  Whole-Body Control Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqun Duan, Jinzhao Zhou, Xiaowei Jiang, Qiang Zhang, Jingkai Sun, Jiahang Cao, Jiaxu Wang, Yiqian Yang, Wen Zhao, Gang Han, Yijie Guo, Chin-Teng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in humanoid robotics, including the integration of
hierarchical reinforcement learning-based control and the utilization of LLM
planning, have significantly enhanced the ability of robots to perform complex
tasks. In contrast to the highly developed humanoid robots, the human factors
involved remain relatively unexplored. Directly controlling humanoid robots
with the brain has already appeared in many science fiction novels, such as
Pacific Rim and Gundam. In this work, we present E2H (EEG-to-Humanoid), an
innovative framework that pioneers the control of humanoid robots using
high-frequency non-invasive neural signals. As the none-invasive signal quality
remains low in decoding precise spatial trajectory, we decompose the E2H
framework in an innovative two-stage formation: 1) decoding neural signals
(EEG) into semantic motion keywords, 2) utilizing LLM facilitated motion
generation with a precise motion imitation control policy to realize humanoid
robotics control. The method of directly driving robots with brainwave commands
offers a novel approach to human-machine collaboration, especially in
situations where verbal commands are impractical, such as in cases of speech
impairments, space exploration, or underwater exploration, unlocking
significant potential. E2H offers an exciting glimpse into the future, holding
immense potential for human-computer interaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rel-A.I.: An Interaction-Centered Approach To Measuring Human-LM
  Reliance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07950v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07950v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaitlyn Zhou, Jena D. Hwang, Xiang Ren, Nouha Dziri, Dan Jurafsky, Maarten Sap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to communicate uncertainty, risk, and limitation is crucial for
the safety of large language models. However, current evaluations of these
abilities rely on simple calibration, asking whether the language generated by
the model matches appropriate probabilities. Instead, evaluation of this aspect
of LLM communication should focus on the behaviors of their human
interlocutors: how much do they rely on what the LLM says? Here we introduce an
interaction-centered evaluation framework called Rel-A.I. (pronounced "rely"})
that measures whether humans rely on LLM generations. We use this framework to
study how reliance is affected by contextual features of the interaction (e.g,
the knowledge domain that is being discussed), or the use of greetings
communicating warmth or competence (e.g., "I'm happy to help!"). We find that
contextual characteristics significantly affect human reliance behavior. For
example, people rely 10% more on LMs when responding to questions involving
calculations and rely 30% more on LMs that are perceived as more competent. Our
results show that calibration and language quality alone are insufficient in
evaluating the risks of human-LM interactions, and illustrate the need to
consider features of the interactional context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">PRompt</span> Optimization in Multi-Step Tasks (PROMST): Integrating Human
  Feedback and Heuristic-based Sampling <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08702v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08702v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Chen, Jacob Arkin, Yilun Hao, Yang Zhang, Nicholas Roy, Chuchu Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt optimization aims to find the best prompt to a large language model
(LLM) for a given task. LLMs have been successfully used to help find and
improve prompt candidates for single-step tasks. However, realistic tasks for
agents are multi-step and introduce new challenges: (1) Prompt content is
likely to be more extensive and complex, making it more difficult for LLMs to
analyze errors, (2) the impact of an individual step is difficult to evaluate,
and (3) different people may have varied preferences about task execution.
While humans struggle to optimize prompts, they are good at providing feedback
about LLM outputs; we therefore introduce a new LLM-driven discrete prompt
optimization framework PRompt Optimization in Multi-Step Tasks (PROMST) that
incorporates human-designed feedback rules to automatically offer direct
suggestions for improvement. We also use an extra learned heuristic model that
predicts prompt performance to efficiently sample from prompt candidates. This
approach significantly outperforms both human-engineered prompts and several
other prompt optimization methods across 11 representative multi-step tasks (an
average 10.6\%-29.3\% improvement to current best methods on five LLMs
respectively). We believe our work can serve as a benchmark for automatic
prompt optimization for LLM-driven multi-step tasks. Datasets and Codes are
available at https://github.com/yongchao98/PROMST. Project Page is available at
https://yongchao98.github.io/MIT-REALM-PROMST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>62 pages, 14 figures, Published in EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract
  Patients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04620v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04620v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pragnya Ramjee, Bhuvan Sachdeva, Satvik Golechha, Shreyas Kulkarni, Geeta Fulari, Kaushik Murali, Mohit Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The healthcare landscape is evolving, with patients seeking reliable
information about their health conditions and available treatment options.
Despite the abundance of information sources, the digital age overwhelms
individuals with excess, often inaccurate information. Patients primarily trust
medical professionals, highlighting the need for expert-endorsed health
information. However, increased patient loads on experts has led to reduced
communication time, impacting information sharing. To address this gap, we
develop CataractBot, an experts-in-the-loop chatbot powered by LLMs, in
collaboration with an eye hospital in India. CataractBot answers cataract
surgery related questions instantly by querying a curated knowledge base, and
provides expert-verified responses asynchronously. It has multimodal and
multilingual capabilities. In an in-the-wild deployment study with 55
participants, CataractBot proved valuable, providing anytime accessibility,
saving time, accommodating diverse literacy levels, alleviating power
differences, and adding a privacy layer between patients and doctors. Users
reported that their trust in the system was established through expert
verification. Broadly, our results could inform future work on designing
expert-mediated LLM bots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Practicing Stress Relief for the Everyday: Designing Social Simulation
  Using VR, AR, and LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01672v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01672v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Fang, Hriday Chhabria, Alekhya Maram, Haiyi Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stress is an inevitable part of day-to-day life yet many find themselves
unable to manage it themselves, particularly when professional or peer support
are not always readily available. As self-care becomes increasingly vital for
mental well-being, this paper explores the potential of social simulation as a
safe, virtual environment for practicing stress relief for everyday situations.
Leveraging the immersive capabilities of VR, AR, and LLMs, we developed eight
interactive prototypes for various everyday stressful scenarios (e.g. public
speaking) then conducted prototype-driven semi-structured interviews with 19
participants. We reveal that people currently lack effective means to support
themselves through everyday stress and found that social simulation fills a gap
for simulating real environments for training mental health practices. We
outline key considerations for future development of simulation for self-care,
including risks of trauma from hyper-realism, distrust of LLM-recommended
timing for mental health recommendations, and the value of accessibility for
self-care interventions.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Programming Languages <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Long Way to Deforestation (Technical Report): A Type Inference and
  Elaboration Technique for Removing Intermediate Data Structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijia Chen, Lionel Parreaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deforestation is a compiler optimization that removes intermediate data
structure allocations from functional programs to improve their efficiency.
This is an old idea, but previous approaches have proved limited or
impractical: they either only worked on compositions of predefined combinators
(shortcut fusion), or involved the aggressive unfolding of recursive
definitions until a depth limit was reached or a reoccurring pattern was found
to tie the recursive knot, resulting in impractical algorithmic complexity and
large amounts of code duplication. We present Lumberhack, a general-purpose
deforestation approach for purely functional call-by-value programs. Lumberhack
uses subtype inference to reason about data structure production and
consumption and uses an elaboration pass to fuse the corresponding recursive
definitions. It fuses large classes of mutually recursive definitions while
avoiding much of the unproductive (and sometimes counter-productive) code
duplication inherent in previous approaches. We prove the soundness of
Lumberhack using logical relations and experimentally demonstrate significant
speedups in the standard nofib benchmark suite.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is the technical report version of the paper published at ICFP
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The why, what, and how of AI-based coding in scientific research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tonghe Zhuang, Zhicheng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer programming (coding) is indispensable for researchers across
disciplines, yet it remains challenging to learn and time-consuming to carry
out. Generative AI, particularly large language models (LLMs), has the
potential to transform coding into intuitive conversations, but best practices
and effective workflows are only emerging. We dissect AI-based coding through
three key lenses: the nature and role of LLMs in coding (why), six types of
coding assistance they provide (what), and a five-step workflow in action with
practical implementation strategies (how). Additionally, we address the
limitations and future outlook of AI in coding. By offering actionable
insights, this framework helps to guide researchers in effectively leveraging
AI to enhance coding practices and education, accelerating scientific progress.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 7 figure, 3 boxes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A robust graph-based approach to observational equivalence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1907.01257v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1907.01257v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dan R. Ghica, Koko Muroya, Todd Waugh Ambridge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new step-wise approach to proving observational equivalence, and
in particular reasoning about fragility of observational equivalence. Our
approach is based on what we call local reasoning. The local reasoning exploits
the graphical concept of neighbourhood, and it extracts a new, formal, concept
of robustness as a key sufficient condition of observational equivalence.
Moreover, our proof methodology is capable of proving a generalised notion of
observational equivalence. The generalised notion can be quantified over
syntactically restricted contexts instead of all contexts, and also
quantitatively constrained in terms of the number of reduction steps. The
operational machinery we use is given by a hypergraph-rewriting abstract
machine inspired by Girard's Geometry of Interaction. The behaviour of language
features, including function abstraction and application, is provided by
hypergraph-rewriting rules. We demonstrate our proof methodology using the
call-by-value lambda-calculus equipped with (higher-order) state.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Operating Systems <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The eBPF Runtime in the Linux Kernel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00026v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00026v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolaji Gbadamosi, Luigi Leonardi, Tobias Pulls, Toke Høiland-Jørgensen, Simone Ferlin-Reiter, Simo Sorce, Anna Brunström
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extended Berkeley Packet Filter (eBPF) is a runtime that enables users to
load programs into the operating system (OS) kernel, like Linux or Windows, and
execute them safely and efficiently at designated kernel hooks. Each program
passes through a verifier that reasons about the safety guarantees for
execution. Hosting a safe virtual machine runtime within the kernel makes it
dynamically programmable. Unlike the popular approach of bypassing or
completely replacing the kernel, eBPF gives users the flexibility to modify the
kernel on the fly, rapidly experiment and iterate, and deploy solutions to
achieve their workload-specific needs, while working in concert with the
kernel.
  In this paper, we present the first comprehensive description of the design
and implementation of the eBPF runtime in the Linux kernel. We argue that eBPF
today provides a mature and safe programming environment for the kernel. It has
seen wide adoption since its inception and is increasingly being used not just
to extend, but program entire components of the kernel, while preserving its
runtime integrity. We outline the compelling advantages it offers for
real-world production usage, and illustrate current use cases. Finally, we
identify its key challenges, and discuss possible future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Securing Cloud File Systems with Trusted Execution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18639v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18639v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quinn Burke, Yohan Beugin, Blaine Hoak, Rachel King, Eric Pauley, Ryan Sheatsley, Mingli Yu, Ting He, Thomas La Porta, Patrick McDaniel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloud file systems offer organizations a scalable and reliable file storage
solution. However, cloud file systems have become prime targets for
adversaries, and traditional designs are not equipped to protect organizations
against the myriad of attacks that may be initiated by a malicious cloud
provider, co-tenant, or end-client. Recently proposed designs leveraging
cryptographic techniques and trusted execution environments (TEEs) still force
organizations to make undesirable trade-offs, consequently leading to either
security, functional, or performance limitations. In this paper, we introduce
BFS, a cloud file system that leverages the security capabilities provided by
TEEs to bootstrap new security protocols that deliver strong security
guarantees, high-performance, and a transparent POSIX-like interface to
clients. BFS delivers stronger security guarantees and up to a 2.5X speedup
over a state-of-the-art secure file system. Moreover, compared to the industry
standard NFS, BFS achieves up to 2.2X speedups across micro-benchmarks and
incurs <1X overhead for most macro-benchmark workloads. BFS demonstrates a
holistic cloud file system design that does not sacrifice an organizations'
security yet can embrace all of the functional and performance advantages of
outsourcing.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Networking and Internet Architecture <span class="chip" style="font-size: 60%">16</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digital Twin for O-RAN Towards 6G 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan X. Nguyen, Kexuan Sun, Duc To, Quoc-Tuan Vien, Tuan Anh Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In future wireless systems of beyond 5G and 6G, addressing diverse
applications with varying quality requirements is essential. Open Radio Access
Network (O-RAN) architectures offer the potential for dynamic resource
adaptation based on traffic demands. However, achieving real-time resource
orchestration remains a challenge. Simultaneously, Digital Twin (DT) technology
holds promise for testing and analysing complex systems, offering a unique
platform for addressing dynamic operation and automation in O-RAN
architectures. Yet, developing DTs for complex 5G/6G networks poses challenges,
including data exchanges, ML model training data availability, network
dynamics, processing power limitations, interdisciplinary collaboration needs,
and a lack of standardized methodologies. This paper provides an overview of
Open RAN architecture, trend and challenges, proposing the DT concepts for
O-RAN with solution examples showcasing its integration into the framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Communications Magazine 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Similarity-Based One-Shot Clustering for Multi-Task Hierarchical
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdulmoneam Ali, Ahmed Arafa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of cluster identity estimation in a hierarchical
federated learning setting in which users work toward learning different tasks.
To overcome the challenge of task heterogeneity, users need to be grouped in a
way such that users with the same task are in the same group, conducting
training together, while sharing the weights of feature extraction layers with
the other groups. Toward that end, we propose a one-shot clustering algorithm
that can effectively identify and group users based on their data similarity.
This enables more efficient collaboration and sharing of a common layer
representation within the federated learning system. Our proposed algorithm not
only enhances the clustering process, but also overcomes challenges related to
privacy concerns, communication overhead, and the need for prior knowledge
about learning models or loss function behaviors. We validate our proposed
algorithm using various datasets such as CIFAR-10 and Fashion MNIST, and show
that it outperforms the baseline in terms of accuracy and variance reduction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Asilomar 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ User-centric Immersive Communications in 6G: A Data-oriented Approach
  via Digital Twin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Conghao Zhou, Shisheng Hu, Jie Gao, Xinyu Huang, Weihua Zhuang, Xuemin Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we present a novel user-centric service provision for
immersive communications (IC) in 6G to deal with the uncertainty of individual
user behaviors while satisfying unique requirements on the quality of
multi-sensory experience. To this end, we propose a data-oriented approach for
network resource management, featuring personalized data management that can
support network modeling tailored to different user demands. Our approach
leverages the digital twin (DT) technique as a key enabler. Particularly, a DT
is established for each user, and the data attributes in the DT are customized
based on the characteristics of the user. The DT functions, corresponding to
various data operations, are customized in the development, evaluation, and
update of network models to meet unique user demands. A trace-driven case study
demonstrates the effectiveness of our approach in achieving user-centric IC and
the significance of personalized data management in 6G.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Research Directions and Modeling Guidelines for Industrial Internet of
  Things Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giampaolo Cuozzo, Enrico Testi, Salvatore Riolo, Luciano Miuccio, Gianluca Cena, Gianni Pasolini, Luca De Nardis, Daniela Panno, Marco Chiani, Maria-Gabriella Di Benedetto, Enrico Buracchini, Roberto Verdone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Industrial Internet of Things (IIoT) paradigm has emerged as a
transformative force, revolutionizing industrial processes by integrating
advanced wireless technologies into traditional procedures to enhance their
efficiency. The importance of this paradigm shift has produced a massive, yet
heterogeneous, proliferation of scientific contributions. However, these works
lack a standardized and cohesive characterization of the IIoT framework coming
from different entities, like the 3rd Generation Partnership Project (3GPP) or
the 5G Alliance for Connected Industries and Automation (5G-ACIA), resulting in
divergent perspectives and potentially hindering interoperability. To bridge
this gap, this article offers a unified characterization of (i) the main IIoT
application domains, (ii) their respective requirements, (iii) the principal
technological gaps existing in the current literature, and, most importantly,
(iv) we propose a systematic approach for assessing and addressing the
identified research challenges. Therefore, this article serves as a roadmap for
future research endeavors, promoting a unified vision of the IIoT paradigm and
fostering collaborative efforts to advance the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning Approaches for Active Queue Management: A <span class="highlight-title">Survey</span>,
  Taxonomy, and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Parsa Toopchinezhad, Mahmood Ahmadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active Queue Management (AQM), a network-layer congestion control technique
endorsed by the Internet Engineering Task Force (IETF), encourages routers to
discard packets before the occurrence of buffer overflow. Traditional AQM
techniques often employ heuristic approaches that require meticulous parameter
adjustments, limiting their real-world applicability. In contrast, Machine
Learning (ML) approaches offer highly adaptive, data-driven solutions custom to
dynamic network conditions. Consequently, many researchers have adapted ML for
AQM throughout the years, resulting in a wide variety of algorithms ranging
from predicting congestion via supervised learning to discovering optimal
packet-dropping policies with reinforcement learning. Despite these remarkable
advancements, no previous work has compiled these methods in the form of a
survey article. This paper presents the first thorough documentation and
analysis of ML-based algorithms for AQM, in which the strengths and limitations
of each proposed method are evaluated and compared. In addition, a novel
taxonomy of ML approaches based on methodology is also established. The review
is concluded by discussing unexplored research gaps and potential new
directions for more robust ML-AQM methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Digital Twinning of Random Systems with Twinning Rate
  Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caglar Tunc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the massive advancements in processing power, Digital Twins (DTs) have
become powerful tools to monitor and analyze physical entities. However, due to
the potentially very high number of Physical Systems (PSs) to be tracked and
emulated, for instance, in a factory environment or an Internet of Things (IoT)
network, continuous twinning might become infeasible. In this paper, a DT
system is investigated with a set of random PSs, where the twinning rate is
limited due to resource constraints. Three cost functions are considered to
quantify and penalize the twinning delay. For these cost functions, the optimal
twinning problem under twinning rate constraints is formulated. In a numerical
example, the proposed cost functions are evaluated for two, one push-based and
one pull-based, benchmark twinning policies. The proposed methodology is the
first to investigate the optimal twinning problem with random PSs and twinning
rate constraints, and serves as a guideline for real-world implementations on
how frequently PSs should be twinned.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Load Balancing-based Topology Adaptation for Integrated Access and
  Backhaul Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raul Victor de O. Paiva, Fco. Italo G. Carvalho, Fco. Rafael M. Lima, Victor F. Monteiro, Diego A. Sousa, Darlan C. Moreira, Tarcisio F. Maciel, Behrooz Makki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrated access and backhaul (IAB) technology is a flexible solution for
network densification. IAB nodes can also be deployed in moving nodes such as
buses and trains, i.e., mobile IAB (mIAB). As mIAB nodes can move around the
coverage area, the connection between mIAB nodes and their parent macro base
stations (BSs), IAB donor, is sometimes required to change in order to keep an
acceptable backhaul link, the so called topology adaptation (TA). The change
from one IAB donor to another may strongly impact the system load distribution,
possibly causing unsatisfactory backhaul service due to the lack of radio
resources. Based on this, TA should consider both backhaul link quality and
traffic load. In this work, we propose a load balancing algorithm based on TA
for IAB networks, and compare it with an approach in which TA is triggered
based on reference signal received power (RSRP) only. The results show that our
proposed algorithm improves the passengers worst connections throughput in
uplink (UL) and, more modestly, also in downlink (DL), without impairing the
pedestrian quality of service (QoS) significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper submitted to Journal of Communication and Information Systems
  (JCIS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cellular Network Densification: a System-level Analysis with IAB, NCR
  and RIS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel C. M. da Silva, Victor F. Monteiro, Diego A. Sousa, Darlan C. Moreira, Tarcisio F. Maciel, Fco. Rafael M. Lima, Behrooz Makki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the number of user equipments increases in fifth generation (5G) and
beyond, it is desired to densify the cellular network with auxiliary nodes
assisting the base stations. Examples of these nodes are integrated access and
backhaul (IAB) nodes, network-controlled repeaters (NCRs) and reconfigurable
intelligent surfaces (RISs). In this context, this work presents a system level
overview of these three nodes. Moreover, this work evaluates through
simulations the impact of network planning aiming at enhancing the performance
of a network used to cover an outdoor sport event. We show that, in the
considered scenario, in general, IAB nodes provide an improved signal to
interference-plus-noise ratio and throughput, compared to NCRs and RISs.
However, there are situations where NCR outperforms IAB due to higher level of
interference caused by the latter. Finally, we show that the deployment of
these nodes in unmanned aerial vehicles (UAVs) also achieves performance gains
due to their aerial mobility. However, UAV constraints related to aerial
deployment may prevent these nodes from reaching results as good as the ones
achieved by their stationary deployment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper submitted to IEEE Systems Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AirTags for Human Localization, Not Just Objects <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed I. Hany, Hamada Rizk, Moustafa Youssef
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Indoor localization has become increasingly important due to its wide-ranging
applications in indoor navigation, emergency services, the Internet of Things
(IoT), and accessibility for individuals with special needs. Traditional
localization systems often require extensive calibration to achieve high
accuracy. We introduce UbiLoc, an innovative, calibration-free indoor
localization system that leverages Apple AirTags in a novel way to localize
users instead of tracking objects. By utilizing the ubiquitous presence of
AirTags and their Ultra-Wideband (UWB) technology, UbiLoc achieves
centimeter-level accuracy, surpassing traditional WiFi and Bluetooth Low Energy
(BLE) systems. UbiLoc addresses key challenges, including ranging errors caused
by multipath and noise, through a novel AirTag selection technique. The system
operates without the need for manual calibration, ensuring robustness and
self-maintenance. Deployed on various Apple devices and tested in real-world
environments, UbiLoc achieved median localization errors as low as 26 cm in a
campus building and 31.5 cm in an apartment setting. These results demonstrate
that UbiLoc is the first system to offer reliable, cm-level accuracy using
widely available technology without requiring calibration, making it a
promising solution for next-generation indoor localization systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in 2nd ACM SIGSPATIAL International Workshop
  on Geo-Privacy and Data Utility for Smart Societies: 7 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Reinforcement Learning to Optimize Teleoperated Driving
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippo Bragato, Marco Giordani, Michele Zorzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several sixth generation (6G) use cases have tight requirements in terms of
reliability and latency, in particular teleoperated driving (TD). To address
those requirements, Predictive Quality of Service (PQoS), possibly combined
with reinforcement learning (RL), has emerged as a valid approach to
dynamically adapt the configuration of the TD application (e.g., the level of
compression of automotive data) to the experienced network conditions. In this
work, we explore different classes of RL algorithms for PQoS, namely MAB
(stateless), SARSA (stateful on-policy), Q-Learning (stateful off-policy), and
DSARSA and DDQN (with Neural Network (NN) approximation). We trained the agents
in a federated learning (FL) setup to improve the convergence time and
fairness, and to promote privacy and security. The goal is to optimize the
trade-off between Quality of Service (QoS), measured in terms of the end-to-end
latency, and Quality of Experience (QoE), measured in terms of the quality of
the resulting compression operation. We show that Q-Learning uses a small
number of learnable parameters, and is the best approach to perform PQoS in the
TD scenario in terms of average reward, convergence, and computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication at IEEE Global
  Communications Conference (GLOBECOM), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MTDNS: Moving Target Defense for Resilient DNS Infrastructure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdullah Aydeger, Pei Zhou, Sanzida Hoque, Marco Carvalho, Engin Zeydan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most critical components of the Internet that an attacker could
exploit is the DNS (Domain Name System) protocol and infrastructure.
Researchers have been constantly developing methods to detect and defend
against the attacks against DNS, specifically DNS flooding attacks. However,
most solutions discard packets for defensive approaches, which can cause
legitimate packets to be dropped, making them highly dependable on detection
strategies. In this paper, we propose MTDNS, a resilient MTD-based approach
that employs Moving Target Defense techniques through Software Defined
Networking (SDN) switches to redirect traffic to alternate DNS servers that are
dynamically created and run under the Network Function Virtualization (NFV)
framework. The proposed approach is implemented in a testbed environment by
running our DNS servers as separate Virtual Network Functions, NFV Manager, SDN
switches, and an SDN Controller. The experimental result shows that the MTDNS
approach achieves a much higher success rate in resolving DNS queries and
significantly reduces average latency even if there is a DNS flooding attack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, Accepted for publication at IEEE CCNC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resource Allocation Based on Optimal Transport Theory in ISAC-Enabled
  Multi-UAV Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufeng Zheng, Lixin Li, Wensheng Lin, Wei Liang, Qinghe Du, Zhu Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the resource allocation optimization for cooperative
communication with non-cooperative localization in integrated sensing and
communications (ISAC)-enabled multi-unmanned aerial vehicle (UAV) cooperative
networks. Our goal is to maximize the weighted sum of the system's average sum
rate and the localization quality of service (QoS) by jointly optimizing cell
association, communication power allocation, and sensing power allocation.
Since the formulated problem is a mixed-integer nonconvex problem, we propose
the alternating iteration algorithm based on optimal transport theory (AIBOT)
to solve the optimization problem more effectively. Simulation results
demonstrate that the AIBOT can improve the system sum rate by nearly 12% and
reduce the localization Cr'amer-Rao bound (CRB) by almost 29% compared to
benchmark algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SC-CDM: Enhancing Quality of Image Semantic Communication with a Compact
  Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02121v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02121v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kexin Zhang, Lixin Li, Wensheng Lin, Yuna Yan, Wenchi Cheng, Zhu Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic Communication (SC) is an emerging technology that has attracted much
attention in the sixth-generation (6G) mobile communication systems. However,
few literature has fully considered the perceptual quality of the reconstructed
image. To solve this problem, we propose a generative SC for wireless image
transmission (denoted as SC-CDM). This approach leverages compact diffusion
models to improve the fidelity and semantic accuracy of the images
reconstructed after transmission, ensuring that the essential content is
preserved even in bandwidth-constrained environments. Specifically, we aim to
redesign the swin Transformer as a new backbone for efficient semantic feature
extraction and compression. Next, the receiver integrates the slim prior and
image reconstruction networks. Compared to traditional Diffusion Models (DMs),
it leverages DMs' robust distribution mapping capability to generate a compact
condition vector, guiding image recovery, thus enhancing the perceptual details
of the reconstructed images. Finally, a series of evaluation and ablation
studies are conducted to validate the effectiveness and robustness of the
proposed algorithm and further increase the Peak Signal-to-Noise Ratio (PSNR)
by over 17% on top of CNN-based DeepJSCC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2408.05112</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lossy Cooperative UAV Relaying Networks: Outage Probability Analysis and
  Location Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ya Lian, Wensheng Lin, Lixin Li, Fucheng Yang, Zhu Han, Tad Matsumoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, performance of a lossy cooperative unmanned aerial vehicle
(UAV) relay communication system is analyzed. In this system, the UAV relay
adopts lossy forward (LF) strategy and the receiver has certain distortion
requirements for the received information. For the system described above, we
first derive the achievable rate distortion region of the system. Then, on the
basis of the region analysis, the system outage probability when the channel
suffers Nakagami-$m$ fading is analyzed. Finally, we design an optimal relay
position identification algorithm based on the Soft Actor-Critic (SAC)
algorithm, which determines the optimal UAV position to minimize the outage
probability. The simulation results show that the proposed algorithm can
optimize the UAV position and reduce the system outage probability effectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Routing and Spectrum Allocation in Broadband Degenerate EPR-Pair
  Distribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14613v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14613v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan Bali, Ashley Tittelbaugh, Shelbi L. Jenkins, Anuj Agrawal, Jerry Horgan, Marco Ruffini, Daniel Kilper, Boulat A. Bash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate resource allocation for quantum entanglement distribution over
an optical network. We characterize and model a network architecture that
employs a single quasideterministic time-frequency heralded EPR-pair source,
and develop a routing scheme for distributing entangled photon pairs over such
a network. We focus on fairness in entanglement distribution, and compare both
the performance of various spectrum allocation schemes as well as their Jain
index.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wykorzystanie Rekonfigurowalnych Iinteligentnych Matryc Antenowych w
  Łączu Dosyłowym Sieci 5G/6G Wykorzystującej Bezzałogowe
  Statki Powietrzne 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salim Janji, Paweł Sroka, Adrian Kliks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drony, dzi\k{e}ki mo\.zliwo\'sci ich szybkiego rozmieszczenia w trudnym
terenie, uwa\.zane s\k{a} za jeden z kluczowych element\'ow system\'ow
bezprzewodowych 6G. Jednak w celu wykorzystania ich jako punkty dost\k{e}powe
sieci konieczne jest zapewnienie {\l}\k{a}cza dosy{\l}owego o odpowiedniej
przepustowo\'sci. Dlatego w niniejszym artykule rozwa\.zane jest
zwi\k{e}kszenie zasi\k{e}gu sieci bezprzewodowej przez zapewnienie {\l}\k{a}cza
dosy{\l}owego dla ko\'ncowego punktu dost\k{e}powego z wykorzystaniem
okre\'slonej liczby dron\'ow-przeka\'znik\'ow oraz rekonfigurowalnych
inteligentnych matryc antenowych (RIS). Zaprezentowane wyniki bada\'n
symulacyjnych pokazuj\k{a}, \.ze u\.zycie RIS pozwala na znacz\k{a}ce
zwi\k{e}kszenie zasi\k{e}gu sieci bez konieczno\'sci stosowania dodatkowych
przeka\'znik\'ow.
  --
  Unmanned Aerial Vehicles, due to the possibility of their fast deployment,
are considered an essential element of the future wireless 6G communication
systems. However, an essential enabler for their use as access points is to
provide a sufficient throughput wireless backhaul link. Thus, in this paper we
consider the aspect of extension of network coverage with the use of
drone-based relaying and reconfigurable intelligent surfaces (RIS) for
backhauling. Presented results of simulation experiments indicate that the use
of RIS allows for significant improvement of network coverage without the need
to use additional relays.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in Polish language</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">17</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedPeWS: Personalized Warmup via Subnetworks for Enhanced Heterogeneous
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nurbek Tastan, Samuel Horvath, Martin Takac, Karthik Nandakumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Statistical data heterogeneity is a significant barrier to convergence in
federated learning (FL). While prior work has advanced heterogeneous FL through
better optimization objectives, these methods fall short when there is extreme
data heterogeneity among collaborating participants. We hypothesize that
convergence under extreme data heterogeneity is primarily hindered due to the
aggregation of conflicting updates from the participants in the initial
collaboration rounds. To overcome this problem, we propose a warmup phase where
each participant learns a personalized mask and updates only a subnetwork of
the full model. This personalized warmup allows the participants to focus
initially on learning specific subnetworks tailored to the heterogeneity of
their data. After the warmup phase, the participants revert to standard
federated optimization, where all parameters are communicated. We empirically
demonstrate that the proposed personalized warmup via subnetworks (FedPeWS)
approach improves accuracy and convergence speed over standard federated
optimization methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EinDecomp: Decomposition of Declaratively-Specified Machine Learning and
  Numerical Computations for Parallel Execution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Bourgeois, Zhimin Ding, Dimitrije Jankov, Jiehui Li, Mahmoud Sleem, Yuxin Tang, Jiawen Yao, Xinyu Yao, Chris Jermaine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of automatically decomposing operations over tensors
or arrays so that they can be executed in parallel on multiple devices. We
address two, closely-linked questions. First, what programming abstraction
should systems for tensor-based computing offer to enable such decompositions?
Second, given that abstraction, how should such systems automatically decompose
a tensor-based computation? We assert that tensor-based systems should offer a
programming abstraction based on an extended Einstein summation notation, which
is a fully declarative, mathematical specification for tensor computations. We
show that any computation specified in the Einstein summation notation can be
re-written into an equivalent tensor-relational computation, and this re-write
generalizes existing notations of tensor parallelism such as "data parallel''
and "model parallel.'' We consider the algorithmic problem of optimally
computing a tensor-relational decomposition of a graph of operations specified
in our extended Einstein summation notation, and we experimentally show the
value of the algorithm that we develop.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disaggregated Memory with SmartNIC Offloading: a Case Study on Graph
  Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Wahlgren, Gabin Schieffer, Maya Gokhale, Roger Pearce, Ivy Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Disaggregated memory breaks the boundary of monolithic servers to enable
memory provisioning on demand. Using network-attached memory to provide memory
expansion for memory-intensive applications on compute nodes can improve the
overall memory utilization on a cluster and reduce the total cost of ownership.
However, current software solutions for leveraging network-attached memory must
consume resources on the compute node for memory management tasks. Emerging
off-path smartNICs provide general-purpose programmability at low-cost
low-power cores. This work provides a general architecture design that enables
network-attached memory and offloading tasks onto off-path programmable
SmartNIC. We provide a prototype implementation called SODA on Nvidia BlueField
DPU. SODA adapts communication paths and data transfer alternatives, pipelines
data movement stages, and enables customizable data caching and prefetching
optimizations. We evaluate SODA in five representative graph applications on
real-world graphs. Our results show that SODA can achieve up to 7.9x speedup
compared to node-local SSD and reduce network traffic by 42% compared to
disaggregated memory without SmartNIC offloading at similar or better
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated k-Core Decomposition: A Secure Distributed Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Guo, Emil Sekerinski, Lingyang Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As one of the most well-studied cohesive subgraph models, the $k$-core is
widely used to find graph nodes that are ``central'' or ``important'' in many
applications, such as biological networks, social networks, ecological
networks, and financial networks. For distributed networks, e.g., Decentralized
Online Social Networks (DOSNs) such that each vertex is a client as a single
computing unit, the distributed $k$-core decomposition algorithms are already
proposed. However, current distributed approaches fail to adequately protect
privacy and security. In today's data-driven world, data privacy and security
have attracted more and more attention, e.g., DOSNs are proposed to protect
privacy by storing user information locally without using a single centralized
server. In this work, we are the first to propose the secure version of the
distributed $k$-core decomposition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fair Decentralized Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayan Biswas, Anne-Marie Kermarrec, Rishi Sharma, Thibaud Trinca, Martijn de Vos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decentralized learning (DL) is an emerging approach that enables nodes to
collaboratively train a machine learning model without sharing raw data. In
many application domains, such as healthcare, this approach faces challenges
due to the high level of heterogeneity in the training data's feature space.
Such feature heterogeneity lowers model utility and negatively impacts
fairness, particularly for nodes with under-represented training data. In this
paper, we introduce \textsc{Facade}, a clustering-based DL algorithm
specifically designed for fair model training when the training data exhibits
several distinct features. The challenge of \textsc{Facade} is to assign nodes
to clusters, one for each feature, based on the similarity in the features of
their local data, without requiring individual nodes to know apriori which
cluster they belong to. \textsc{Facade} (1) dynamically assigns nodes to their
appropriate clusters over time, and (2) enables nodes to collaboratively train
a specialized model for each cluster in a fully decentralized manner. We
theoretically prove the convergence of \textsc{Facade}, implement our
algorithm, and compare it against three state-of-the-art baselines. Our
experimental results on three datasets demonstrate the superiority of our
approach in terms of model accuracy and fairness compared to all three
competitors. Compared to the best-performing baseline, \textsc{Facade} on the
CIFAR-10 dataset also reduces communication costs by 32.3\% to reach a target
accuracy when cluster sizes are imbalanced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Edge-Computing based Industrial Gateway for Industry 4.0 using ARM
  TrustZone Technology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandeep Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Secure and efficient communication to establish a seamless nexus between the
five levels of a typical automation pyramid is paramount to Industry 4.0.
Specifically, vertical and horizontal integration of these levels is an
overarching requirement to accelerate productivity and improve operational
activities. Vertical integration can improve visibility, flexibility, and
productivity by connecting systems and applications. Horizontal integration can
provide better collaboration and adaptability by connecting internal production
facilities, multi-site operations, and third-party partners in a supply chain.
In this paper, we propose an Edge-computing-based Industrial Gateway for
interfacing information technology and operational technology that can enable
Industry 4.0 vertical and horizontal integration. Subsequently, we design and
develop a working prototype to demonstrate a remote production-line maintenance
use case with a strong focus on security aspects and the edge paradigm to bring
computational resources and data storage closer to data sources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Predictive Coding for Gradient Compression in Distributed
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Edin, Zheng Chen, Michel Kieffer, Mikael Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a prediction-based gradient compression method for
distributed learning with event-triggered communication. Our goal is to reduce
the amount of information transmitted from the distributed agents to the
parameter server by exploiting temporal correlation in the local gradients. We
use a linear predictor that \textit{combines past gradients to form a
prediction of the current gradient}, with coefficients that are optimized by
solving a least-square problem. In each iteration, every agent transmits the
predictor coefficients to the server such that the predicted local gradient can
be computed. The difference between the true local gradient and the predicted
one, termed the \textit{prediction residual, is only transmitted when its norm
is above some threshold.} When this additional communication step is omitted,
the server uses the prediction as the estimated gradient. This proposed design
shows notable performance gains compared to existing methods in the literature,
achieving convergence with reduced communication costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, presented at the 60th Allerton conference on
  Communication, Control, and Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Federated Learning for Generative AI-Assisted Semantic
  Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Peng, Feibo Jiang, Li Dong, Kezhi Wang, Kun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic Communication (SC) focuses on transmitting only the semantic
information rather than the raw data. This approach offers an efficient
solution to the issue of spectrum resource utilization caused by the various
intelligent applications on Mobile Users (MUs). Generative Artificial
Intelligence (GAI) models have recently exhibited remarkable content generation
and signal processing capabilities, presenting new opportunities for enhancing
SC. Therefore, we propose a GAI-assisted SC (GSC) model deployed between MUs
and the Base Station (BS). Then, to train the GSC model using the local data of
MUs while ensuring privacy and accommodating heterogeneous requirements of MUs,
we introduce Personalized Semantic Federated Learning (PSFL). This approach
incorporates a novel Personalized Local Distillation (PLD) and Adaptive Global
Pruning (AGP). In PLD, each MU selects a personalized GSC model as a mentor
tailored to its local resources and a unified Convolutional Neural Networks
(CNN)-based SC (CSC) model as a student. This mentor model is then distilled
into the student model for global aggregation. In AGP, we perform network
pruning on the aggregated global model according to real-time communication
environments, reducing communication energy. Finally, numerical results
demonstrate the feasibility and efficiency of the proposed PSFL scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-Pilot: Characterize and Optimize Performance of your LLM Inference
  Services <span class="chip">SC '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Małgorzata Łazuka, Andreea Anghel, Thomas Parnell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) are rapidly growing in popularity, LLM
inference services must be able to serve requests from thousands of users while
satisfying performance requirements. The performance of an LLM inference
service is largely determined by the hardware onto which it is deployed, but
understanding of which hardware will deliver on performance requirements
remains challenging. In this work we present LLM-Pilot - a first-of-its-kind
system for characterizing and predicting performance of LLM inference services.
LLM-Pilot performs benchmarking of LLM inference services, under a realistic
workload, across a variety of GPUs, and optimizes the service configuration for
each considered GPU to maximize performance. Finally, using this
characterization data, LLM-Pilot learns a predictive model, which can be used
to recommend the most cost-effective hardware for a previously unseen LLM.
Compared to existing methods, LLM-Pilot can deliver on performance requirements
33% more frequently, whilst reducing costs by 60% on average.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the International Conference for High Performance
  Computing, Networking, Storage and Analysis (SC '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selection Guidelines for Geographical SMR Protocols: A Communication
  Pattern-based Latency Modeling Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kohya Shiozaki, Junya Nakamura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State machine replication (SMR) is a replication technique that ensures fault
tolerance by duplicating a service. Geographical SMR can enhance its robustness
against disasters by distributing replicas in separate geographical locations.
Several geographical SMR protocols have been proposed in the literature, each
of which tailored to specific requirements; for example, protocols designed to
meet the requirement of latency reduction by either sacrificing a part of their
fault tolerance or limiting the content of responses to clients. However, this
diversity complicates the decision-making process for selecting the best
protocol for a particular service. In this study, we introduce a latency
estimation model for these SMR protocols based on the communication patterns of
the protocols and perform simulations for various cases. Based on the
simulation results and an experimental evaluation, we present five selection
guidelines for geographical SMR protocols based on their log management policy,
distances between replicas, number of replicas, frequency of slow paths, and
client distribution. These selection guidelines enable determining the best
geographical SMR protocol for each situation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was submitted to the 26th International Symposium on
  Stabilization, Safety, and Security of Distributed Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MTDNS: Moving Target Defense for Resilient DNS Infrastructure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdullah Aydeger, Pei Zhou, Sanzida Hoque, Marco Carvalho, Engin Zeydan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most critical components of the Internet that an attacker could
exploit is the DNS (Domain Name System) protocol and infrastructure.
Researchers have been constantly developing methods to detect and defend
against the attacks against DNS, specifically DNS flooding attacks. However,
most solutions discard packets for defensive approaches, which can cause
legitimate packets to be dropped, making them highly dependable on detection
strategies. In this paper, we propose MTDNS, a resilient MTD-based approach
that employs Moving Target Defense techniques through Software Defined
Networking (SDN) switches to redirect traffic to alternate DNS servers that are
dynamically created and run under the Network Function Virtualization (NFV)
framework. The proposed approach is implemented in a testbed environment by
running our DNS servers as separate Virtual Network Functions, NFV Manager, SDN
switches, and an SDN Controller. The experimental result shows that the MTDNS
approach achieves a much higher success rate in resolving DNS queries and
significantly reduces average latency even if there is a DNS flooding attack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, Accepted for publication at IEEE CCNC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extracting the Potential of Emerging Hardware Accelerators for Symmetric
  Eigenvalue Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansheng Wang, Lu Shi, Zhekai duan, Panruo Wu, Liwei Guo, Shaoshuai Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benefiting from the advancement of hardware accelerators such as GPUs, deep
neural networks and scientific computing applications can achieve superior
performance. Recently, the computing capacity of emerging hardware accelerators
has increased rapidly, while memory bandwidth has not kept pace with this
growth. This disparity exacerbates the gap between computing and memory,
leading to inefficiencies on conventional algorithms, as they're likely to be
converted from compute-bound to memory-bound. Symmetric eigenvalue
decomposition (EVD), a critical operation in various research domains including
scientific computing, deep learning training, and inference algorithms,
exhibits suboptimal performance due to achieving less than 3\% hardware
computing utilization on the H100 GPU. In this paper, we analyze the features
of emerging hardware accelerators to identify the bottlenecks inherent in
conventional EVD algorithms. To improve EVD performance, we propose several
algorithmic optimizations aimed at solving the memory-bound problem and
providing a better utilization of the rich computing capacity and parallelism
on the emerging hardware accelerators. Experimentally, our proposed method
demonstrates significant speedups on tridiagonalization, which is the main
workload that takes over 90\% elapsed time of EVD, compared to the SOTA
cuSOLVER tridiagonalization, achieving up to 10.1x, 7.5x, and 2.3x improvements
on H100, A100, and RTX 4090 GPUs, respectively. And the end-to-end the
performance of EVD solver is also up to 4.1x faster than cuSOVLER.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preble: Efficient Distributed <span class="highlight-title">Prompt</span> Scheduling for LLM Serving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00023v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00023v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vikranth Srivatsa, Zijian He, Reyna Abhyankar, Dongming Li, Yiying Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompts to large language models (LLMs) have evolved beyond simple user
questions. For LLMs to solve complex problems, today's practices are to include
domain-specific instructions, illustration of tool usages, and/or long context
such as textbook chapters in prompts. As such, many parts of prompts are
repetitive across requests. Recent works propose to cache and reuse KV state of
prompts. However, they are all confined to a single-GPU optimization, while
production LLM serving systems are distributed by nature.
  This paper proposes Preble, the first distributed LLM serving platform that
targets and optimizes for prompt sharing. We designed a distributed scheduling
system that co-optimizes KV state reuse and computation load-balancing with a
new scheduling algorithm and a hierarchical scheduling mechanism. Our
evaluation of Preble with real workloads and request arrival patterns on two
open-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X
to 14.5X on average latency and 2X to 10X on p99 latency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TURNIP: A "Nondeterministic" GPU Runtime with CPU RAM Offload 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16283v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16283v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhimin Ding, Jiawen Yao, Brianna Barrow, Tania Lorido Botran, Christopher Jermaine, Yuxin Tang, Jiehui Li, Xinyu Yao, Sleem Mahmoud Abdelghafar, Daniel Bourgeois
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An obvious way to alleviate memory difficulties in GPU-based AI computing is
via CPU offload, where data are moved between GPU and CPU RAM, so inexpensive
CPU RAM is used to increase the amount of storage available. While CPU offload
is an obvious idea, it can greatly slow down a computation, due to the
relatively slow transfer rate between CPU RAM and GPU RAM. Thus, any system for
CPU offload needs to ensure that when such a transfer needs to happen, no
computation is blocked waiting for the transfer to finish. One of the key
challenges when using CPU offload is that memory transfers introduce
nondeterminacy into the system: it is not possible to know before runtime when
the transfers will finish, and hence what is the best order of operations to
run to ensure there is no blocking. In this paper, we describe TURNIP, which is
a system for running AI computations using CPU offload. The key innovation in
TURNIP is the compilation of the AI computation into a dependency graph that
gives the TURNIP runtime freedom to run operations such as GPU kernel calls in
many different orders; at runtime, TURNIP chooses the best order in response to
real-time events.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Matrix Multiplications for Lookup Table-Quantized LLMs <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10960v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10960v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Guo, William Brandon, Radostin Cholakov, Jonathan Ragan-Kelley, Eric P. Xing, Yoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of large language models (LLMs) is often constrained by memory
bandwidth, where the primary bottleneck is the cost of transferring model
parameters from the GPU's global memory to its registers. When coupled with
custom kernels that fuse the dequantization and matmul operations, weight-only
quantization can thus enable faster inference by reducing the amount of memory
movement. However, developing high-performance kernels for weight-quantized
LLMs presents substantial challenges, especially when the weights are
compressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,
lookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup
table engine for LUT-quantized LLMs, which uses offline restructuring of the
quantized weight matrix to minimize bit manipulations associated with
unpacking, and vectorization and duplication of the lookup table to mitigate
shared memory bandwidth constraints. At batch sizes < 32 and quantization group
size of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster
than existing GEMM kernels. As an application of FLUTE, we explore a simple
extension to lookup table-based NormalFloat quantization and apply it to
quantize LLaMA3 to various configurations, obtaining competitive quantization
performance against strong baselines while obtaining an end-to-end throughput
increase of 1.5 to 2 times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Effective Heterogeneous Federated Learning via Efficient
  Hypernetwork-based Weight Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujin Shin, Kichang Lee, Sungmin Lee, You Rim Choi, Hyung-Sin Kim, JeongGil Ko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While federated learning leverages distributed client resources, it faces
challenges due to heterogeneous client capabilities. This necessitates
allocating models suited to clients' resources and careful parameter
aggregation to accommodate this heterogeneity. We propose HypeMeFed, a novel
federated learning framework for supporting client heterogeneity by combining a
multi-exit network architecture with hypernetwork-based model weight
generation. This approach aligns the feature spaces of heterogeneous model
layers and resolves per-layer information disparity during weight aggregation.
To practically realize HypeMeFed, we also propose a low-rank factorization
approach to minimize computation and memory overhead associated with
hypernetworks. Our evaluations on a real-world heterogeneous device testbed
indicate that \system enhances accuracy by 5.12% over FedAvg, reduces the
hypernetwork memory requirements by 98.22%, and accelerates its operations by
1.86x compared to a naive hypernetwork approach. These results demonstrate
HypeMeFed's effectiveness in leveraging and engaging heterogeneous clients for
federated learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stalactite: Toolbox for Fast Prototyping of Vertical Federated Learning
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15558v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15558v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasiia Zakharova, Dmitriy Alexandrov, Maria Khodorchenko, Nikolay Butakov, Alexey Vasilev, Maxim Savchenko, Alexander Grigorievskiy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) models trained on datasets owned by different
organizations and physically located in remote databases offer benefits in many
real-world use cases. State regulations or business requirements often prevent
data transfer to a central location, making it difficult to utilize standard
machine learning algorithms. Federated Learning (FL) is a technique that
enables models to learn from distributed datasets without revealing the
original data. Vertical Federated learning (VFL) is a type of FL where data
samples are divided by features across several data owners. For instance, in a
recommendation task, a user can interact with various sets of items, and the
logs of these interactions are stored by different organizations. In this demo
paper, we present \emph{Stalactite} - an open-source framework for VFL that
provides the necessary functionality for building prototypes of VFL systems. It
has several advantages over the existing frameworks. In particular, it allows
researchers to focus on the algorithmic side rather than engineering and to
easily deploy learning in a distributed environment. It implements several VFL
algorithms and has a built-in homomorphic encryption layer. We demonstrate its
use on a real-world recommendation datasets.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-02T00:00:00Z">2024-10-02</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Software Engineering <span class="chip" style="font-size: 60%">17</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of Generative AI on Collaborative Open-Source Software
  Development: Evidence from GitHub Copilot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangchen Song, Ashish Agarwal, Wen Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative artificial intelligence (AI) has opened the possibility of
automated content production, including coding in software development, which
can significantly influence the participation and performance of software
developers. To explore this impact, we investigate the role of GitHub Copilot,
a generative AI pair programmer, on software development in open-source
community, where multiple developers voluntarily collaborate on software
projects. Using GitHub's dataset for open-source repositories and a generalized
synthetic control method, we find that Copilot significantly enhances
project-level productivity by 6.5%. Delving deeper, we dissect the key
mechanisms driving this improvement. Our findings reveal a 5.5% increase in
individual productivity and a 5.4% increase in participation. However, this is
accompanied with a 41.6% increase in integration time, potentially due to
higher coordination costs. Interestingly, we also observe the differential
effects among developers. We discover that core developers achieve greater
project-level productivity gains from using Copilot, benefiting more in terms
of individual productivity and participation compared to peripheral developers,
plausibly due to their deeper familiarity with software projects. We also find
that the increase in project-level productivity is accompanied with no change
in code quality. We conclude that AI pair programmers bring benefits to
developers to automate and augment their code, but human developers' knowledge
of software projects can enhance the benefits. In summary, our research
underscores the role of AI pair programmers in impacting project-level
productivity within the open-source community and suggests potential
implications for the structure of open-source software projects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QuickCheck for VDM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Battle, Markus Solecki Ellyton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe recent work on a lightweight verification tool for VDM
specifications, called QuickCheck. The objective of the tool is to quickly
categorise proof obligations: identifying those that fail with counterexamples,
those that are probably provable and those that require deeper analysis. The
paper discusses the design of the tool and its use of pluggable strategies for
adding extra checking. We present the results of the tool being used to check a
large set of VDM specifications, and suggest future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 1 figure, submitted to the 22nd Overture Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding
  Capabilities of CodeLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dung Nguyen Manh, Thang Phan Chau, Nam Le Hai, Thong T. Doan, Nam V. Nguyen, Quang Pham, Nghi D. Q. Bui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Code Large Language Models (CodeLLMs) have
predominantly focused on open-ended code generation tasks, often neglecting the
critical aspect of code understanding and comprehension. To bridge this gap, we
present CodeMMLU, a comprehensive multiple-choice question-answer benchmark
designed to evaluate the depth of software and code understanding in LLMs.
CodeMMLU includes over 10,000 questions sourced from diverse domains,
encompassing tasks such as code analysis, defect detection, and software
engineering principles across multiple programming languages. Unlike
traditional benchmarks, CodeMMLU assesses models's ability to reason about code
rather than merely generate it, providing deeper insights into their grasp of
complex software concepts and systems. Our extensive evaluation reveals that
even state-of-the-art models face significant challenges with CodeMMLU,
highlighting deficiencies in comprehension beyond code generation. By
underscoring the crucial relationship between code understanding and effective
generation, CodeMMLU serves as a vital resource for advancing AI-assisted
software development, ultimately aiming to create more reliable and capable
coding assistants.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The potential of LLM-generated reports in DevSecOps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Lykousas, Vasileios Argyropoulos, Fran Casino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alert fatigue is a common issue faced by software teams using the DevSecOps
paradigm. The overwhelming number of warnings and alerts generated by security
and code scanning tools, particularly in smaller teams where resources are
limited, leads to desensitization and diminished responsiveness to security
warnings, potentially exposing systems to vulnerabilities. This paper explores
the potential of LLMs in generating actionable security reports that emphasize
the financial impact and consequences of detected security issues, such as
credential leaks, if they remain unaddressed. A survey conducted among
developers indicates that LLM-generated reports significantly enhance the
likelihood of immediate action on security issues by providing clear,
comprehensive, and motivating insights. Integrating these reports into
DevSecOps workflows can mitigate attention saturation and alert fatigue,
ensuring that critical security warnings are addressed effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in AIESE 2024 (International Conference on AI empowered
  Software Engineering)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing LLM Fine-tuning for Text-to-SQLs by SQL Quality Measurement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shouvon Sarker, Xishuang Dong, Xiangfang Li, Lijun Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQLs enables non-expert users to effortlessly retrieve desired
information from relational databases using natural language queries. While
recent advancements, particularly with Large Language Models (LLMs) like GPT
and T5, have shown impressive performance on large-scale benchmarks such as
BIRD, current state-of-the-art (SOTA) LLM-based Text-to-SQLs models often
require significant efforts to develop auxiliary tools like SQL classifiers to
achieve high performance. This paper proposed a novel approach that only needs
SQL Quality Measurement to enhance LLMs-based Text-to-SQLs performance. It
establishes a SQL quality evaluation mechanism to assess the generated SQL
queries against predefined criteria and actual database responses. This
feedback loop enables continuous learning and refinement of model outputs based
on both syntactic correctness and semantic accuracy. The proposed method
undergoes comprehensive validation on the BIRD benchmark, assessing Execution
Accuracy (EX) and Valid Efficiency Score (VES) across various Text-to-SQLs
difficulty levels. Experimental results reveal competitive performance in both
EX and VES compared to SOTA models like GPT4 and T5.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LightSC: The Making of a Usable Security Classification Tool for
  DevSecOps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manish Shrestha, Christian Johansen, Johanna Johansen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DevSecOps, as the extension of DevOps with security training and tools, has
become a popular way of developing modern software, especially in the Internet
of Things arena, due to its focus on rapid development, with short release
cycles, involving the user/client very closely. Security classification
methods, on the other hand, are heavy and slow processes that require high
expertise in security, the same as in other similar areas such as risk analysis
or certification. As such, security classification methods are hardly
compatible with the DevSecOps culture, which to the contrary, has moved away
from the traditional style of penetration testing done only when the software
product is in the final stages or already deployed.
  In this work, we first propose five principles for a security classification
to be \emph{DevOps-ready}, two of which will be the focus for the rest of the
paper, namely to be tool-based and easy to use for non-security experts, such
as ordinary developers or system architects. We then exemplify how one can make
a security classification methodology DevOps-ready. We do this through an
interaction design process, where we create and evaluate the usability of a
tool implementing the chosen methodology. Since such work seems to be new
within the usable security community, and even more so in the software
development (DevOps) community, we extract from our process a general,
three-steps `recipe' that others can follow when making their own security
methodologies DevOps-ready. The tool that we build is in itself a contribution
of this process, as it can be independently used, extended, and/or integrated
by developer teams into their DevSecOps tool-chains. Our tool is perceived (by
the test subjects) as most useful in the design phase, but also during the
testing phase where the security class would be one of the metrics used to
evaluate the quality of their software.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages of which 7 are appendix with figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of the COVID-19 Pandemic on Women's Contribution to Public
  Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annalí Casanueva, Davide Rossi, Stefano Zacchiroli, Théo Zimmermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite its promise of openness and inclusiveness, the development of free
and open source software (FOSS) remains significantly unbalanced in terms of
gender representation among contributors. To assist open source project
maintainers and communities in addressing this imbalance, it is crucial to
understand the causes of this inequality.In this study, we aim to establish how
the COVID-19 pandemic has influenced the ability of women to contribute to
public code. To do so, we use the Software Heritage archive, which holds the
largest dataset of commits to public code, and the difference in differences
(DID) methodology from econometrics that enables the derivation of causality
from historical data.Our findings show that the COVID-19 pandemic has
disproportionately impacted women's ability to contribute to the development of
public code, relatively to men. Further, our observations of specific
contributor subgroups indicate that COVID-19 particularly affected women
hobbyists, identified using contribution patterns and email address domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Empirical Software Engineering, In press</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QCRMut: Quantum Circuit Random Mutant generator tool 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sinhué García Gil, Luis Llana Díaz, José Ignacio Requeno Jarabo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum computing has been on the rise in recent years, evidenced by a surge
in publications on quantum software engineering and testing. Progress in
quantum hardware has also been notable, with the introduction of impressive
systems like Condor boasting 1121 qubits, and IBM Quantum System Two, which
employs three 133-qubit Heron processors. As this technology edges closer to
practical application, ensuring the efficacy of our software becomes
imperative. Mutation testing, a well-established technique in classical
computing, emerges as a valuable approach in this context.
  In our paper, we aim to introduce QCRMut, a mutation tool tailored for
quantum programs, leveraging the inherent Quantum Circuit structure. We propose
a randomised approach compared to previous works with exhaustive creation
processes and the capability for marking immutable positions within the
circuit. These features facilitate the preservation of program structure, which
is crucial for future applications such as metamorphic testing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Codev-Bench: How Do LLMs Understand Developer-Centric Code Completion? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Pan, Rongyu Cao, Yongchang Cao, Yingwei Ma, Binhua Li, Fei Huang, Han Liu, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code completion, a key downstream task in code generation, is one of the most
frequent and impactful methods for enhancing developer productivity in software
development. As intelligent completion tools evolve, we need a robust
evaluation benchmark that enables meaningful comparisons between products and
guides future advancements. However, existing benchmarks focus more on
coarse-grained tasks without industrial analysis resembling general code
generation rather than the real-world scenarios developers encounter. Moreover,
these benchmarks often rely on costly and time-consuming human annotation, and
the standalone test cases fail to leverage minimal tests for maximum
repository-level understanding and code coverage. To address these limitations,
we first analyze business data from an industrial code completion tool and
redefine the evaluation criteria to better align with the developer's intent
and desired completion behavior throughout the coding process. Based on these
insights, we introduce Codev-Agent, an agent-based system that automates
repository crawling, constructs execution environments, extracts dynamic
calling chains from existing unit tests, and generates new test samples to
avoid data leakage, ensuring fair and effective comparisons. Using Codev-Agent,
we present the Code-Development Benchmark (Codev-Bench), a fine-grained,
real-world, repository-level, and developer-centric evaluation framework.
Codev-Bench assesses whether a code completion tool can capture a developer's
immediate intent and suggest appropriate code across diverse contexts,
providing a more realistic benchmark for code completion in modern software
development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CodeBenchGen: Creating Scalable Execution-based Code Generation
  Benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00566v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00566v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqing Xie, Alex Xie, Divyanshu Sheth, Pengfei Liu, Daniel Fried, Carolyn Rose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To adequately test modern code generation systems, evaluation benchmarks must
execute and test the code generated by the system. However, these execution and
testing requirements have largely limited benchmarks to settings where code is
easily executable or has human-written tests. To facilitate evaluation of code
generation systems across diverse scenarios, we present CodeBenchGen, a
framework to create scalable execution-based benchmarks from naturally
occurring code sources. Specifically, we leverage a large language model (LLM)
to sandbox arbitrary pieces of code into evaluation examples, including test
cases for execution-based evaluation. We illustrate the usefulness of our
framework by creating a dataset, Exec-CSN, which includes 1,931 examples
involving 293 libraries converted from code in 367 GitHub repositories taken
from the Code- SearchNet dataset. To demonstrate the solvability of examples in
Exec-CSN, we present a human study demonstrating that 81.3% of the examples can
be solved by humans and 61% are rated as "requires effort to solve". We conduct
code generation experiments on open-source and proprietary models and analyze
the performance of both humans and models. We provide code and data at:
https://github.com/yiqingxyq/CodeBenchGen.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ScenicNL: Generating Probabilistic Scenario Programs from Natural
  Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03709v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03709v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karim Elmaaroufi, Devan Shanker, Ana Cismaru, Marcell Vazquez-Chanlatte, Alberto Sangiovanni-Vincentelli, Matei Zaharia, Sanjit A. Seshia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For cyber-physical systems (CPS), including robotics and autonomous vehicles,
mass deployment has been hindered by fatal errors that occur when operating in
rare events. To replicate rare events such as vehicle crashes, many companies
have created logging systems and employed crash reconstruction experts to
meticulously recreate these valuable events in simulation. However, in these
methods, "what if" questions are not easily formulated and answered. We present
ScenarioNL, an AI System for creating scenario programs from natural language.
Specifically, we generate these programs from police crash reports. Reports
normally contain uncertainty about the exact details of the incidents which we
represent through a Probabilistic Programming Language (PPL), Scenic. By using
Scenic, we can clearly and concisely represent uncertainty and variation over
CPS behaviors, properties, and interactions. We demonstrate how commonplace
prompting techniques with the best Large Language Models (LLM) are incapable of
reasoning about probabilistic scenario programs and generating code for
low-resource languages such as Scenic. Our system is comprised of several LLMs
chained together with several kinds of prompting strategies, a compiler, and a
simulator. We evaluate our system on publicly available autonomous vehicle
crash reports in California from the last five years and share insights into
how we generate code that is both semantically meaningful and syntactically
correct.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 3 figures. Published at COLM 2024.
  https://ke7.github.io/ScenicNL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-Based Test-Driven Interactive Code Generation: User Study and
  Empirical Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarah Fakhoury, Aaditya Naik, Georgios Sakkas, Saikat Chakraborty, Shuvendu K. Lahiri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown great potential in automating
significant aspects of coding by producing natural code from informal natural
language (NL) intent. However, given NL is informal, it does not lend easily to
checking that the generated code correctly satisfies the user intent. In this
paper, we propose a novel interactive workflow TiCoder for guided intent
clarification (i.e., partial formalization) through tests to support the
generation of more accurate code suggestions. Through a mixed methods user
study with 15 programmers, we present an empirical evaluation of the
effectiveness of the workflow to improve code generation accuracy. We find that
participants using the proposed workflow are significantly more likely to
correctly evaluate AI generated code, and report significantly less
task-induced cognitive load. Furthermore, we test the potential of the workflow
at scale with four different state-of-the-art LLMs on two python datasets,
using an idealized proxy for a user feedback. We observe an average absolute
improvement of 45.97% in the pass@1 code generation accuracy for both datasets
and across all LLMs within 5 user interactions, in addition to the automatic
generation of accompanying unit tests.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Software Engineering, vol. 50, no. 09, pp.
  2254-2268, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Model Openness Framework: Promoting Completeness and Openness for
  Reproducibility, Transparency, and Usability in Artificial Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13784v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13784v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matt White, Ibrahim Haddad, Cailean Osborne, Xiao-Yang Liu Yanglet, Ahmed Abdelmonsef, Sachin Varghese
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI (GAI) offers numerous opportunities for research and
innovation, but its commercialization has raised concerns about transparency,
reproducibility, and safety. Most open GAI models lack the necessary components
for full understanding, auditing, and reproducibility, and some use restrictive
licenses whilst claiming to be "open-source". To address these concerns, we
introduce the Model Openness Framework (MOF), a ranked classification system
that rates machine learning models based on their completeness and openness,
following principles of open science, as well as the Model Openness Tool (MOT),
which provides a reference implementation designed to evaluate ML models
against the principles outlined by the MOF. The MOF requires specific
components of the model development lifecycle to be included and released under
appropriate open licenses. This framework aims to prevent misrepresentation of
models claiming to be open, to guide researchers and developers in providing
all model components under permissive licenses, and to help individuals and
organizations identify models that can be safely adopted. By promoting
transparency and reproducibility, the MOF combats open-washing and establishes
completeness and openness as core tenets of responsible AI research and
development. Widespread adoption of the MOF will foster a more open AI
ecosystem, benefiting research, innovation, and the adoption of
state-of-the-art models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LogicAsker: Evaluating and Improving the Logical Reasoning Ability of
  Large Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00757v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00757v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Wan, Wenxuan Wang, Yiliu Yang, Youliang Yuan, Jen-tse Huang, Pinjia He, Wenxiang Jiao, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LogicAsker, a novel approach for evaluating and enhancing the
logical reasoning capabilities of large language models (LLMs) such as ChatGPT
and GPT-4. Despite LLMs' prowess in tasks like writing assistance, code
generation, and machine translation, assessing their ability to reason has been
challenging. Traditional evaluations often prioritize accuracy on downstream
tasks over direct assessments of reasoning processes. LogicAsker addresses this
gap by employing a set of atomic reasoning skills grounded in propositional and
predicate logic to systematically examine and improve the reasoning prowess of
LLMs. Our methodology reveals significant gaps in LLMs' learning of logical
rules, with identified reasoning failures ranging from 29\% to 90\% across
different models. Moreover, we leverage these findings to construct targeted
demonstration examples and fine-tune data, notably enhancing logical reasoning
in models like GPT-4o by up to 5\%. To our knowledge, this is the first effort
to utilize test case outcomes to effectively refine LLMs' formal reasoning
capabilities. We make our code, data, and results publicly available
(https://github.com/yxwan123/LogicAsker) to facilitate further research and
replication of our findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CodeGRAG: Bridging the Gap between Natural Language and Programming
  Language via Graphical Retrieval Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kounianhua Du, Jizheng Chen, Renting Rui, Huacan Chai, Lingyue Fu, Wei Xia, Yasheng Wang, Ruiming Tang, Yong Yu, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing large language models to generate codes has shown promising meaning
in software development revolution. Despite the intelligence shown by the
general large language models, their specificity in code generation can still
be improved due to the syntactic gap and mismatched vocabulary existing among
natural language and different programming languages. In this paper, we propose
CodeGRAG, a Graphical Retrieval Augmented Code Generation framework to enhance
the performance of LLMs. CodeGRAG builds the graphical view of code blocks
based on the control flow and data flow of them to fill the gap between
programming languages and natural language, which can facilitate natural
language based LLMs for better understanding of code syntax and serve as a
bridge among different programming languages. To take the extracted structural
knowledge into the foundation models, we propose 1) a hard meta-graph prompt
template to transform the challenging graphical representation into informative
knowledge for tuning-free models and 2) a soft prompting technique that injects
the domain knowledge of programming languages into the model parameters via
finetuning the models with the help of a pretrained GNN expert model. Various
experiments and ablations are done on four datasets including both the C++ and
python languages to validate the hard meta-graph prompt, the soft prompting
technique, and the effectiveness of the objectives for pretrained GNN expert.
CodeGRAG improves the code generation ability of LLMs and can even offer
performance gain for cross-lingual code generation. The implementation is
available at https://anonymous.4open.science/r/Code-5970/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Synthesis of Green Architectural Tactics for ML-Enabled Systems <span class="chip">ICSE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09610v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09610v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heli Järvenpää, Patricia Lago, Justus Bogner, Grace Lewis, Henry Muccini, Ipek Ozkaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid adoption of artificial intelligence (AI) and machine learning (ML)
has generated growing interest in understanding their environmental impact and
the challenges associated with designing environmentally friendly ML-enabled
systems. While Green AI research, i.e., research that tries to minimize the
energy footprint of AI, is receiving increasing attention, very few concrete
guidelines are available on how ML-enabled systems can be designed to be more
environmentally sustainable. In this paper, we provide a catalog of 30 green
architectural tactics for ML-enabled systems to fill this gap. An architectural
tactic is a high-level design technique to improve software quality, in our
case environmental sustainability. We derived the tactics from the analysis of
51 peer-reviewed publications that primarily explore Green AI, and validated
them using a focus group approach with three experts. The 30 tactics we
identified are aimed to serve as an initial reference guide for further
exploration into Green AI from a software engineering perspective, and assist
in designing sustainable ML-enabled systems. To enhance transparency and
facilitate their widespread use and extension, we make the tactics available
online in easily consumable formats. Wide-spread adoption of these tactics has
the potential to substantially reduce the societal impact of ML-enabled systems
regarding their energy and carbon footprint.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 2024 International Conference on
  Software Engineering - Software Engineering in Society (ICSE-SEIS'2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Creation of Representative Samples of Software Repositories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00639v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00639v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        June Gorostidi, Adem Ait, Jordi Cabot, Javier Luis Cánovas Izquierdo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software repositories is one of the sources of data in Empirical Software
Engineering, primarily in the Mining Software Repositories field, aimed at
extracting knowledge from the dynamics and practice of software projects. With
the emergence of social coding platforms such as GitHub, researchers have now
access to millions of software repositories to use as source data for their
studies. With this massive amount of data, sampling techniques are needed to
create more manageable datasets. The creation of these datasets is a crucial
step, and researchers have to carefully select the repositories to create
representative samples according to a set of variables of interest. However,
current sampling methods are often based on random selection or rely on
variables which may not be related to the research study (e.g., popularity or
activity). In this paper, we present a methodology for creating representative
samples of software repositories, where such representativeness is properly
aligned with both the characteristics of the population of repositories and the
requirements of the empirical study. We illustrate our approach with use cases
based on Hugging Face repositories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted for publication in the Proceedings of the
  18th International Symposium on Empirical Software Engineering and
  Measurement (ESEM 2024)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">33</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of Generative AI on Collaborative Open-Source Software
  Development: Evidence from GitHub Copilot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangchen Song, Ashish Agarwal, Wen Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative artificial intelligence (AI) has opened the possibility of
automated content production, including coding in software development, which
can significantly influence the participation and performance of software
developers. To explore this impact, we investigate the role of GitHub Copilot,
a generative AI pair programmer, on software development in open-source
community, where multiple developers voluntarily collaborate on software
projects. Using GitHub's dataset for open-source repositories and a generalized
synthetic control method, we find that Copilot significantly enhances
project-level productivity by 6.5%. Delving deeper, we dissect the key
mechanisms driving this improvement. Our findings reveal a 5.5% increase in
individual productivity and a 5.4% increase in participation. However, this is
accompanied with a 41.6% increase in integration time, potentially due to
higher coordination costs. Interestingly, we also observe the differential
effects among developers. We discover that core developers achieve greater
project-level productivity gains from using Copilot, benefiting more in terms
of individual productivity and participation compared to peripheral developers,
plausibly due to their deeper familiarity with software projects. We also find
that the increase in project-level productivity is accompanied with no change
in code quality. We conclude that AI pair programmers bring benefits to
developers to automate and augment their code, but human developers' knowledge
of software projects can enhance the benefits. In summary, our research
underscores the role of AI pair programmers in impacting project-level
productivity within the open-source community and suggests potential
implications for the structure of open-source software projects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SeeSay: An Assistive Device for the Visually Impaired Using Retrieval
  Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melody Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present SeeSay, an assistive device designed for
individuals with visual impairments. This system leverages large language
models (LLMs) for speech recognition and visual querying. It effectively
identifies, records, and responds to the user's environment by providing audio
guidance using retrieval-augmented generation (RAG). Our experiments
demonstrate the system's capability to recognize its surroundings and respond
to queries with audio feedback in diverse settings. We hope that the SeeSay
system will facilitate users' comprehension and recollection of their
surroundings, thereby enhancing their environmental perception, improving
navigational capabilities, and boosting overall independence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing Criteria Development Across Domain Experts, Lay Users, and
  Models in Large Language Model Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annalisa Szymanski, Simret Araya Gebreegziabher, Oghenemaro Anuyah, Ronald A. Metoyer, Toby Jia-Jun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly utilized for domain-specific
tasks, yet integrating domain expertise into evaluating their outputs remains
challenging. A common approach to evaluating LLMs is to use metrics, or
criteria, which are assertions used to assess performance that help ensure that
their outputs align with domain-specific standards. Previous efforts have
involved developers, lay users, or the LLMs themselves in creating these
criteria, however, evaluation particularly from a domain expertise perspective,
remains understudied. This study explores how domain experts contribute to LLM
evaluation by comparing their criteria with those generated by LLMs and lay
users. We further investigate how the criteria-setting process evolves,
analyzing changes between a priori and a posteriori stages. Our findings
emphasize the importance of involving domain experts early in the evaluation
process while utilizing complementary strengths of lay users and LLMs. We
suggest implications for designing workflows that leverage these strengths at
different evaluation stages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digital Eyes: Social Implications of XR EyeSight 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maurizio Vergari, Tanja Kojić, Wafaa Wardah, Maximilian Warsinke, Sebastian Möller, Jan-Niklas Voigt-Antons, Robert P. Spang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The EyeSight feature, introduced with the new Apple Vision Pro XR headset,
promises to revolutionize user interaction by simulating real human eye
expressions on a digital display. This feature could enhance XR devices' social
acceptability and social presence when communicating with others outside the XR
experience. In this pilot study, we explore the implications of the EyeSight
feature by examining social acceptability, social presence, emotional
responses, and technology acceptance. Eight participants engaged in
conversational tasks in three conditions to contrast experiencing the Apple
Vision Pro with EyeSight, the Meta Quest 3 as a reference XR headset, and a
face-to-face setting. Our preliminary findings indicate that while the EyeSight
feature improves perceptions of social presence and acceptability compared to
the reference headsets, it does not match the social connectivity of direct
human interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In 30th ACM Symposium on Virtual Reality Software and Technology
  (VRST 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SkyAI Sim: An Open-Source Simulation of UAV Aerial Imaging from
  Satellite Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. Parisa Dajkhosh, Peter M. Le, Orges Furxhi, Eddie L. Jacobs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capturing real-world aerial images for vision-based navigation (VBN) is
challenging due to limited availability and conditions that make it nearly
impossible to access all desired images from any location. The complexity
increases when multiple locations are involved. The state of the art solutions,
such as flying a UAV (Unmanned Aerial Vehicle) to take pictures or using
existing research databases, have significant limitations. SkyAI Sim offers a
compelling alternative by simulating a UAV to capture bird's-eye view satellite
images at zero-yaw with real-world visible-band specifications. This
open-source tool allows users to specify the bounding box (top-left and
bottom-right) coordinates of any region on a map. Without the need to
physically fly a drone, the virtual Python UAV performs a raster search to
capture satellite images using the Google Maps Static API. Users can define
parameters such as flight altitude, aspect ratio and diagonal field of view of
the camera, and the overlap between consecutive images. SkyAI Sim's
capabilities range from capturing a few low-altitude images for basic
applications to generating extensive datasets of entire cities for complex
tasks like deep learning. This versatility makes SkyAI a valuable tool for not
only VBN, but also other applications including environmental monitoring,
construction, and city management. The open-source nature of the tool also
allows for extending the raster search to other missions. A dataset of Memphis,
TN has been provided along with this simulator, partially generated using SkyAI
and, also includes data from a 3D world generation package for comparison.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DreamGarden: A Designer Assistant for Growing Games from a Single <span class="highlight-title">Prompt</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Earle, Samyak Parajuli, Andrzej Banburski-Fahey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coding assistants are increasingly leveraged in game design, both generating
code and making high-level plans. To what degree can these tools align with
developer workflows, and what new modes of human-computer interaction can
emerge from their use? We present DreamGarden, an AI system capable of
assisting with the development of diverse game environments in Unreal Engine.
At the core of our method is an LLM-driven planner, capable of breaking down a
single, high-level prompt -- a dream, memory, or imagined scenario provided by
a human user -- into a hierarchical action plan, which is then distributed
across specialized submodules facilitating concrete implementation. This system
is presented to the user as a garden of plans and actions, both growing
independently and responding to user intervention via seed prompts, pruning,
and feedback. Through a user study, we explore design implications of this
system, charting courses for future work in semi-autonomous assistants and
open-ended simulation design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages + appendix, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LightSC: The Making of a Usable Security Classification Tool for
  DevSecOps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manish Shrestha, Christian Johansen, Johanna Johansen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DevSecOps, as the extension of DevOps with security training and tools, has
become a popular way of developing modern software, especially in the Internet
of Things arena, due to its focus on rapid development, with short release
cycles, involving the user/client very closely. Security classification
methods, on the other hand, are heavy and slow processes that require high
expertise in security, the same as in other similar areas such as risk analysis
or certification. As such, security classification methods are hardly
compatible with the DevSecOps culture, which to the contrary, has moved away
from the traditional style of penetration testing done only when the software
product is in the final stages or already deployed.
  In this work, we first propose five principles for a security classification
to be \emph{DevOps-ready}, two of which will be the focus for the rest of the
paper, namely to be tool-based and easy to use for non-security experts, such
as ordinary developers or system architects. We then exemplify how one can make
a security classification methodology DevOps-ready. We do this through an
interaction design process, where we create and evaluate the usability of a
tool implementing the chosen methodology. Since such work seems to be new
within the usable security community, and even more so in the software
development (DevOps) community, we extract from our process a general,
three-steps `recipe' that others can follow when making their own security
methodologies DevOps-ready. The tool that we build is in itself a contribution
of this process, as it can be independently used, extended, and/or integrated
by developer teams into their DevSecOps tool-chains. Our tool is perceived (by
the test subjects) as most useful in the design phase, but also during the
testing phase where the security class would be one of the metrics used to
evaluate the quality of their software.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages of which 7 are appendix with figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Customizing Generated Signs and Voices of AI Avatars: Deaf-Centric
  Mixed-Reality Design for Deaf-Hearing Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Si Chen, Haocong Cheng, Suzy Su, Stephanie Patterson, Raja Kushalnagar, Qi Wang, Yun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates innovative interaction designs for communication and
collaborative learning between learners of mixed hearing and signing abilities,
leveraging advancements in mixed reality technologies like Apple Vision Pro and
generative AI for animated avatars. Adopting a participatory design approach,
we engaged 15 d/Deaf and hard of hearing (DHH) students to brainstorm ideas for
an AI avatar with interpreting ability (sign language to English, voice to
English) that would facilitate their face-to-face communication with hearing
peers. Participants envisioned the AI avatars to address some issues with human
interpreters, such as lack of availability, and provide affordable options to
expensive personalized interpreting service. Our findings indicate a range of
preferences for integrating the AI avatars with actual human figures of both
DHH and hearing communication partners. The participants highlighted the
importance of having control over customizing the AI avatar, such as
AI-generated signs, voices, facial expressions, and their synchronization for
enhanced emotional display in communication. Based on our findings, we propose
a suite of design recommendations that balance respecting sign language norms
with adherence to hearing social norms. Our study offers insights on improving
the authenticity of generative AI in scenarios involving specific, and
sometimes unfamiliar, social norms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Avatar Appearance and Behavior of Potential Harassers Affect Users'
  Perceptions and Response Strategies in Social Virtual Reality (VR): A
  Mixed-Methods Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuetong Wang, Ziyan Wang, Mingmin Zhang, Kangyou Yu, Pan Hui, Mingming Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sexual harassment has been recognized as a significant social issue. In
recent years, the emergence of harassment in social virtual reality (VR) has
become an important and urgent research topic. We employed a mixed-methods
approach by conducting online surveys with VR users (N = 166) and
semi-structured interviews with social VR users (N = 18) to investigate how
users perceive sexual harassment in social VR, focusing on the influence of
avatar appearance. Moreover, we derived users' response strategies to sexual
harassment and gained insights on platform regulation. This study contributes
to the research on sexual harassment in social VR by examining the moderating
effect of avatar appearance on user perception of sexual harassment and
uncovering the underlying reasons behind response strategies. Moreover, it
presents novel prospects and challenges in platform design and regulation
domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ACE: A LLM-based Negotiation Coaching System <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Shea, Aymen Kallala, Xin Lucy Liu, Michael W. Morris, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing prominence of LLMs has led to an increase in the development of
AI tutoring systems. These systems are crucial in providing underrepresented
populations with improved access to valuable education. One important area of
education that is unavailable to many learners is strategic bargaining related
to negotiation. To address this, we develop a LLM-based Assistant for Coaching
nEgotiation (ACE). ACE not only serves as a negotiation partner for users but
also provides them with targeted feedback for improvement. To build our system,
we collect a dataset of negotiation transcripts between MBA students. These
transcripts come from trained negotiators and emulate realistic bargaining
scenarios. We use the dataset, along with expert consultations, to design an
annotation scheme for detecting negotiation mistakes. ACE employs this scheme
to identify mistakes and provide targeted feedback to users. To test the
effectiveness of ACE-generated feedback, we conducted a user experiment with
two consecutive trials of negotiation and found that it improves negotiation
performances significantly compared to a system that doesn't provide feedback
and one which uses an alternative method of providing feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angela Lopez-Cardona, Carlos Segura, Alexandros Karatzoglou, Sergi Abadal, Ioannis Arapakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in Natural Language Processing (NLP), have led to the emergence
of Large Language Models (LLMs) such as GPT, Llama, Claude, and Gemini, which
excel across a range of tasks but require extensive fine-tuning to align their
outputs with human expectations. A widely used method for achieving this
alignment is Reinforcement Learning from Human Feedback (RLHF), which, despite
its success, faces challenges in accurately modelling human preferences. In
this paper, we introduce GazeReward, a novel framework that integrates implicit
feedback -- and specifically eye-tracking (ET) data -- into the Reward Model
(RM). In addition, we explore how ET-based features can provide insights into
user preferences. Through ablation studies we test our framework with different
integration methods, LLMs, and ET generator models, demonstrating that our
approach significantly improves the accuracy of the RM on established human
preference datasets. This work advances the ongoing discussion on optimizing AI
alignment with human values, exploring the potential of cognitive data for
shaping future NLP research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Getting in the Door: Streamlining Intake in Civil Legal Services with
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quinten Steenhuis, Hannes Westermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal intake, the process of finding out if an applicant is eligible for help
from a free legal aid program, takes significant time and resources. In part
this is because eligibility criteria are nuanced, open-textured, and require
frequent revision as grants start and end. In this paper, we investigate the
use of large language models (LLMs) to reduce this burden. We describe a
digital intake platform that combines logical rules with LLMs to offer
eligibility recommendations, and we evaluate the ability of 8 different LLMs to
perform this task. We find promising results for this approach to help close
the access to justice gap, with the best model reaching an F1 score of .82,
while minimizing false negatives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-vocabulary Multimodal Emotion Recognition: <span class="highlight-title">Dataset</span>, Metric, and
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Lian, Haiyang Sun, Licai Sun, Lan Chen, Haoyu Chen, Hao Gu, Zhuofan Wen, Shun Chen, Siyuan Zhang, Hailiang Yao, Mingyu Xu, Kang Chen, Bin Liu, Rui Liu, Shan Liang, Ya Li, Jiangyan Yi, Jianhua Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Emotion Recognition (MER) is an important research topic. This
paper advocates for a transformative paradigm in MER. The rationale behind our
work is that current approaches often rely on a limited set of basic emotion
labels, which do not adequately represent the rich spectrum of human emotions.
These traditional and overly simplistic emotion categories fail to capture the
inherent complexity and subtlety of human emotional experiences, leading to
limited generalizability and practicality. Therefore, we propose a new MER
paradigm called Open-vocabulary MER (OV-MER), which encompasses a broader range
of emotion labels to reflect the richness of human emotions. This paradigm
relaxes the label space, allowing for the prediction of arbitrary numbers and
categories of emotions. To support this transition, we provide a comprehensive
solution that includes a newly constructed database based on LLM and human
collaborative annotations, along with corresponding metrics and a series of
benchmarks. We hope this work advances emotion recognition from basic emotions
to more nuanced emotions, contributing to the development of emotional AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Smart Chair for Health Monitoring in Daily Life 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nguyen Thi Minh Huong, Vo Quoc Bao, Nguyen Trung Hau, Huynh Quang Linh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has focused on the risks associated with poor sitting posture
and the impact of sitting on biological parameters, such as heart rate because
prolonged sitting is common across all ages and professions. In this work, we
propose a novel approach that can display simultaneously posture and heart rate
in real-time. In this device, pressure sensors are embedded into a flexible
separate cushion easily put on any chair to provide sitting behaviours and a
smartwatch-like PPG module is worn on the user's wrist. Regarding posture
classification, pressure figures of ten pressure sensors under the seat bottom
are inputs of four machine learning models, giving a high accuracy of 99 per
cent. Besides, the Electrocardiography recording module is illustrated with the
same results as a commercial device called DFRobot. Another advantage of this
smart chair is that it not only simultaneously displays both sitting postures
and heart rates on external devices like laptops, mobile phones, or televisions
through microcontrollers but also offers the relationship between them to help
people adjust their sitting behaviours, avoiding influencing heart rate. The
smart chair is expected to be useful equipment for people with a sedentary
lifestyle, especially office workers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can We Delegate Learning to Automation?: A Comparative Study of LLM
  Chatbots, Search Engines, and Books 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeonsun Yang, Ahyeon Shin, Mincheol Kang, Jiheon Kang, Jean Young Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning is a key motivator behind information search behavior. With the
emergence of LLM-based chatbots, students are increasingly turning to these
tools as their primary resource for acquiring knowledge. However, the
transition from traditional resources like textbooks and web searches raises
concerns among educators. They worry that these fully-automated LLMs might lead
students to delegate critical steps of search as learning. In this paper, we
systematically uncover three main concerns from educators' perspectives. In
response to these concerns, we conducted a mixed-methods study with 92
university students to compare three learning sources with different automation
levels. Our results show that LLMs support comprehensive understanding of key
concepts without promoting passive learning, though their effectiveness in
knowledge retention was limited. Additionally, we found that academic
performance impacted both learning outcomes and search patterns. Notably,
higher-competence learners engaged more deeply with content through
reading-intensive behaviors rather than relying on search activities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intelligent CAD 2.0 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Zou, Yincai Wu, Zhenyu Liu, Weiwei Xu, Shuming Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating modern artificial intelligence (AI) techniques, particularly
generative AI, holds the promise of revolutionizing computer-aided design (CAD)
tools and the engineering design process. However, the direction of "AI+CAD"
remains unclear: how will the current generation of intelligent CAD (ICAD)
differ from its predecessor in the 1980s and 1990s, what strategic pathways
should researchers and engineers pursue for its implementation, and what
potential technical challenges might arise?
  As an attempt to address these questions, this paper investigates the
transformative role of modern AI techniques in advancing CAD towards ICAD. It
first analyzes the design process and reconsiders the roles AI techniques can
assume in this process, highlighting how they can restructure the path humans,
computers, and designs interact with each other. The primary conclusion is that
ICAD systems should assume an intensional rather than extensional role in the
design process. This offers insights into the evaluation of the previous
generation of ICAD (ICAD 1.0) and outlines a prospective framework and
trajectory for the next generation of ICAD (ICAD 2.0).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published in the journal of Visual Informatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSLens: Towards Better Deploying Charging Stations via Visual Analytics
  -- A Coupled Networks Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutian Zhang, Liwen Xu, Shaocong Tao, Quanxue Guan, Quan Li, Haipeng Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the global adoption of electric vehicles (EVs) has surged,
prompting a corresponding rise in the installation of charging stations. This
proliferation has underscored the importance of expediting the deployment of
charging infrastructure. Both academia and industry have thus devoted to
addressing the charging station location problem (CSLP) to streamline this
process. However, prevailing algorithms addressing CSLP are hampered by
restrictive assumptions and computational overhead, leading to a dearth of
comprehensive evaluations in the spatiotemporal dimensions. Consequently, their
practical viability is restricted. Moreover, the placement of charging stations
exerts a significant impact on both the road network and the power grid, which
necessitates the evaluation of the potential post-deployment impacts on these
interconnected networks holistically. In this study, we propose CSLens, a
visual analytics system designed to inform charging station deployment
decisions through the lens of coupled transportation and power networks. CSLens
offers multiple visualizations and interactive features, empowering users to
delve into the existing charging station layout, explore alternative deployment
solutions, and assess the ensuring impact. To validate the efficacy of CSLens,
we conducted two case studies and engaged in interviews with domain experts.
Through these efforts, we substantiated the usability and practical utility of
CSLens in enhancing the decision-making process surrounding charging station
deployment. Our findings underscore CSLens's potential to serve as a valuable
asset in navigating the complexities of charging infrastructure planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures; Accepted by IEEE IEEE Transactions on
  Visualization and Computer Graphics, 2024 (TVCG)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MARLens: Understanding Multi-agent Reinforcement Learning for Traffic
  Signal Control via Visual Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutian Zhang, Guohong Zheng, Zhiyuan Liu, Quan Li, Haipeng Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The issue of traffic congestion poses a significant obstacle to the
development of global cities. One promising solution to tackle this problem is
intelligent traffic signal control (TSC). Recently, TSC strategies leveraging
reinforcement learning (RL) have garnered attention among researchers. However,
the evaluation of these models has primarily relied on fixed metrics like
reward and queue length. This limited evaluation approach provides only a
narrow view of the model's decision-making process, impeding its practical
implementation. Moreover, effective TSC necessitates coordinated actions across
multiple intersections. Existing visual analysis solutions fall short when
applied in multi-agent settings. In this study, we delve into the challenge of
interpretability in multi-agent reinforcement learning (MARL), particularly
within the context of TSC. We propose MARLens a visual analytics system
tailored to understand MARL-based TSC. Our system serves as a versatile
platform for both RL and TSC researchers. It empowers them to explore the
model's features from various perspectives, revealing its decision-making
processes and shedding light on interactions among different agents. To
facilitate quick identification of critical states, we have devised multiple
visualization views, complemented by a traffic simulation module that allows
users to replay specific training scenarios. To validate the utility of our
proposed system, we present three comprehensive case studies, incorporate
insights from domain experts through interviews, and conduct a user study.
These collective efforts underscore the feasibility and effectiveness of
MARLens in enhancing our understanding of MARL-based TSC systems and pave the
way for more informed and efficient traffic management strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 8 figures; Accepted by IEEE Transactions on Visualization
  and Computer Graphics, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Did I Say Again? Relating User Needs to Search Outcomes in
  Conversational Commerce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Schott, Andrea Papenmeier, Daniel Hienert, Dagmar Kern
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in natural language processing and deep learning have
accelerated the development of digital assistants. In conversational commerce,
these assistants help customers find suitable products in online shops through
natural language conversations. During the dialogue, the assistant identifies
the customer's needs and preferences and subsequently suggests potentially
relevant products. Traditional online shops often allow users to filter search
results based on their preferences using facets. Selected facets can also serve
as a reminder of how the product base was filtered. In conversational commerce,
however, the absence of facets and the use of advanced natural language
processing techniques can leave customers uncertain about how their input was
processed by the system. This can hinder transparency and trust, which are
critical factors influencing customers' purchase intentions. To address this
issue, we propose a novel text-based digital assistant that, in the product
assessment step, explains how specific product aspects relate to the user's
previous utterances to enhance transparency and facilitate informed
decision-making. We conducted a user study (N=135) and found a significant
increase in user-perceived transparency when natural language explanations and
highlighted text passages were provided, demonstrating their potential to
extend system transparency to the product assessment step in conversational
commerce.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic deductive coding in discourse analysis: an application of
  large language models in learning analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lishan Zhang, Han Wu, Xiaoshan Huang, Tengfei Duan, Hanxiang Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deductive coding is a common discourse analysis method widely used by
learning science and learning analytics researchers for understanding teaching
and learning interactions. It often requires researchers to manually label all
discourses to be analyzed according to a theoretically guided coding scheme,
which is time-consuming and labor-intensive. The emergence of large language
models such as GPT has opened a new avenue for automatic deductive coding to
overcome the limitations of traditional deductive coding. To evaluate the
usefulness of large language models in automatic deductive coding, we employed
three different classification methods driven by different artificial
intelligence technologies, including the traditional text classification method
with text feature engineering, BERT-like pretrained language model and GPT-like
pretrained large language model (LLM). We applied these methods to two
different datasets and explored the potential of GPT and prompt engineering in
automatic deductive coding. By analyzing and comparing the accuracy and Kappa
values of these three classification methods, we found that GPT with prompt
engineering outperformed the other two methods on both datasets with limited
number of training samples. By providing detailed prompt structures, the
reported work demonstrated how large language models can be used in the
implementation of automatic deductive coding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perceptions of Moderators as a Large-Scale Measure of Online Community
  Governance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16610v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16610v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Galen Weld, Leon Leibmann, Amy X. Zhang, Tim Althoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Millions of online communities are governed by volunteer moderators, who
shape their communities by setting and enforcing rules, recruiting additional
moderators, and participating in the community themselves. These moderators
must regularly make decisions about how to govern, yet measuring the 'success'
of governance is complex and nuanced, making it challenging to determine what
governance strategies are most successful. Furthermore, prior work has shown
that communities have differing values, suggesting that 'one-size-fits-all'
approaches to governance are unlikely to serve all communities well. In this
work, we assess governance practices on reddit by classifying the sentiment of
community members' public discussion of their own moderators. We label 1.89
million posts and comments made on reddit over an 18 month period. We relate
these perceptions to characteristics of community governance, and to different
actions that community moderators take. We identify types of communities where
moderators are perceived particularly positively and negatively, and highlight
promising strategies for moderator teams. Amongst other findings, we show that
strict rule enforcement is linked to more favorable perceptions of moderators
of communities dedicated to certain topics, such as news communities, than
others. We investigate what kinds of moderators are associated with improved
community perceptions upon their addition to a mod team, and find that
moderators who are active community members before and during their mod tenures
are seen more favorably. We make all our models, datasets, and code public.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-Based Hand Gesture Customization from a Single Demonstration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08420v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08420v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soroush Shahi, Vimal Mollyn, Cori Tymoszek Park, Richard Kang, Asaf Liberman, Oron Levy, Jun Gong, Abdelkareem Bedri, Gierad Laput
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hand gesture recognition is becoming a more prevalent mode of human-computer
interaction, especially as cameras proliferate across everyday devices. Despite
continued progress in this field, gesture customization is often underexplored.
Customization is crucial since it enables users to define and demonstrate
gestures that are more natural, memorable, and accessible. However,
customization requires efficient usage of user-provided data. We introduce a
method that enables users to easily design bespoke gestures with a monocular
camera from one demonstration. We employ transformers and meta-learning
techniques to address few-shot learning challenges. Unlike prior work, our
method supports any combination of one-handed, two-handed, static, and dynamic
gestures, including different viewpoints, and the ability to handle irrelevant
hand movements. We implement three real-world applications using our
customization method, conduct a user study, and achieve up to 94% average
recognition accuracy from one demonstration. Our work provides a viable path
for vision-based gesture customization, laying the foundation for future
advancements in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 (UIST' 24). USA, 14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ink and Individuality: Crafting a Personalised Narrative in the Age of
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00026v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00026v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azmine Toushik Wasi, Raima Islam, Mst Rafia Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Individuality and personalization comprise the distinctive characteristics
that make each writer unique and influence their words in order to effectively
engage readers while conveying authenticity. However, our growing reliance on
LLM-based writing assistants risks compromising our creativity and
individuality over time. We often overlook the negative impacts of this trend
on our creativity and uniqueness, despite the possible consequences. This study
investigates these concerns by performing a brief survey to explore different
perspectives and concepts, as well as trying to understand people's viewpoints,
in conjunction with past studies in the area. Addressing these issues is
essential for improving human-computer interaction systems and enhancing
writing assistants for personalization and individuality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 Pages, 4 Figures. Accepted in The Third Workshop on Intelligent and
  Interactive Writing Assistants at CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership
  and Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00027v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00027v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azmine Toushik Wasi, Mst Rafia Islam, Raima Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sense of ownership in writing confines our investment of thoughts, time, and
contribution, leading to attachment to the output. However, using writing
assistants introduces a mental dilemma, as some content isn't directly our
creation. For instance, we tend to credit Large Language Models (LLMs) more in
creative tasks, even though all tasks are equal for them. Additionally, while
we may not claim complete ownership of LLM-generated content, we freely claim
authorship. We conduct a short survey to examine these issues and understand
underlying cognitive processes in order to gain a better knowledge of
human-computer interaction in writing and improve writing aid systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 Pages, 3 Figures. Accepted in The Third Workshop on Intelligent and
  Interactive Writing Assistants at CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What is lost in Normalization? Exploring Pitfalls in Multilingual ASR
  Model Evaluations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02449v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02449v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kavya Manohar, Leena G Pillai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the pitfalls in evaluating multilingual automatic speech
recognition (ASR) models, with a particular focus on Indic language scripts. We
investigate the text normalization routine employed by leading ASR models,
including OpenAI Whisper, Meta's MMS, Seamless, and Assembly AI's Conformer,
and their unintended consequences on performance metrics. Our research reveals
that current text normalization practices, while aiming to standardize ASR
outputs for fair comparison, by removing inconsistencies such as variations in
spelling, punctuation, and special characters, are fundamentally flawed when
applied to Indic scripts. Through empirical analysis using text similarity
scores and in-depth linguistic examination, we demonstrate that these flaws
lead to artificially improved performance metrics for Indic languages. We
conclude by proposing a shift towards developing text normalization routines
that leverage native linguistic expertise, ensuring more robust and accurate
evaluations of multilingual ASR models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Social Conjuring: Multi-User Runtime Collaboration with AI in Building
  Virtual 3D Worlds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amina Kobenova, Cyan DeVeaux, Samyak Parajuli, Andrzej Banburski-Fahey, Judith Amores Fernandez, Jaron Lanier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative artificial intelligence has shown promise in prompting virtual
worlds into existence, yet little attention has been given to understanding how
this process unfolds as social interaction. We present Social Conjurer, a
framework for AI-augmented dynamic 3D scene co-creation, where multiple users
collaboratively build and modify virtual worlds in real-time. Through an
expanded set of interactions, including social and tool-based engagements as
well as spatial reasoning, our framework facilitates the creation of rich,
diverse virtual environments. Findings from a preliminary user study (N=12)
provide insight into the user experience of this approach, how social contexts
shape the prompting of spatial environments, and perspective on social
applications of prompt-based 3D co-creation. In addition to highlighting the
potential of AI-supported multi-user world creation and offering new pathways
for AI-augmented creative processes in VR, this article presents a set of
implications for designing human-centered interfaces that incorporate AI models
into 3D content generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages + Appendix, 16 figures; fixed some minor UTF-8 encoding
  issues in arXiv compilation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WatChat: Explaining perplexing programs by debugging mental models <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kartik Chandra, Katherine M. Collins, Will Crichton, Tony Chen, Tzu-Mao Li, Adrian Weller, Rachit Nigam, Joshua Tenenbaum, Jonathan Ragan-Kelley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Often, a good explanation for a program's unexpected behavior is a bug in the
programmer's code. But sometimes, an even better explanation is a bug in the
programmer's mental model of the language or API they are using. Instead of
merely debugging our current code ("giving the programmer a fish"), what if our
tools could directly debug our mental models ("teaching the programmer to
fish")? In this paper, we apply recent ideas from computational cognitive
science to offer a principled framework for doing exactly that. Given a "why?"
question about a program, we automatically infer potential misconceptions about
the language/API that might cause the user to be surprised by the program's
behavior -- and then analyze those misconceptions to provide explanations of
the program's behavior. Our key idea is to formally represent misconceptions as
counterfactual (erroneous) semantics for the language/API, which can be
inferred and debugged using program synthesis techniques. We demonstrate our
framework, WatChat, by building systems for explanation in two domains:
JavaScript type coercion, and the Git version control system. We evaluate
WatChatJS and WatChatGit by comparing their outputs to experimentally-collected
human-written explanations in these two domains: we show that WatChat's
explanations exhibit key features of human-written explanation, unlike those of
a state-of-the-art language model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a preprint of work presented in early-stage non-archival form
  at the ACL Natural Language Reasoning and Structured Explanations Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Web3 and the State: Indian state's redescription of blockchain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00320v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00320v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debarun Sarkar, Cheshta Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The article closely reads a discussion paper by the National Institution for
Transforming India (NITI) Aayog and a strategy paper by the Ministry of
Electronics and Information Technology (MeitY) advocating non-financial use
cases of blockchain in India. By noting the discursive shift from transparency
to trust to adjustably transparent enacted in these two documents, and
consequently the Indian state's redescription of blockchain, the paper
foregrounds how blockchain systems are being designated as "decentral" but have
recentralizing effects where the state reinvents and re-establishes itself as
an intermediary. The paper illustrates how discursive shifts concerning trust,
transparency, (de)centralization and (dis)intermediation are crucial sites for
investigating redescriptions of emerging sociotechnical systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cost-Effective Online Multi-LLM Selection with Versatile Reward Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangxiang Dai, Jin Li, Xutong Liu, Anqi Yu, John C. S. Lui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of large language models (LLMs), the diversity of
multi-LLM tasks and the variability in their pricing structures have become
increasingly important, as costs can vary greatly between different LLMs. To
tackle these challenges, we introduce the \textit{C2MAB-V}, a
\underline{C}ost-effective \underline{C}ombinatorial \underline{M}ulti-armed
\underline{B}andit with \underline{V}ersatile reward models for optimal LLM
selection and usage. This online model differs from traditional static
approaches or those reliant on a single LLM without cost consideration. With
multiple LLMs deployed on a scheduling cloud and a local server dedicated to
handling user queries, \textit{C2MAB-V} facilitates the selection of multiple
LLMs over a combinatorial search space, specifically tailored for various
collaborative task types with different reward models. Based on our designed
online feedback mechanism and confidence bound technique, \textit{C2MAB-V} can
effectively address the multi-LLM selection challenge by managing the
exploration-exploitation trade-off across different models, while also
balancing cost and reward for diverse tasks. The NP-hard integer linear
programming problem for selecting multiple LLMs with trade-off dilemmas is
addressed by: i) decomposing the integer problem into a relaxed form by the
local server, ii) utilizing a discretization rounding scheme that provides
optimal LLM combinations by the scheduling cloud, and iii) continual online
updates based on feedback. Theoretically, we prove that \textit{C2MAB-V} offers
strict guarantees over versatile reward models, matching state-of-the-art
results for regret and violations in some degenerate cases. Empirically, we
show that \textit{C2MAB-V} effectively balances performance and cost-efficiency
with nine LLMs for three application scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 14 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Show Me What's Wrong!: Combining Charts and Text to Guide Data Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beatriz Feliciano, Rita Costa, Jean Alves, Javier Liébana, Diogo Duarte, Pedro Bizarro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing and finding anomalies in multi-dimensional datasets is a cumbersome
but vital task across different domains. In the context of financial fraud
detection, analysts must quickly identify suspicious activity among
transactional data. This is an iterative process made of complex exploratory
tasks such as recognizing patterns, grouping, and comparing. To mitigate the
information overload inherent to these steps, we present a tool combining
automated information highlights, Large Language Model generated textual
insights, and visual analytics, facilitating exploration at different levels of
detail. We perform a segmentation of the data per analysis area and visually
represent each one, making use of automated visual cues to signal which require
more attention. Upon user selection of an area, our system provides textual and
graphical summaries. The text, acting as a link between the high-level and
detailed views of the chosen segment, allows for a quick understanding of
relevant details. A thorough exploration of the data comprising the selection
can be done through graphical representations. The feedback gathered in a study
performed with seven domain experts suggests our tool effectively supports and
guides exploratory analysis, easing the identification of suspicious
information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UI-JEPA: Towards Active Perception of User Intent through Onscreen User
  Activity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04081v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04081v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yicheng Fu, Raviteja Anantha, Prabal Vashisht, Jianpeng Cheng, Etai Littwin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating user intent from a sequence of user interface (UI) actions is a
core challenge in comprehensive UI understanding. Recent advancements in
multimodal large language models (MLLMs) have led to substantial progress in
this area, but their demands for extensive model parameters, computing power,
and high latency makes them impractical for scenarios requiring lightweight,
on-device solutions with low latency or heightened privacy. Additionally, the
lack of high-quality datasets has hindered the development of such lightweight
models. To address these challenges, we propose UI-JEPA, a novel framework that
employs masking strategies to learn abstract UI embeddings from unlabeled data
through self-supervised learning, combined with an LLM decoder fine-tuned for
user intent prediction. We also introduce two new UI-grounded multimodal
datasets, "Intent in the Wild" (IIW) and "Intent in the Tame" (IIT), designed
for few-shot and zero-shot UI understanding tasks. IIW consists of 1.7K videos
across 219 intent categories, while IIT contains 914 videos across 10
categories. We establish the first baselines for these datasets, showing that
representations learned using a JEPA-style objective, combined with an LLM
decoder, can achieve user intent predictions that match the performance of
state-of-the-art large MLLMs, but with significantly reduced annotation and
deployment resources. Measured by intent similarity scores, UI-JEPA outperforms
GPT-4 Turbo and Claude 3.5 Sonnet by 10.0% and 7.2% respectively, averaged
across two datasets. Notably, UI-JEPA accomplishes the performance with a 50.5x
reduction in computational cost and a 6.6x improvement in latency in the IIW
dataset. These results underscore the effectiveness of UI-JEPA, highlighting
its potential for lightweight, high-performance UI understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In Situ AI Prototyping: Infusing Multimodal <span class="highlight-title">Prompt</span>s into Mobile Settings
  with MobileMaker 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03806v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03806v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Savvas Petridis, Michael Xieyang Liu, Alexander J. Fiannaca, Vivian Tsai, Michael Terry, Carrie J. Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in multimodal large language models (LLMs) have made it
easier to rapidly prototype AI-powered features, especially for mobile use
cases. However, gathering early, mobile-situated user feedback on these AI
prototypes remains challenging. The broad scope and flexibility of LLMs means
that, for a given use-case-specific prototype, there is a crucial need to
understand the wide range of in-the-wild input users are likely to provide and
their in-context expectations for the AI's behavior. To explore the concept of
in situ AI prototyping and testing, we created MobileMaker: a platform that
enables designers to rapidly create and test mobile AI prototypes directly on
devices. This tool also enables testers to make on-device, in-the-field
revisions of prototypes using natural language. In an exploratory study with 16
participants, we explored how user feedback on prototypes created with
MobileMaker compares to that of existing prototyping tools (e.g., Figma, prompt
editors). Our findings suggest that MobileMaker prototypes enabled more
serendipitous discovery of: model input edge cases, discrepancies between AI's
and user's in-context interpretation of the task, and contextual signals missed
by the AI. Furthermore, we learned that while the ability to make in-the-wild
revisions led users to feel more fulfilled as active participants in the design
process, it might also constrain their feedback to the subset of changes
perceived as more actionable or implementable by the prototyping tool.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models as Zero-Shot Human Models for Human-Robot
  Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Zhang, Harold Soh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human models play a crucial role in human-robot interaction (HRI), enabling
robots to consider the impact of their actions on people and plan their
behavior accordingly. However, crafting good human models is challenging;
capturing context-dependent human behavior requires significant prior knowledge
and/or large amounts of interaction data, both of which are difficult to
obtain. In this work, we explore the potential of large-language models (LLMs)
-- which have consumed vast amounts of human-generated text data -- to act as
zero-shot human models for HRI. Our experiments on three social datasets yield
promising results; the LLMs are able to achieve performance comparable to
purpose-built models. That said, we also discuss current limitations, such as
sensitivity to prompts and spatial/numerical reasoning mishaps. Based on our
findings, we demonstrate how LLM-based human models can be integrated into a
social robot's planning process and applied in HRI scenarios. Specifically, we
present one case study on a simulated trust-based table-clearing task and
replicate past results that relied on custom models. Next, we conduct a new
robot utensil-passing experiment (n = 65) where preliminary results show that
planning with a LLM-based human model can achieve gains over a basic myopic
plan. In summary, our results show that LLMs offer a promising (but incomplete)
approach to human modeling for HRI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Programming Languages <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Survey</span>ing the Rust Verification Landscape 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Le Blanc, Patrick Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rust aims to be a safe programming language applicable to systems programming
applications. In particular, its type system has strong guardrails to prevent a
variety of issues, such as memory safety bugs and data races. However, these
guardrails can be sidestepped via the unsafe keyword. unsafe allows certain
otherwise-prohibited operations, but shifts the onus of preventing undefined
behaviour from the Rust language's compile-time checks to the developer. We
believe that tools have a role to play in ensuring the absence of undefined
behaviour in the presence of unsafe code. Moreover, safety aside, programs
would also benefit from being verified for functional correctness, ensuring
that they meet their specifications.
  In this research proposal, we explore what it means to do Rust verification.
Specifically, we explore which properties are worth verifying for Rust; what
techniques exist to verify them; and which code is worth verifying. In doing
so, we motivate an effort to verify safety properties of the Rust standard
library, presenting the relevant challenges along with ideas to address them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SecCoder: Towards Generalizable and Robust Secure Code Generation <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyu Zhang, Tianyu Du, Junkai Tong, Xuhong Zhang, Kingsum Chow, Sheng Cheng, Xun Wang, Jianwei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  After large models (LMs) have gained widespread acceptance in code-related
tasks, their superior generative capacity has greatly promoted the application
of the code LM. Nevertheless, the security of the generated code has raised
attention to its potential damage. Existing secure code generation methods have
limited generalizability to unseen test cases and poor robustness against the
attacked model, leading to safety failures in code generation. In this paper,
we propose a generalizable and robust secure code generation method SecCoder by
using in-context learning (ICL) and the safe demonstration. The dense retriever
is also used to select the most helpful demonstration to maximize the
improvement of the generated code's security. Experimental results show the
superior generalizability of the proposed model SecCoder compared to the
current secure code generation method, achieving a significant security
improvement of an average of 7.20% on unseen test cases. The results also show
the better robustness of SecCoder compared to the current attacked code LM,
achieving a significant security improvement of an average of 7.74%. Our
analysis indicates that SecCoder enhances the security of LMs in generating
code, and it is more generalizable and robust.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To Appear in the 2024 Conference on Empirical Methods in Natural
  Language Processing (EMNLP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ScenicNL: Generating Probabilistic Scenario Programs from Natural
  Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03709v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03709v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karim Elmaaroufi, Devan Shanker, Ana Cismaru, Marcell Vazquez-Chanlatte, Alberto Sangiovanni-Vincentelli, Matei Zaharia, Sanjit A. Seshia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For cyber-physical systems (CPS), including robotics and autonomous vehicles,
mass deployment has been hindered by fatal errors that occur when operating in
rare events. To replicate rare events such as vehicle crashes, many companies
have created logging systems and employed crash reconstruction experts to
meticulously recreate these valuable events in simulation. However, in these
methods, "what if" questions are not easily formulated and answered. We present
ScenarioNL, an AI System for creating scenario programs from natural language.
Specifically, we generate these programs from police crash reports. Reports
normally contain uncertainty about the exact details of the incidents which we
represent through a Probabilistic Programming Language (PPL), Scenic. By using
Scenic, we can clearly and concisely represent uncertainty and variation over
CPS behaviors, properties, and interactions. We demonstrate how commonplace
prompting techniques with the best Large Language Models (LLM) are incapable of
reasoning about probabilistic scenario programs and generating code for
low-resource languages such as Scenic. Our system is comprised of several LLMs
chained together with several kinds of prompting strategies, a compiler, and a
simulator. We evaluate our system on publicly available autonomous vehicle
crash reports in California from the last five years and share insights into
how we generate code that is both semantically meaningful and syntactically
correct.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 3 figures. Published at COLM 2024.
  https://ke7.github.io/ScenicNL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WatChat: Explaining perplexing programs by debugging mental models <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kartik Chandra, Katherine M. Collins, Will Crichton, Tony Chen, Tzu-Mao Li, Adrian Weller, Rachit Nigam, Joshua Tenenbaum, Jonathan Ragan-Kelley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Often, a good explanation for a program's unexpected behavior is a bug in the
programmer's code. But sometimes, an even better explanation is a bug in the
programmer's mental model of the language or API they are using. Instead of
merely debugging our current code ("giving the programmer a fish"), what if our
tools could directly debug our mental models ("teaching the programmer to
fish")? In this paper, we apply recent ideas from computational cognitive
science to offer a principled framework for doing exactly that. Given a "why?"
question about a program, we automatically infer potential misconceptions about
the language/API that might cause the user to be surprised by the program's
behavior -- and then analyze those misconceptions to provide explanations of
the program's behavior. Our key idea is to formally represent misconceptions as
counterfactual (erroneous) semantics for the language/API, which can be
inferred and debugged using program synthesis techniques. We demonstrate our
framework, WatChat, by building systems for explanation in two domains:
JavaScript type coercion, and the Git version control system. We evaluate
WatChatJS and WatChatGit by comparing their outputs to experimentally-collected
human-written explanations in these two domains: we show that WatChat's
explanations exhibit key features of human-written explanation, unlike those of
a state-of-the-art language model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a preprint of work presented in early-stage non-archival form
  at the ACL Natural Language Reasoning and Structured Explanations Workshop</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Operating Systems <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global Scheduling of Weakly-Hard Real-Time Tasks using Job-Level
  Priority Classes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        V. Gabriel Moyano, Zain A. H. Hammadeh, Selma Saidi, Daniel Lüdtke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time systems are intrinsic components of many pivotal applications, such
as self-driving vehicles, aerospace and defense systems. The trend in these
applications is to incorporate multiple tasks onto fewer, more powerful
hardware platforms, e.g., multi-core systems, mainly for reducing cost and
power consumption. Many real-time tasks, like control tasks, can tolerate
occasional deadline misses due to robust algorithms. These tasks can be modeled
using the weakly-hard model. Literature shows that leveraging the weakly-hard
model can relax the over-provisioning associated with designed real-time
systems. However, a wide-range of the research focuses on single-core
platforms. Therefore, we strive to extend the state-of-the-art of scheduling
weakly-hard real-time tasks to multi-core platforms. We present a global
job-level fixed priority scheduling algorithm together with its schedulability
analysis. The scheduling algorithm leverages the tolerable continuous deadline
misses to assigning priorities to jobs. The proposed analysis extends the
Response Time Analysis (RTA) for global scheduling to test the schedulability
of tasks. Hence, our analysis scales with the number of tasks and number of
cores because, unlike literature, it depends neither on Integer Linear
Programming nor reachability trees. Schedulability analyses show that the
schedulability ratio is improved by 40% comparing to the global Rate Monotonic
(RM) scheduling and up to 60% more than the global EDF scheduling, which are
the state-of-the-art schedulers on the RTEMS real-time operating system. Our
evaluation on industrial embedded multi-core platform running RTEMS shows that
the scheduling overhead of our proposal does not exceed 60 Nanosecond.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Networking and Internet Architecture <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clid: Identifying TLS Clients With Unsupervised Learning on Domain Names 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ihyun Nam, Gerry Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Clid, a Transport Layer Security (TLS) client
identification tool based on unsupervised learning on domain names in the
server name indication (SNI) field. Clid aims to provide some information on a
wide range of clients, even though it may not be able to identify a definitive
characteristic about each one of the clients. This is a different approach from
that of many existing rule-based client identification tools that rely on
hardcoded databases to identify granular characteristics of a few clients.
Often times, these tools can identify only a small number of clients in a
real-world network as their databases grow outdated, which motivates an
alternative approach like Clid. For this research, we utilize some 345 million
anonymized TLS handshakes collected from a large university campus network.
From each handshake, we create a TCP fingerprint that identifies each unique
client that corresponds to a physical device on the network. Clid uses Bayesian
optimization to find the 'optimal' DBSCAN clustering of clients and domain
names for a set of TLS connections. Clid maps each client cluster to one or
more domain clusters that are most strongly associated with it based on the
frequency and exclusivity of their TLS connections. While learning highly
associated domain names of a client may not immediately tell us specific
characteristics of the client like its the operating system, manufacturer, or
TLS configuration, it may serve as a strong first step to doing so. We evaluate
Clid's performance on various subsets of our captured TLS handshakes and on
different parameter settings that affect the granularity of identification
results. Our experiments show that Clid is able to identify 'strongly
associated' domain names for at least 60% of all clients in all our
experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Resilience of Fast Failover Routing Against Dynamic Link Failures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenkai Dai, Klaus-Tycho Foerster, Stefan Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern communication networks feature local fast failover mechanisms in the
data plane, to swiftly respond to link failures with pre-installed rerouting
rules. This paper explores resilient routing meant to tolerate $\leq k$
simultaneous link failures, ensuring packet delivery, provided that the source
and destination remain connected. While past theoretical works studied failover
routing under static link failures, i.e., links which permanently and
simultaneously fail, real-world networks often face link flapping--dynamic down
states caused by, e.g., numerous short-lived software-related faults. Thus, in
this initial work, we re-investigate the resilience of failover routing against
link flapping, by categorizing link failures into static, semi-dynamic
(removing the assumption that links fail simultaneously), and dynamic (removing
the assumption that links fail permanently) types, shedding light on the
capabilities and limitations of failover routing under these scenarios.
  We show that $k$-edge-connected graphs exhibit $(k-1)$-resilient routing
against dynamic failures for $k \leq 5$. We further show that this result
extends to arbitrary $k$ if it is possible to rewrite $\log k$ bits in the
packet header.
  Rewriting $3$ bits suffices to cope with $k$ semi-dynamic failures. However,
on general graphs, tolerating $2$ dynamic failures becomes impossible without
bit-rewriting. Even by rewriting $\log k$ bits, resilient routing cannot
resolve $k$ dynamic failures, demonstrating the limitation of local fast
rerouting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decentralized Collaborative Inertial Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alpha Diallo, Benoit Garbinato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although people spend most of their time indoors, outdoor tracking systems,
such as the Global Positioning System (GPS), are predominantly used for
location-based services. These systems are accurate outdoors, easy to use, and
operate autonomously on each mobile device. In contrast, Indoor Tracking
Systems~(ITS) lack standardization and are often difficult to operate because
they require costly infrastructure. In this paper, we propose an indoor
tracking algorithm that uses collected data from inertial sensors embedded in
most mobile devices. In this setting, mobile devices autonomously estimate
their location, hence removing the burden of deploying and maintaining complex
and scattered hardware infrastructure. In addition, these devices collaborate
by anonymously exchanging data with other nearby devices, using wireless
communication, such as Bluetooth, to correct errors in their location
estimates. Our collaborative algorithm relies on low-complexity geometry
operations and can be deployed on any recent mobile device with
commercial-grade sensors. We evaluate our solution on real-life data collected
by different devices. Experimentation with 16 simultaneously moving and
collaborating devices shows an average accuracy improvement of 44% compared to
the standalone Pedestrian Dead Reckoning algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACCEPTED FOR PUBLICATION AND PRESENTED IN EAI MOBIQUITOUS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beamforming in Secure Integrated Sensing and Communication Systems with
  Antenna Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunxiang Shi, Lixin Li, Wensheng Lin, Wei Liang, Zhu Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider joint antenna allocation and transmit beamforming
design in secure integrated sensing and communication (ISAC) systems. A
dual-function base station (DFBS) aims to securely deliver messages to a
single-antenna receiver while detecting potential eavesdroppers. To prevent
eavesdropping, we incorporate specialized sensing signals, intentionally
reducing communication signal power toward suspicious targets to improve
sensing. We prioritize minimizing the matching error between the transmitting
and required beampatterns for sensing and communication. Our design optimizes
antenna allocation and beamforming at the DFBS, meeting minimum secrecy rate
and power constraints. We propose solvers based on alternating optimization for
the non-convex design problem. Simulations show that the antenna allocation
scheme significantly improves safety performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAFE: Semantic Adaptive Feature Extraction with Rate Control for 6G
  Wireless Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuna Yan, Lixin Li, Xin Zhang, Wensheng Lin, Wenchi Cheng, Zhu Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most current Deep Learning-based Semantic Communication (DeepSC) systems are
designed and trained exclusively for particular single-channel conditions,
which restricts their adaptability and overall bandwidth utilization. To
address this, we propose an innovative Semantic Adaptive Feature Extraction
(SAFE) framework, which significantly improves bandwidth efficiency by allowing
users to select different sub-semantic combinations based on their channel
conditions. This paper also introduces three advanced learning algorithms to
optimize the performance of SAFE framework as a whole. Through a series of
simulation experiments, we demonstrate that the SAFE framework can effectively
and adaptively extract and transmit semantics under different channel bandwidth
conditions, of which effectiveness is verified through objective and subjective
quality evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-Native Network Digital Twin for Intelligent Network Management in 6G 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Wu, Xinyu Huangm, Tom H. Luan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a pivotal virtualization technology, network digital twin is expected to
accurately reflect real-time status and abstract features in the on-going sixth
generation (6G) networks. In this article, we propose an artificial
intelligence (AI)-native network digital twin framework for 6G networks to
enable the synergy of AI and network digital twin, thereby facilitating
intelligent network management. In the proposed framework, AI models are
utilized to establish network digital twin models to facilitate network status
prediction, network pattern abstraction, and network management
decision-making. Furthermore, potential solutions are proposed for enhance the
performance of network digital twin. Finally, a case study is presented,
followed by a discussion of open research issues that are essential for
AI-native network digital twin in 6G networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article is submitted to IEEE Wireless Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Outage Probability Analysis for OTFS in Lossy Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Wensheng Lin, Lixin Li, Fucheng Yang, Zhu Han, Tad Matsumoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper analyzes the outage probability of orthogonal time frequency space
(OTFS) modulation under a lossy communication scenario. First of all, we
introduce the channel model and the vector form representation of OTFS this
paper uses. Then, we derive an exact expression of the OTFS outage probability
in lossy communication scenarios, using Shannon's lossy source-channel
separation theorem. Because the channel is time-varying, calculating the exact
outage probability is computationally expensive. Therefore, this paper aims to
derive a lower bound of the outage probability, which can relatively easily be
calculated. Thus, given the distortion requirement and number of the resolvable
paths, we can obtain a performance limit under the optimal condition as a
reference. Finally, the experimental results of outage probability are obtained
by Monte-Carlo method, and compared with the theoretical results calculated by
the closed-from expression of the lower bound.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weighted Sum Power Minimization for Cooperative Spectrum Sharing in
  Cognitive Radio Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This letter introduces weighted sum power (WSP), a new performance metric for
wireless resource allocation during cooperative spectrum sharing in cognitive
radio networks, where the primary and secondary nodes have different priorities
and quality of service (QoS) requirements. Compared to using energy efficiency
(EE) and weighted sum energy efficiency (WSEE) as performance metrics and
optimization objectives of wireless resource allocation towards green
communication, the linear character of WSP can reduce the complexity of
optimization problems. Meanwhile, the weights assigned to different nodes are
beneficial for managing their power budget. Using WSP as the optimization
objective, a suboptimal resource allocation scheme is proposed, leveraging
linear programming and Newton's method. Simulations verify that the proposed
scheme provides near-optimal performance with low computation time.
Furthermore, the initial approximate value selection in Newton's method is also
optimized to accelerate the proposed scheme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task-Oriented Edge-Assisted Cooperative Data Compression, Communications
  and Computing for UGV-Enhanced Warehouse Logistics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Yang, Zhen Meng, Xiangmin Xu, Kan Chen, Emma Liying Li, Philip Guodong G. Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Only the chairs can edit This paper explores the growing need for
task-oriented communications in warehouse logistics, where traditional
communication Key Performance Indicators (KPIs)-such as latency, reliability,
and throughput-often do not fully meet task requirements. As the complexity of
data flow management in large-scale device networks increases, there is also a
pressing need for innovative cross-system designs that balance data
compression, communication, and computation. To address these challenges, we
propose a task-oriented, edge-assisted framework for cooperative data
compression, communication, and computing in Unmanned Ground Vehicle
(UGV)-enhanced warehouse logistics. In this framework, two UGVs collaborate to
transport cargo, with control functions-navigation for the front UGV and
following/conveyance for the rear UGV-offloaded to the edge server to
accommodate their limited on-board computing resources. We develop a Deep
Reinforcement Learning (DRL)-based two-stage point cloud data compression
algorithm that dynamically and collaboratively adjusts compression ratios
according to task requirements, significantly reducing communication overhead.
System-level simulations of our UGV logistics prototype demonstrate the
framework's effectiveness and its potential for swift real-world
implementation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building a real-time physical layer labeled data logging facility for 6G
  research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Franco Minucci, Raquel Marina Noguera Oishi, Haoqiu Xiong, Dieter Verbruggen, Cel Thys, Rizqi Hersyandika, Robbert Beerten, Achiel Colpaert, Vida Ranjbar, Sofie Pollin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work describes the architecture and vision of designing and implementing
a new test infrastructure for 6G physical layer research at KU Leuven. The
Testbed is designed for physical layer research and experimentation following
several emerging trends, such as cell-free networking, integrated
communication, sensing, open disaggregated Radio Access Networks, AI-Native
design, and multiband operation. The software is almost entirely based on free
and open-source software, making contributing and reusing any component easy.
The open Testbed is designed to provide real-time and labeled data on all parts
of the physical layer, from raw IQ data to synchronization statistics, channel
state information, or symbol/bit/packet error rates. Real-time labeled datasets
can be collected by synchronizing the physical layer data logging with a
positioning and motion capture system. One of the main goals of the design is
to make it open and accessible to external users remotely. Most tests and data
captures can easily be automated, and experiment code can be remotely deployed
using standard containers (e.g., Docker or Podman). Finally, the paper
describes how the Testbed can be used for our research on joint communication
and sensing, over-the-air synchronization, distributed processing, and AI in
the loop.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asynchronous Federated Reinforcement Learning with Policy Gradient
  Updates: Algorithm Design and Convergence Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08003v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08003v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangchen Lan, Dong-Jun Han, Abolfazl Hashemi, Vaneet Aggarwal, Christopher G. Brinton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To improve the efficiency of reinforcement learning (RL), we propose a novel
asynchronous federated reinforcement learning (FedRL) framework termed AFedPG,
which constructs a global model through collaboration among $N$ agents using
policy gradient (PG) updates. To address the challenge of lagged policies in
asynchronous settings, we design a delay-adaptive lookahead technique
\textit{specifically for FedRL} that can effectively handle heterogeneous
arrival times of policy gradients. We analyze the theoretical global
convergence bound of AFedPG, and characterize the advantage of the proposed
algorithm in terms of both the sample complexity and time complexity.
Specifically, our AFedPG method achieves $O(\frac{{\epsilon}^{-2.5}}{N})$
sample complexity for global convergence at each agent on average. Compared to
the single agent setting with $O(\epsilon^{-2.5})$ sample complexity, it enjoys
a linear speedup with respect to the number of agents. Moreover, compared to
synchronous FedPG, AFedPG improves the time complexity from
$O(\frac{t_{\max}}{N})$ to $O({\sum_{i=1}^{N} \frac{1}{t_{i}}})^{-1}$, where
$t_{i}$ denotes the time consumption in each iteration at agent $i$, and
$t_{\max}$ is the largest one. The latter complexity $O({\sum_{i=1}^{N}
\frac{1}{t_{i}}})^{-1}$ is always smaller than the former one, and this
improvement becomes significant in large-scale federated settings with
heterogeneous computing powers ($t_{\max}\gg t_{\min}$). Finally, we
empirically verify the improved performance of AFedPG in four widely-used
MuJoCo environments with varying numbers of agents. We also demonstrate the
advantages of AFedPG in various computing heterogeneity scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The NetMob2024 <span class="highlight-title">Dataset</span>: Population Density and OD Matrices from Four
  LMIC Countries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00453v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00453v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlan Zhang, Miguel Nunez del Prado, Vincent Gauthier, Sveta Milusheva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The NetMob24 dataset offers a unique opportunity for researchers from a range
of academic fields to access comprehensive spatiotemporal data sets spanning
four countries (India, Mexico, Indonesia, and Colombia) over the course of two
years (2019 and 2020). This dataset, developed in collaboration with Cuebiq
(Also referred to as Spectus), comprises privacy-preserving aggregated data
sets derived from mobile application (app) data collected from users who have
voluntarily consented to anonymous data collection for research purposes. It is
our hope that this reference dataset will foster the production of new research
methods and the reproducibility of research outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimization of End-to-End AoI in Edge-Enabled Vehicular Fog Systems: A
  Dueling-DQN Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02815v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02815v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seifu Birhanu Tadele, Binayak Kar, Frezer Guteta Wakgra, Asif Uddin Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-time status update services for the Internet of Things (IoT), the
timely dissemination of information requiring timely updates is crucial to
maintaining its relevance. Failing to keep up with these updates results in
outdated information. The age of information (AoI) serves as a metric to
quantify the freshness of information. The Existing works to optimize AoI
primarily focus on the transmission time from the information source to the
monitor, neglecting the transmission time from the monitor to the destination.
This oversight significantly impacts information freshness and subsequently
affects decision-making accuracy. To address this gap, we designed an
edge-enabled vehicular fog system to lighten the computational burden on IoT
devices. We examined how information transmission and request-response times
influence end-to-end AoI. As a solution, we proposed Dueling-Deep Queue Network
(dueling-DQN), a deep reinforcement learning (DRL)-based algorithm and compared
its performance with DQN policy and analytical results. Our simulation results
demonstrate that the proposed dueling-DQN algorithm outperforms both DQN and
analytical methods, highlighting its effectiveness in improving real-time
system information freshness. Considering the complete end-to-end transmission
process, our optimization approach can improve decision-making performance and
overall system efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">23</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XChainWatcher: Monitoring and Identifying Attacks in Cross-Chain Bridges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02029v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02029v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Augusto, Rafael Belchior, Jonas Pfannschmidt, André Vasconcelos, Miguel Correia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-chain bridges are widely used blockchain interoperability mechanisms.
However, several of these bridges have vulnerabilities that have caused 3.2
billion dollars in losses since May 2021. Some studies have revealed the
existence of these vulnerabilities, but little quantitative research is
available, and there are no safeguard mechanisms to protect bridges from such
attacks. We propose XChainWatcher, the first mechanism for monitoring bridges
and detecting attacks against them. XChainWatcher relies on a cross-chain model
powered by a Datalog engine, designed to be pluggable into any cross-chain
bridge. Analyzing data from the Ronin and Nomad bridges, we successfully
identified the transactions that led to losses of \$611M and \$190M USD,
respectively. XChainWatcher not only uncovers successful attacks but also
reveals unintended behavior, such as 37 cross-chain transactions (cctx) that
these bridges should not have accepted, failed attempts to exploit Nomad, over
\$7.8M locked on one chain but never released on Ethereum, and \$200K lost due
to inadequate interaction with bridges. We provide the first open-source
dataset of 81,000 cctxs across three blockchains, capturing \$585M and \$3.7B
in token transfers in Nomad and Ronin, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Resilience of Fast Failover Routing Against Dynamic Link Failures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenkai Dai, Klaus-Tycho Foerster, Stefan Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern communication networks feature local fast failover mechanisms in the
data plane, to swiftly respond to link failures with pre-installed rerouting
rules. This paper explores resilient routing meant to tolerate $\leq k$
simultaneous link failures, ensuring packet delivery, provided that the source
and destination remain connected. While past theoretical works studied failover
routing under static link failures, i.e., links which permanently and
simultaneously fail, real-world networks often face link flapping--dynamic down
states caused by, e.g., numerous short-lived software-related faults. Thus, in
this initial work, we re-investigate the resilience of failover routing against
link flapping, by categorizing link failures into static, semi-dynamic
(removing the assumption that links fail simultaneously), and dynamic (removing
the assumption that links fail permanently) types, shedding light on the
capabilities and limitations of failover routing under these scenarios.
  We show that $k$-edge-connected graphs exhibit $(k-1)$-resilient routing
against dynamic failures for $k \leq 5$. We further show that this result
extends to arbitrary $k$ if it is possible to rewrite $\log k$ bits in the
packet header.
  Rewriting $3$ bits suffices to cope with $k$ semi-dynamic failures. However,
on general graphs, tolerating $2$ dynamic failures becomes impossible without
bit-rewriting. Even by rewriting $\log k$ bits, resilient routing cannot
resolve $k$ dynamic failures, demonstrating the limitation of local fast
rerouting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decentralized Collaborative Inertial Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alpha Diallo, Benoit Garbinato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although people spend most of their time indoors, outdoor tracking systems,
such as the Global Positioning System (GPS), are predominantly used for
location-based services. These systems are accurate outdoors, easy to use, and
operate autonomously on each mobile device. In contrast, Indoor Tracking
Systems~(ITS) lack standardization and are often difficult to operate because
they require costly infrastructure. In this paper, we propose an indoor
tracking algorithm that uses collected data from inertial sensors embedded in
most mobile devices. In this setting, mobile devices autonomously estimate
their location, hence removing the burden of deploying and maintaining complex
and scattered hardware infrastructure. In addition, these devices collaborate
by anonymously exchanging data with other nearby devices, using wireless
communication, such as Bluetooth, to correct errors in their location
estimates. Our collaborative algorithm relies on low-complexity geometry
operations and can be deployed on any recent mobile device with
commercial-grade sensors. We evaluate our solution on real-life data collected
by different devices. Experimentation with 16 simultaneously moving and
collaborating devices shows an average accuracy improvement of 44% compared to
the standalone Pedestrian Dead Reckoning algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACCEPTED FOR PUBLICATION AND PRESENTED IN EAI MOBIQUITOUS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constant pH Simulation with FMM Electrostatics in GROMACS. (B) GPU
  Accelerated Hamiltonian Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bartosz Kohnke, Eliane Briand, Carsten Kutzner, Helmut Grubmüller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The structural dynamics of biological macromolecules, such as proteins,
DNA/RNA, or their complexes, are strongly influenced by protonation changes of
their typically many titratable groups, which explains their pH sensitivity. In
turn, conformational and environmental changes in the biomolecule affect the
protonation state of these groups. With a few exceptions, conventional force
field-based molecular dynamics (MD) simulations do not account for these
effects, nor do they allow for coupling to a pH buffer.
  The $\lambda$-dynamics method implements this coupling and thus allows for MD
simulations at constant pH. It uses separate Hamiltonians for the protonated
and deprotonated states of each titratable group, with a $\lambda$ variable
that continuously interpolates between them. However, rigorous implementations
of Hamiltonian Interpolation (HI) $\lambda$-dynamics are prohibitively slow
when used with Particle Mesh Ewald (PME). To circumvent this problem, it has
been proposed to interpolate the charges instead of the Hamiltonians (QI).
  Here, we propose a rigorous yet efficient Multipole-Accelerated Hamiltonian
Interpolation (MAHI) method to perform $\lambda$-dynamics in GROMACS. Starting
from a charge-scaled Hamiltonian, precomputed with the Fast Multipole Method
(FMM) or with PME, the correct HI forces are calculated with negligible
computational overhead. We compare HI with QI and show that HI leads to more
frequent transitions between protonation states, resulting in better sampling
and accuracy. Our performance benchmarks show that introducing, e.g., 512
titratable sites to a one million atom MD system increases runtime by less than
20% compared to a regular FMM-based simulation. We have integrated the scheme
into our GPU-FMM code for the simulation software GROMACS, allowing an easy and
effortless transition from standard force field simulations to constant pH
simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable and Consistent Graph Neural Networks for Distributed Mesh-based
  Data-driven Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivam Barwey, Riccardo Balin, Bethany Lusch, Saumil Patel, Ramesh Balakrishnan, Pinaki Pal, Romit Maulik, Venkatram Vishwanath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work develops a distributed graph neural network (GNN) methodology for
mesh-based modeling applications using a consistent neural message passing
layer. As the name implies, the focus is on enabling scalable operations that
satisfy physical consistency via halo nodes at sub-graph boundaries. Here,
consistency refers to the fact that a GNN trained and evaluated on one rank
(one large graph) is arithmetically equivalent to evaluations on multiple ranks
(a partitioned graph). This concept is demonstrated by interfacing GNNs with
NekRS, a GPU-capable exascale CFD solver developed at Argonne National
Laboratory. It is shown how the NekRS mesh partitioning can be linked to the
distributed GNN training and inference routines, resulting in a scalable
mesh-based data-driven modeling workflow. We study the impact of consistency on
the scalability of mesh-based GNNs, demonstrating efficient scaling in
consistent GNNs for up to O(1B) graph nodes on the Frontier exascale
supercomputer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Framework of Horizontal-Vertical Hybrid Federated Learning for
  EdgeIoT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Li, Yilei Liang, Xin Yuan, Wei Ni, Jon Crowcroft, Chau Yuen, Ozgur B. Akan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This letter puts forth a new hybrid horizontal-vertical federated learning
(HoVeFL) for mobile edge computing-enabled Internet of Things (EdgeIoT). In
this framework, certain EdgeIoT devices train local models using the same data
samples but analyze disparate data features, while the others focus on the same
features using non-independent and identically distributed (non-IID) data
samples. Thus, even though the data features are consistent, the data samples
vary across devices. The proposed HoVeFL formulates the training of local and
global models to minimize the global loss function. Performance evaluations on
CIFAR-10 and SVHN datasets reveal that the testing loss of HoVeFL with 12
horizontal FL devices and six vertical FL devices is 5.5% and 25.2% higher,
respectively, compared to a setup with six horizontal FL devices and 12
vertical FL devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constant pH Simulation with FMM Electrostatics in GROMACS. (A) Design
  and Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eliane Briand, Bartosz Kohnke, Carsten Kutzner, Helmut Grubmüller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The structural dynamics of biological macromolecules, such as proteins,
DNA/RNA, or complexes thereof, are strongly influenced by protonation changes
of their typically many titratable groups, which explains their sensitivity to
pH changes. Conversely, conformational and environmental changes of the
biomolecule affect the protonation state of these groups. With few exceptions,
conventional force field-based molecular dynamics (MD) simulations do not
account for these effects, nor do they allow for coupling to a pH buffer.
  Here we present a GROMACS implementation of a rigorous Hamiltonian
interpolation $\lambda$-dynamics constant pH method, which rests on
GPU-accelerated Fast Multipole Method (FMM) electrostatics. Our implementation
supports both CHARMM36m and Amber99sb*-ILDN force fields and is largely
automated to enable seamless switching from regular MD to constant pH MD,
involving minimal changes to the input files. Here, the first of two companion
papers describes the underlying constant pH protocol and sample applications to
several prototypical benchmark systems such as cardiotoxin V, lysozyme, and
staphylococcal nuclease. Enhanced convergence is achieved through a new dynamic
barrier height optimization method, and high p$K_a$ accuracy is demonstrated.
We use Functional Mode Analysis and Mutual Information to explore the complex
intra- and intermolecular couplings between the protonation states of
titratable groups as well as those between protonation states and
conformational dynamics. We identify striking conformation-dependent p$K_a$
variations and unexpected inter-residue couplings. Conformation-protonation
coupling is identified as a primary cause of the slow protonation convergence
notorious to constant pH simulations involving multiple titratable groups,
suggesting enhanced sampling methods to accelerate convergence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-level Memory-Centric Profiling on ARM Processors with ARM SPE <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Miksits, Ruimin Shi, Maya Gokhale, Jacob Wahlgren, Gabin Schieffer, Ivy Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-end ARM processors are emerging in data centers and HPC systems, posing
as a strong contender to x86 machines. Memory-centric profiling is an important
approach for dissecting an application's bottlenecks on memory access and
guiding optimizations. Many existing memory profiling tools leverage hardware
performance counters and precise event sampling, such as Intel PEBS and AMD
IBS, to achieve high accuracy and low overhead. In this work, we present a
multi-level memory profiling tool for ARM processors, leveraging Statistical
Profiling Extension (SPE). We evaluate the tool using both HPC and Cloud
workloads on the ARM Ampere processor. Our results provide the first
quantitative assessment of time overhead and sampling accuracy of ARM SPE for
memory-centric profiling at different sampling periods and aux buffer sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in Workshop Proceedings of The International
  Conference for High Performance Computing Networking, Storage, and Analysis
  (SC-W '24) (2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Federated Learning on Flowing Data Heterogeneity under
  Restricted Storage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sixing Tan, Xianmin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years, researchers focused on personalized federated learning (pFL) to
address the inconsistent requirements of clients causing by data heterogeneity
in federated learning (FL). However, existing pFL methods typically assume that
local data distribution remains unchanged during FL training, the changing data
distribution in actual heterogeneous data scenarios can affect model
convergence rate and reduce model performance. In this paper, we focus on
solving the pFL problem under the situation where data flows through each
client like a flowing stream which called Flowing Data Heterogeneity under
Restricted Storage, and shift the training goal to the comprehensive
performance of the model throughout the FL training process. Therefore, based
on the idea of category decoupling, we design a local data distribution
reconstruction scheme and a related generator architecture to reduce the error
of the controllable replayed data distribution, then propose our pFL framework,
pFedGRP, to achieve knowledge transfer and personalized aggregation.
Comprehensive experiments on five datasets with multiple settings show the
superiority of pFedGRP over eight baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theoretical Lower Bounds for the Oven Scheduling Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesca Da Ros, Marie-Louise Lackner, Nysret Musliu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Oven Scheduling Problem (OSP) is an NP-hard real-world parallel batch
scheduling problem arising in the semiconductor industry. The objective of the
problem is to schedule a set of jobs on ovens while minimizing several factors,
namely total oven runtime, job tardiness, and setup costs. At the same time, it
must adhere to various constraints such as oven eligibility and availability,
job release dates, setup times between batches, and oven capacity limitations.
The key to obtaining efficient schedules is to process compatible jobs
simultaneously in batches. In this paper, we develop theoretical,
problem-specific lower bounds for the OSP that can be computed very quickly. We
thoroughly examine these lower bounds, evaluating their quality and exploring
their integration into existing solution methods. Specifically, we investigate
their contribution to exact methods and a metaheuristic local search approach
using simulated annealing. Moreover, these problem-specific lower bounds enable
us to assess the solution quality for large instances for which exact methods
often fail to provide tight lower bounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2203.12517</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-World Data and Calibrated Simulation Suite for Offline Training of
  Reinforcement Learning Agents to Optimize Energy and Emission in Buildings
  for Environmental Sustainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Judah Goldfeder, John Sipple
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Commercial office buildings contribute 17 percent of Carbon Emissions in the
US, according to the US Energy Information Administration (EIA), and improving
their efficiency will reduce their environmental burden and operating cost. A
major contributor of energy consumption in these buildings are the Heating,
Ventilation, and Air Conditioning (HVAC) devices. HVAC devices form a complex
and interconnected thermodynamic system with the building and outside weather
conditions, and current setpoint control policies are not fully optimized for
minimizing energy use and carbon emission. Given a suitable training
environment, a Reinforcement Learning (RL) agent is able to improve upon these
policies, but training such a model, especially in a way that scales to
thousands of buildings, presents many practical challenges. Most existing work
on applying RL to this important task either makes use of proprietary data, or
focuses on expensive and proprietary simulations that may not be grounded in
the real world. We present the Smart Buildings Control Suite, the first open
source interactive HVAC control dataset extracted from live sensor measurements
of devices in real office buildings. The dataset consists of two components:
six years of real-world historical data from three buildings, for offline RL,
and a lightweight interactive simulator for each of these buildings, calibrated
using the historical data, for online and model-based RL. For ease of use, our
RL environments are all compatible with the OpenAI gym environment standard. We
also demonstrate a novel method of calibrating the simulator, as well as
baseline results on training an RL agent on the simulator, predicting
real-world data, and training an RL agent directly from data. We believe this
benchmark will accelerate progress and collaboration on building optimization
and environmental sustainability research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ParallelSFL: A Novel Split Federated Learning Framework Tackling
  Heterogeneity Issues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunming Liao, Yang Xu, Hongli Xu, Zhiwei Yao, Liusheng Huang, Chunming Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile devices contribute more than half of the world's web traffic,
providing massive and diverse data for powering various federated learning (FL)
applications. In order to avoid the communication bottleneck on the parameter
server (PS) and accelerate the training of large-scale models on
resourceconstraint workers in edge computing (EC) system, we propose a novel
split federated learning (SFL) framework, termed ParallelSFL. Concretely, we
split an entire model into a bottom submodel and a top submodel, and divide
participating workers into multiple clusters, each of which collaboratively
performs the SFL training procedure and exchanges entire models with the PS.
However, considering the statistical and system heterogeneity in edge systems,
it is challenging to arrange suitable workers to specific clusters for
efficient model training. To address these challenges, we carefully develop an
effective clustering strategy by optimizing a utility function related to
training efficiency and model accuracy. Specifically, ParallelSFL partitions
workers into different clusters under the heterogeneity restrictions, thereby
promoting model accuracy as well as training efficiency. Meanwhile, ParallelSFL
assigns diverse and appropriate local updating frequencies for each cluster to
further address system heterogeneity. Extensive experiments are conducted on a
physical platform with 80 NVIDIA Jetson devices, and the experimental results
show that ParallelSFL can reduce the traffic consumption by at least 21%, speed
up the model training by at least 1.36x, and improve model accuracy by at least
5% in heterogeneous scenarios, compared to the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2311.13348</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConServe: Harvesting GPUs for Low-Latency and High-Throughput Large
  Language Model Serving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Qiao, Shu Anzai, Shan Yu, Haoran Ma, Yang Wang, Miryung Kim, Harry Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many applications are leveraging large language models (LLMs) for complex
tasks, and they generally demand low inference latency and high serving
throughput for interactive online jobs such as chatbots. However, the tight
latency requirement and high load variance of applications pose challenges to
serving systems in achieving high GPU utilization. Due to the high costs of
scheduling and preemption, today's systems generally use separate clusters to
serve online and offline inference tasks, and dedicate GPUs for online
inferences to avoid interference. This approach leads to underutilized GPUs
because one must reserve enough GPU resources for the peak expected load, even
if the average load is low.
  This paper proposes to harvest stranded GPU resources for offline LLM
inference tasks such as document summarization and LLM benchmarking. Unlike
online inferences, these tasks usually run in a batch-processing manner with
loose latency requirements, making them a good fit for stranded resources that
are only available shortly. To enable safe and efficient GPU harvesting without
interfering with online tasks, we built ConServe, an LLM serving system that
contains (1) an execution engine that preempts running offline tasks upon the
arrival of online tasks, (2) an incremental checkpointing mechanism that
minimizes the amount of recomputation required by preemptions, and (3) a
scheduler that adaptively batches offline tasks for higher GPU utilization. Our
evaluation demonstrates that ConServe achieves strong performance isolation
when co-serving online and offline tasks but at a much higher GPU utilization.
When colocating practical online and offline workloads on popular models such
as Llama-2-7B, ConServe achieves 2.35$\times$ higher throughput than
state-of-the-art online serving systems and reduces serving latency by
84$\times$ compared to existing co-serving systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Fine-grained Task Parallelism on Simultaneous Multithreading
  Cores 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Denis Los, Igor Petushkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, latency-critical, high-performance applications are parallelized
even on power-constrained client systems to improve performance. However, an
important scenario of fine-grained tasking on simultaneous multithreading CPU
cores in such systems has not been well researched in previous works. Hence, in
this paper, we conduct performance analysis of state-of-the-art shared-memory
parallel programming frameworks on simultaneous multithreading cores using
real-world fine-grained application kernels. We introduce a specialized and
simple software-only parallel programming framework called Relic to enable
extremely fine-grained tasking on simultaneous multithreading cores. Using
Relic framework, we increase performance speedups over serial implementations
of benchmark kernels by 19.1% compared to LLVM OpenMP, by 31.0% compared to GNU
OpenMP, by 20.2% compared to Intel OpenMP, by 33.2% compared to X-OpenMP, by
30.1% compared to oneTBB, by 23.0% compared to Taskflow, and by 21.4% compared
to OpenCilk.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Debiasing Federated Learning with Correlated Client Participation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Sun, Ziyang Zhang, Zheng Xu, Gauri Joshi, Pranay Sharma, Ermin Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In cross-device federated learning (FL) with millions of mobile clients, only
a small subset of clients participate in training in every communication round,
and Federated Averaging (FedAvg) is the most popular algorithm in practice.
Existing analyses of FedAvg usually assume the participating clients are
independently sampled in each round from a uniform distribution, which does not
reflect real-world scenarios. This paper introduces a theoretical framework
that models client participation in FL as a Markov chain to study optimization
convergence when clients have non-uniform and correlated participation across
rounds. We apply this framework to analyze a more general and practical
pattern: every client must wait a minimum number of $R$ rounds (minimum
separation) before re-participating. We theoretically prove and empirically
observe that increasing minimum separation reduces the bias induced by
intrinsic non-uniformity of client availability in cross-device FL systems.
Furthermore, we develop an effective debiasing algorithm for FedAvg that
provably converges to the unbiased optimal solution under arbitrary minimum
separation and unknown client availability distribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning the Optimal Path and DNN Partition for Collaborative Edge
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin Huang, Letian Zhang, Jie Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Deep Neural Networks (DNNs) have catalyzed the
development of numerous intelligent mobile applications and services. However,
they also introduce significant computational challenges for
resource-constrained mobile devices. To address this, collaborative edge
inference has been proposed. This method involves partitioning a DNN inference
task into several subtasks and distributing these across multiple network
nodes. Despite its potential, most current approaches presume known network
parameters -- like node processing speeds and link transmission rates -- or
rely on a fixed sequence of nodes for processing the DNN subtasks. In this
paper, we tackle a more complex scenario where network parameters are unknown
and must be learned, and multiple network paths are available for distributing
inference tasks. Specifically, we explore the learning problem of selecting the
optimal network path and assigning DNN layers to nodes along this path,
considering potential security threats and the costs of switching paths. We
begin by deriving structural insights from the DNN layer assignment with
complete network information, which narrows down the decision space and
provides crucial understanding of optimal assignments. We then cast the
learning problem with incomplete network information as a novel adversarial
group linear bandits problem with switching costs, featuring rewards generation
through a combined stochastic and adversarial process. We introduce a new
bandit algorithm, B-EXPUCB, which combines elements of the classical blocked
EXP3 and LinUCB algorithms, and demonstrate its sublinear regret. Extensive
simulations confirm B-EXPUCB's superior performance in learning for
collaborative edge inference over existing algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 15 figures, submitted to IEEE journals for possible
  publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Hybrid Defense for Byzantine Attacks in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06474v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06474v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Yue, Richeng Jin, Chau-Wai Wong, Huaiyu Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) enables multiple clients to collaboratively train a
global model without sharing their local data. Recent studies have highlighted
the vulnerability of FL to Byzantine attacks, where malicious clients send
poisoned updates to degrade model performance. Notably, many attacks have been
developed targeting specific aggregation rules, whereas various defense
mechanisms have been designed for dedicated threat models. This paper studies
the resilience of an attack-agnostic FL scenario, where the server lacks prior
knowledge of both the attackers' strategies and the number of malicious clients
involved. We first introduce a hybrid defense against state-of-the-art attacks.
Our goal is to identify a general-purpose aggregation rule that performs well
on average while also avoiding worst-case vulnerabilities. By adaptively
selecting from available defenses, we demonstrate that the server remains
robust even when confronted with a substantial proportion of poisoned updates.
To better understand this resilience, we then assess the attackers' capability
using a proxy called client heterogeneity. We also emphasize that the existing
FL defenses should not be regarded as secure, as demonstrated through the newly
proposed Trapsetter attack. The proposed attack outperforms other
state-of-the-art attacks by further reducing the model test accuracy by 8-10%.
Our findings highlight the ongoing need for the development of
Byzantine-resilient aggregation algorithms in FL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asynchronous Federated Reinforcement Learning with Policy Gradient
  Updates: Algorithm Design and Convergence Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08003v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08003v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangchen Lan, Dong-Jun Han, Abolfazl Hashemi, Vaneet Aggarwal, Christopher G. Brinton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To improve the efficiency of reinforcement learning (RL), we propose a novel
asynchronous federated reinforcement learning (FedRL) framework termed AFedPG,
which constructs a global model through collaboration among $N$ agents using
policy gradient (PG) updates. To address the challenge of lagged policies in
asynchronous settings, we design a delay-adaptive lookahead technique
\textit{specifically for FedRL} that can effectively handle heterogeneous
arrival times of policy gradients. We analyze the theoretical global
convergence bound of AFedPG, and characterize the advantage of the proposed
algorithm in terms of both the sample complexity and time complexity.
Specifically, our AFedPG method achieves $O(\frac{{\epsilon}^{-2.5}}{N})$
sample complexity for global convergence at each agent on average. Compared to
the single agent setting with $O(\epsilon^{-2.5})$ sample complexity, it enjoys
a linear speedup with respect to the number of agents. Moreover, compared to
synchronous FedPG, AFedPG improves the time complexity from
$O(\frac{t_{\max}}{N})$ to $O({\sum_{i=1}^{N} \frac{1}{t_{i}}})^{-1}$, where
$t_{i}$ denotes the time consumption in each iteration at agent $i$, and
$t_{\max}$ is the largest one. The latter complexity $O({\sum_{i=1}^{N}
\frac{1}{t_{i}}})^{-1}$ is always smaller than the former one, and this
improvement becomes significant in large-scale federated settings with
heterogeneous computing powers ($t_{\max}\gg t_{\min}$). Finally, we
empirically verify the improved performance of AFedPG in four widely-used
MuJoCo environments with varying numbers of agents. We also demonstrate the
advantages of AFedPG in various computing heterogeneity scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TACOS: Topology-Aware Collective Algorithm Synthesizer for Distributed
  Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.05301v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.05301v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Won, Midhilesh Elavazhagan, Sudarshan Srinivasan, Swati Gupta, Tushar Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The surge of artificial intelligence, particularly large language models, has
driven the rapid development of large-scale machine learning clusters.
Executing distributed models on these clusters is often constrained by
communication overhead, making efficient utilization of available network
resources crucial. As a result, the routing algorithm employed for collective
communications (i.e., collective algorithms) plays a pivotal role in
determining overall performance. Unfortunately, existing collective
communication libraries for distributed machine learning are limited by a fixed
set of basic collective algorithms. This limitation hinders communication
optimization, especially in modern clusters with heterogeneous and asymmetric
topologies. Furthermore, manually designing collective algorithms for all
possible combinations of network topologies and collective patterns requires
heavy engineering and validation efforts. To address these challenges, this
paper presents TACOS, an autonomous synthesizer capable of automatically
generating topology-aware collective algorithms tailored to specific collective
patterns and network topologies. TACOS is highly flexible, synthesizing an
All-Reduce algorithm for a heterogeneous 128-NPU system in just 1.08 seconds,
while achieving up to a 4.27x performance improvement over state-of-the-art
synthesizers. Additionally, TACOS demonstrates better scalability with
polynomial synthesis times, in contrast to NP-hard approaches which only scale
to systems with tens of NPUs. TACOS can synthesize for 40K NPUs in just 2.52
hours.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Contains 12 main pages, 21 figures, 5 tables. Artifact appendix
  attached</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging MTD to Mitigate Poisoning Attacks in Decentralized FL with
  Non-IID Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19302v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19302v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Feng, Alberto Huertas Celdrán, Zien Zeng, Zi Ye, Jan von der Assen, Gerome Bovet, Burkhard Stiller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decentralized Federated Learning (DFL), a paradigm for managing big data in a
privacy-preserved manner, is still vulnerable to poisoning attacks where
malicious clients tamper with data or models. Current defense methods often
assume Independently and Identically Distributed (IID) data, which is
unrealistic in real-world applications. In non-IID contexts, existing defensive
strategies face challenges in distinguishing between models that have been
compromised and those that have been trained on heterogeneous data
distributions, leading to diminished efficacy. In response, this paper proposes
a framework that employs the Moving Target Defense (MTD) approach to bolster
the robustness of DFL models. By continuously modifying the attack surface of
the DFL system, this framework aims to mitigate poisoning attacks effectively.
The proposed MTD framework includes both proactive and reactive modes,
utilizing a reputation system that combines metrics of model similarity and
loss, alongside various defensive techniques. Comprehensive experimental
evaluations indicate that the MTD-based mechanism significantly mitigates a
range of poisoning attack types across multiple datasets with different
topologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Instruction Tuning of LLMs with Domain Coverage Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20135v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20135v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zezhou Wang, Yaxin Du, Zhuzhong Qian, Siheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited
cross-client private data together with server-side public data for instruction
augmentation, ultimately boosting model performance within specific domains. To
date, the factors affecting FedDIT remain unclear, and existing instruction
augmentation methods primarily focus on the centralized setting without
considering distributed environments. Our experiments reveal that the
cross-client domain coverage, rather than data heterogeneity, drives model
performance in FedDIT. In response, we propose FedDCA, which optimizes domain
coverage through greedy client center selection and retrieval-based
augmentation. For client-side computational efficiency and system scalability,
FedDCA$^*$, the variant of FedDCA, utilizes heterogeneous encoders with
server-side feature alignment. Extensive experiments across four distinct
domains (code, medical, financial, and mathematical) substantiate the
effectiveness of both methods. Additionally, we investigate privacy
preservation against memory extraction attacks utilizing various amounts of
public data. Results show that there is no significant correlation between the
volume of public data and the privacy-preserving capability. However, as the
fine-tuning rounds increase, the risk of privacy leakage reduces or converges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HybridFlow: A Flexible and Efficient RLHF Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, Chuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback (RLHF) is widely used in Large
Language Model (LLM) alignment. Traditional RL can be modeled as a dataflow,
where each node represents computation of a neural network (NN) and each edge
denotes data dependencies between the NNs. RLHF complicates the dataflow by
expanding each node into a distributed LLM training or generation program, and
each edge into a many-to-many multicast. Traditional RL frameworks execute the
dataflow using a single controller to instruct both intra-node computation and
inter-node communication, which can be inefficient in RLHF due to large control
dispatch overhead for distributed intra-node computation. Existing RLHF systems
adopt a multi-controller paradigm, which can be inflexible due to nesting
distributed computation and data communication. We propose HybridFlow, which
combines single-controller and multi-controller paradigms in a hybrid manner to
enable flexible representation and efficient execution of the RLHF dataflow. We
carefully design a set of hierarchical APIs that decouple and encapsulate
computation and data dependencies in the complex RLHF dataflow, allowing
efficient operation orchestration to implement RLHF algorithms and flexible
mapping of the computation onto various devices. We further design a
3D-HybridEngine for efficient actor model resharding between training and
generation phases, with zero memory redundancy and significantly reduced
communication overhead. Our experimental results demonstrate
1.53$\times$~20.57$\times$ throughput improvement when running various RLHF
algorithms using HybridFlow, as compared with state-of-the-art baselines.
HybridFlow source code will be available at https://github.com/volcengine/verl.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FPGA-based Distributed Union-Find Decoder for Surface Codes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08491v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08491v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Namitha Liyanage, Yue Wu, Siona Tagare, Lin Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fault-tolerant quantum computer must decode and correct errors faster than
they appear to prevent exponential slowdown due to error correction. The
Union-Find (UF) decoder is promising with an average time complexity slightly
higher than $O(d^3)$. We report a distributed version of the UF decoder that
exploits parallel computing resources for further speedup. Using an FPGA-based
implementation, we empirically show that this distributed UF decoder has a
sublinear average time complexity with regard to $d$, given $O(d^3)$ parallel
computing resources. The decoding time per measurement round decreases as $d$
increases, the first time for a quantum error decoder. The implementation
employs a scalable architecture called Helios that organizes parallel computing
resources into a hybrid tree-grid structure. Using a Xilinx VCU129 FPGA, we
successfully implement $d$ up to 21 with an average decoding time of 11.5 ns
per measurement round under 0.1\% phenomenological noise, and 23.7 ns for
$d=17$ under equivalent circuit-level noise. This performance is significantly
faster than any existing decoder implementation. Furthermore, we show that
Helios can optimize for resource efficiency by decoding $d=51$ on a Xilinx
VCU129 FPGA with an average latency of 544ns per measurement round.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The article extends the work in arXiv:2301.08419, which also appeared
  in https://ieeexplore.ieee.org/document/10313800</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-01T00:00:00Z">2024-10-01</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Software Engineering <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Human-LLM Dynamic: A Literature <span class="highlight-title">Survey</span> of LLM Use in
  Programming Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deborah Etsenake, Meiyappan Nagappan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are transforming programming practices, offering
significant capabilities for code generation activities. While researchers have
explored the potential of LLMs in various domains, this paper focuses on their
use in programming tasks, drawing insights from user studies that assess the
impact of LLMs on programming tasks. We first examined the user interaction
behaviors with LLMs observed in these studies, from the types of requests made
to task completion strategies. Additionally, our analysis reveals both benefits
and weaknesses of LLMs showing mixed effects on the human and task. Lastly, we
looked into what factors from the human, LLM or the interaction of both, affect
the human's enhancement as well as the task performance. Our findings highlight
the variability in human-LLM interactions due to the non-deterministic nature
of both parties (humans and LLMs), underscoring the need for a deeper
understanding of these interaction patterns. We conclude by providing some
practical suggestions for researchers as well as programmers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 8 tables, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GEMS: Generative Expert Metric System through Iterative <span class="highlight-title">Prompt</span> Priming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ti-Chung Cheng, Carmen Badea, Christian Bird, Thomas Zimmermann, Robert DeLine, Nicole Forsgren, Denae Ford
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Across domains, metrics and measurements are fundamental to identifying
challenges, informing decisions, and resolving conflicts. Despite the abundance
of data available in this information age, not only can it be challenging for a
single expert to work across multi-disciplinary data, but non-experts can also
find it unintuitive to create effective measures or transform theories into
context-specific metrics that are chosen appropriately. This technical report
addresses this challenge by examining software communities within large
software corporations, where different measures are used as proxies to locate
counterparts within the organization to transfer tacit knowledge. We propose a
prompt-engineering framework inspired by neural activities, demonstrating that
generative models can extract and summarize theories and perform basic
reasoning, thereby transforming concepts into context-aware metrics to support
software communities given software repository data. While this research zoomed
in on software communities, we believe the framework's applicability extends
across various fields, showcasing expert-theory-inspired metrics that aid in
triaging complex challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TestGenEval: A Real World Unit Test Generation and Test Completion
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kush Jain, Gabriel Synnaeve, Baptiste Rozière
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code generation models can help improve many common software tasks ranging
from code completion to defect prediction. Most of the existing benchmarks for
code generation LLMs focus on code authoring or code completion. Surprisingly,
there has been far less effort dedicated to benchmarking software testing,
despite the strong correlation between well-tested software and effective bug
detection. To address this gap, we create and release TestGenEval, a
large-scale benchmark to measure test generation performance. Based on
SWEBench, TestGenEval comprises 68,647 tests from 1,210 code and test file
pairs across 11 well-maintained Python repositories. It covers initial tests
authoring, test suite completion, and code coverage improvements. Test
authoring simulates the process of a developer writing a test suite from
scratch, while test completion mimics the scenario where a developer aims to
improve the coverage of an existing test suite. We evaluate several popular
models, with sizes ranging from 7B to 405B parameters. Our detailed analysis
highlights TestGenEval's contribution to a comprehensive evaluation of test
generation performance. In particular, models struggle to generate
high-coverage test suites, with the best model, GPT-4o, achieving an average
coverage of only 35.2%. This is primarily due to models struggling to reason
about execution, and their frequent assertion errors when addressing complex
code paths.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Auto Validation For Self-Refinement in Web Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruhana Azam, Tamer Abuelsaad, Aditya Vempaty, Ashish Jagmohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As our world digitizes, web agents that can automate complex and monotonous
tasks are becoming essential in streamlining workflows. This paper introduces
an approach to improving web agent performance through multi-modal validation
and self-refinement. We present a comprehensive study of different modalities
(text, vision) and the effect of hierarchy for the automatic validation of web
agents, building upon the state-of-the-art Agent-E web automation framework. We
also introduce a self-refinement mechanism for web automation, using the
developed auto-validator, that enables web agents to detect and self-correct
workflow failures. Our results show significant gains on Agent-E's (a SOTA web
agent) prior state-of-art performance, boosting task-completion rates from
76.2\% to 81.24\% on the subset of the WebVoyager benchmark. The approach
presented in this paper paves the way for more reliable digital assistants in
complex, real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Testing and Analysis of Quantum Software 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Paltenghi, Michael Pradel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum computing is getting increasing interest from both academia and
industry, and the quantum software landscape has been growing rapidly. The
quantum software stack comprises quantum programs, implementing algorithms, and
platforms like IBM Qiskit, Google Cirq, and Microsoft Q#, enabling their
development. To ensure the reliability and performance of quantum software,
various techniques for testing and analyzing it have been proposed, such as
test generation, bug pattern detection, and circuit optimization. However, the
large amount of work and the fact that work on quantum software is performed by
several research communities, make it difficult to get a comprehensive overview
of the existing techniques. In this work, we provide an extensive survey of the
state of the art in testing and analysis of quantum software. We discuss
literature from several research communities, including quantum computing,
software engineering, programming languages, and formal methods. Our survey
covers a wide range of topics, including expected and unexpected behavior of
quantum programs, testing techniques, program analysis approaches,
optimizations, and benchmarks for testing and analyzing quantum software. We
create novel connections between the discussed topics and present them in an
accessible way. Finally, we discuss key challenges and open problems to inspire
future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 12 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Study of Large Language Models for Type and Call Graph
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashwin Prasad Shivarpatna Venkatesh, Rose Sunil, Samkutty Sabu, Amir M. Mir, Sofia Reis, Eric Bodden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly being explored for their
potential in software engineering, particularly in static analysis tasks. In
this study, we investigate the potential of current LLMs to enhance call-graph
analysis and type inference for Python and JavaScript programs. We empirically
evaluated 24 LLMs, including OpenAI's GPT series and open-source models like
LLaMA and Mistral, using existing and newly developed benchmarks. Specifically,
we enhanced TypeEvalPy, a micro-benchmarking framework for type inference in
Python, with auto-generation capabilities, expanding its scope from 860 to
77,268 type annotations for Python. Additionally, we introduced SWARM-CG and
SWARM-JS, comprehensive benchmarking suites for evaluating call-graph
construction tools across multiple programming languages. Our findings reveal a
contrasting performance of LLMs in static analysis tasks. For call-graph
generation in Python, traditional static analysis tools like PyCG significantly
outperform LLMs. In JavaScript, the static tool TAJS underperforms due to its
inability to handle modern language features, while LLMs, despite showing
potential with models like mistral-large-it-2407-123b and GPT-4o, struggle with
completeness and soundness in both languages for call-graph analysis.
Conversely, LLMs demonstrate a clear advantage in type inference for Python,
surpassing traditional tools like HeaderGen and hybrid approaches such as
HiTyper. These results suggest that while LLMs hold promise in type inference,
their limitations in call-graph analysis highlight the need for further
research. Our study provides a foundation for integrating LLMs into static
analysis workflows, offering insights into their strengths and current
limitations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print: Submitted to EMSE journal for review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards an Argument Pattern for the Use of Safety Performance Indicators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Ratiu, Tihomir Rohlinger, Torben Stolte, Stefan Wagner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  UL 4600, the safety standard for autonomous products, mandates the use of
Safety Performance Indicators (SPIs) to continuously ensure the validity of
safety cases by monitoring and taking action when violations are identified.
Despite numerous examples of concrete SPIs available in the standard and
companion literature, their contribution rationale for achieving safety is
often left implicit. In this paper, we present our initial work towards an
argument pattern for the use of SPIs to ensure validity of safety cases
throughout the entire lifecycle of the system. Our aim is to make the implicit
argument behind using SPIs explicit, and based on this, to analyze the
situations that can undermine confidence in the chosen set of SPIs. To maintain
the confidence in SPIs' effectiveness, we propose an approach to continuously
monitor their expected performance by using meta-SPIs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge
  Distillation for Large Language Models in Code Generation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Luo, Xin Li, Hongzhan Lin, Jing Ma, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The impressive performance of proprietary LLMs like GPT4 in code generation
has led to a trend to replicate these capabilities in open-source models
through knowledge distillation (e.g. Code Evol-Instruct). However, these
efforts often neglect the crucial aspect of response quality, relying heavily
on teacher models for direct response distillation. This paradigm, especially
for complex instructions, can degrade the quality of synthesized data,
compromising the knowledge distillation process. To this end, our study
introduces the Adaptive Modular Response Evolution (AMR-Evol) framework, which
employs a two-stage process to refine response distillation. The first stage,
modular decomposition, breaks down the direct response into more manageable
sub-modules. The second stage, adaptive response evolution, automatically
evolves the response with the related function modules. Our experiments with
three popular code benchmarks (HumanEval, MBPP, and EvalPlus) attest to the
superiority of the AMR-Evol framework over baseline response distillation
methods. By comparing with the open-source Code LLMs trained on a similar scale
of data, we observed performance enhancements: more than +3.0 points on
HumanEval-Plus and +1.0 points on MBPP-Plus, which underscores the
effectiveness of our framework. Our codes are available at
https://github.com/ChiYeungLaw/AMR-Evol.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Monitoring of Timed Properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Léo Henry, Thierry Jéron, Nicolas Markey, Victor Roussanaly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In formal verification, runtime monitoring consists of observing the
execution of a system in order to decide as quickly as possible whether or not
it satisfies a given property. We consider monitoring in a distributed setting,
for properties given as reachability timed automata. In such a setting, the
system is made of several components, each equipped with its own local clock
and monitor. The monitors observe events occurring on their associated
component, and receive timestamped events from other monitors through FIFO
channels. Since clocks are local, they cannot be perfectly synchronized,
resulting in imprecise timestamps. Consequently, they must be seen as
intervals, leading monitors to consider possible reorderings of events. In this
context, each monitor aims to provide, as early as possible, a verdict on the
property it is monitoring, based on its potentially incomplete and imprecise
knowledge of the current execution. In this paper, we propose an on-line
monitoring algorithm for timed properties, robust to time imprecision and
partial information from distant components. We first identify the date at
which a monitor can safely compute a verdict based on received events. We then
propose a monitoring algorithm that updates this date when new information
arrives, maintains the current set of states in which the property can reside,
and updates its verdict accordingly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SharkTrack: an accurate, generalisable software for streamlining shark
  and ray underwater video analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20623v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20623v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippo Varini, Joel H. Gayford, Jeremy Jenrette, Matthew J. Witt, Francesco Garzon, Francesco Ferretti, Sophie Wilday, Mark E. Bond, Michael R. Heithaus, Danielle Robinson, Devon Carter, Najee Gumbs, Vincent Webster, Ben Glocker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Elasmobranchs (shark sand rays) represent a critical component of marine
ecosystems. Yet, they are experiencing global population declines and effective
monitoring of populations is essential to their protection. Underwater
stationary videos, such as those from Baited Remote Underwater Video Stations
(BRUVS), are critical for understanding elasmobranch spatial ecology and
abundance. However, processing these videos requires time-consuming manual
analysis that can delay conservation. To address this challenge, we developed
SharkTrack, a semi-automatic underwater video analysis software. SharkTrack
uses Convolutional Neural Networks (CNN) and Multi-Object Tracking to
automatically detect and track elasmobranchs and provides an annotation
pipeline to manually classify elasmobranch species and compute species-specific
MaxN (ssMaxN), the standard metric of relative abundance. When tested on BRUVS
footage from locations unseen by the CNN model during training, SharkTrack
computed ssMaxN with 89% accuracy over 207 hours of footage. The semi-automatic
SharkTrack pipeline required two minutes of manual classification per hour of
video, an estimated 95% reduction of manual analysis time compared to
traditional methods. Furthermore, we demonstrate SharkTrack accuracy across
diverse marine ecosystems and elasmobranch species, an advancement compared to
previous models, which were limited to specific species or locations.
SharkTrack applications extend beyond BRUVS, facilitating the analysis of any
underwater stationary video. By making video analysis faster and more
accessible, SharkTrack enables research and conservation organisations to
monitor elasmobranch populations more efficiently, thereby improving
conservation efforts. To further support these goals, we provide public access
to the SharkTrack software.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automating Semantic Analysis of System Assurance Cases using
  Goal-directed ASP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11699v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11699v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anitha Murugesan, Isaac Wong, Joaquín Arias, Robert Stroud, Srivatsan Varadarajan, Elmer Salazar, Gopal Gupta, Robin Bloomfield, John Rushby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assurance cases offer a structured way to present arguments and evidence for
certification of systems where safety and security are critical. However,
creating and evaluating these assurance cases can be complex and challenging,
even for systems of moderate complexity. Therefore, there is a growing need to
develop new automation methods for these tasks. While most existing assurance
case tools focus on automating structural aspects, they lack the ability to
fully assess the semantic coherence and correctness of the assurance arguments.
  In prior work, we introduced the Assurance 2.0 framework that prioritizes the
reasoning process, evidence utilization, and explicit delineation of
counter-claims (defeaters) and counter-evidence. In this paper, we present our
approach to enhancing Assurance 2.0 with semantic rule-based analysis
capabilities using common-sense reasoning and answer set programming solvers,
specifically s(CASP). By employing these analysis techniques, we examine the
unique semantic aspects of assurance cases, such as logical consistency,
adequacy, indefeasibility, etc. The application of these analyses provides both
system developers and evaluators with increased confidence about the assurance
case.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient and Green Large Language Models for Software Engineering:
  Vision and the Road Ahead 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04566v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04566v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieke Shi, Zhou Yang, David Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently shown remarkable capabilities in
various software engineering tasks, spurring the rapid growth of the Large
Language Models for Software Engineering (LLM4SE) area. However, limited
attention has been paid to developing efficient LLM4SE techniques that demand
minimal computational cost, time, and memory resources, as well as green LLM4SE
solutions that reduce energy consumption, water usage, and carbon emissions.
  This paper aims to redirect the focus of the research community towards the
efficiency and greenness of LLM4SE, while also sharing potential research
directions to achieve this goal. It commences with a brief overview of the
significance of LLM4SE and highlights the need for efficient and green LLM4SE
solutions. Subsequently, the paper presents a vision for a future where
efficient and green LLM4SE revolutionizes the LLM-based software engineering
tool landscape, benefiting various stakeholders, including industry, individual
practitioners, and society. The paper then delineates a roadmap for future
research, outlining specific research paths and potential solutions for the
research community to pursue. While not intended to be a definitive guide, the
paper aims to inspire further progress, with the ultimate goal of establishing
efficient and green LLM4SE as a central element in the future of software
engineering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review in the Special Issue of ACM Transactions on Software
  Engineering and Methodology (TOSEM): 2030 Software Engineering Roadmap</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TRANSAGENT: An LLM-Based Multi-Agent System for Code Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19894v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19894v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Yuan, Weitong Chen, Hanlin Wang, Kai Yu, Xin Peng, Yiling Lou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code translation converts code from one programming language to another while
maintaining its original functionality, which is crucial for software
migration, system refactoring, and cross-platform development. Traditional
rule-based methods rely on manually-written rules, which can be time-consuming
and often result in less readable code. To overcome this, learning-based
methods have been developed, leveraging parallel data to train models for
automated code translation. More recently, the advance of Large Language Models
(LLMs) further boosts learning-based code translation. Although promising,
LLM-translated program still suffers from diverse quality issues (e.g., syntax
errors and semantic errors). In particular, it can be challenging for LLMs to
self-debug these errors when simply provided with the corresponding error
messages.
  In this work, we propose a novel LLM-based multi-agent system TRANSAGENT,
which enhances LLM-based code translation by fixing the syntax errors and
semantic errors with the synergy between four LLM-based agents, including
Initial Code Translator, Syntax Error Fixer, Code Aligner, and Semantic Error
Fixer. The main insight of TRANSAGENT is to first localize the error code block
in the target program based on the execution alignment between the target and
source program, which can narrow down the fixing space and thus lower down the
fixing difficulties. To evaluate TRANSAGENT, we first construct a new benchmark
from recent programming tasks to mitigate the potential data leakage issue. On
our benchmark, TRANSAGENT outperforms the latest LLM-based code translation
technique UniTrans in both translation effectiveness and efficiency;
additionally, our evaluation on different LLMs show the generalization of
TRANSAGENT and our ablation study shows the contribution of each agent.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">16</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mechanic Maker: Accessible Game Development Via Symbolic Learning
  Program Synthesis <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Megan Sumner, Vardan Saini, Matthew Guzdial
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Game development is a highly technical practice that traditionally requires
programming skills. This serves as a barrier to entry for would-be developers
or those hoping to use games as part of their creative expression. While there
have been prior game development tools focused on accessibility, they generally
still require programming, or have major limitations in terms of the kinds of
games they can make. In this paper we introduce Mechanic Maker, a tool for
creating a wide-range of game mechanics without programming. It instead relies
on a backend symbolic learning system to synthesize game mechanics from
examples. We conducted a user study to evaluate the benefits of the tool for
participants with a variety of programming and game development experience. Our
results demonstrated that participants' ability to use the tool was unrelated
to programming ability. We conclude that tools like ours could help democratize
game development, making the practice accessible regardless of programming
skills.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures, AAAI Conference on Artificial Intelligence and
  Interactive Digital Entertainment</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Empty Spaces: Human-in-the-Loop Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Catherine Yeh, Donghao Ren, Yannick Assogba, Dominik Moritz, Fred Hohman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation is crucial to make machine learning models more robust and
safe. However, augmenting data can be challenging as it requires generating
diverse data points to rigorously evaluate model behavior on edge cases and
mitigate potential harms. Creating high-quality augmentations that cover these
"unknown unknowns" is a time- and creativity-intensive task. In this work, we
introduce Amplio, an interactive tool to help practitioners navigate "unknown
unknowns" in unstructured text datasets and improve data diversity by
systematically identifying empty data spaces to explore. Amplio includes three
human-in-the-loop data augmentation techniques: Augment With Concepts, Augment
by Interpolation, and Augment with Large Language Model. In a user study with
18 professional red teamers, we demonstrate the utility of our augmentation
methods in helping generate high-quality, diverse, and relevant model safety
prompts. We find that Amplio enabled red teamers to augment data quickly and
creatively, highlighting the transformative potential of interactive
augmentation workflows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Approach to Elicit Human-Understandable Robot Expressions to Support
  Human-Robot Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Leusmann, Steeven Villa, Thomas Liang, Chao Wang, Albrecht Schmidt, Sven Mayer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the intentions of robots is essential for natural and seamless
human-robot collaboration. Ensuring that robots have means for non-verbal
communication is a basis for intuitive and implicit interaction. For this, we
contribute an approach to elicit and design human-understandable robot
expressions. We outline the approach in the context of non-humanoid robots. We
paired human mimicking and enactment with research from gesture elicitation in
two phases: first, to elicit expressions, and second, to ensure they are
understandable. We present an example application through two studies (N=16 \&
N=260) of our approach to elicit expressions for a simple 6-DoF robotic arm. We
show that it enabled us to design robot expressions that signal curiosity and
interest in getting attention. Our main contribution is an approach to generate
and validate understandable expressions for robots, enabling more natural
human-robot interaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Human-LLM Dynamic: A Literature <span class="highlight-title">Survey</span> of LLM Use in
  Programming Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deborah Etsenake, Meiyappan Nagappan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are transforming programming practices, offering
significant capabilities for code generation activities. While researchers have
explored the potential of LLMs in various domains, this paper focuses on their
use in programming tasks, drawing insights from user studies that assess the
impact of LLMs on programming tasks. We first examined the user interaction
behaviors with LLMs observed in these studies, from the types of requests made
to task completion strategies. Additionally, our analysis reveals both benefits
and weaknesses of LLMs showing mixed effects on the human and task. Lastly, we
looked into what factors from the human, LLM or the interaction of both, affect
the human's enhancement as well as the task performance. Our findings highlight
the variability in human-LLM interactions due to the non-deterministic nature
of both parties (humans and LLMs), underscoring the need for a deeper
understanding of these interaction patterns. We conclude by providing some
practical suggestions for researchers as well as programmers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 8 tables, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "For Us By Us": Intentionally Designing Technology for Lived Black
  Experiences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Egede, Leslie Coney, Brittany Johnson, Christina N. Harrington, Denae Ford
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  HCI research to date has only scratched the surface of the unique approaches
racially minoritized communities take to building, designing, and using
technology systems. While there has been an increase in understanding how
people across racial groups create community across different platforms, there
is still a lack of studies that explicitly center on how Black technologists
design with and for their own communities. In this paper, we present findings
from a series of semi-structured interviews with Black technologists who have
used, created, or curated resources to support lived Black experiences. From
their experiences, we find a multifaceted approach to design as a means of
survival, to stay connected, for cultural significance, and to bask in
celebratory joy. Further, we provide considerations that emphasize the need for
centering lived Black experiences in design and share approaches that can
empower the broader research community to conduct further inquiries into design
focused on those in the margins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs May Not Be Human-Level Players, But They Can Be Testers: Measuring
  Game Difficulty with LLM Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Xiao, Brenda Z. Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have demonstrated their
potential as autonomous agents across various tasks. One emerging application
is the use of LLMs in playing games. In this work, we explore a practical
problem for the gaming industry: Can LLMs be used to measure game difficulty?
We propose a general game-testing framework using LLM agents and test it on two
widely played strategy games: Wordle and Slay the Spire. Our results reveal an
interesting finding: although LLMs may not perform as well as the average human
player, their performance, when guided by simple, generic prompting techniques,
shows a statistically significant and strong correlation with difficulty
indicated by human players. This suggests that LLMs could serve as effective
agents for measuring game difficulty during the development process. Based on
our experiments, we also outline general principles and guidelines for
incorporating LLMs into the game testing process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncovering the Viral Nature of Toxicity in Competitive Online Video
  Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Morrier, Amine Mahmassani, R. Michael Alvarez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Toxicity is a widespread phenomenon in competitive online video games. In
addition to its direct undesirable effects, there is a concern that toxicity
can spread to others, amplifying the harm caused by a single player's
misbehavior. In this study, we estimate whether and to what extent a player's
toxic speech spreads, causing their teammates to behave similarly. To this end,
we analyze proprietary data from the free-to-play first-person action game Call
of Duty: Warzone. We formulate and implement an instrumental variable
identification strategy that leverages the network of interactions among
players across matches. Our analysis reveals that all else equal, all of a
player's teammates engaging in toxic speech increases their probability of
engaging in similar behavior by 26.1 to 30.3 times the average player's
likelihood of engaging in toxic speech. These findings confirm the viral nature
of toxicity, especially toxic speech, in competitive online video games.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative AI and Perceptual Harms: Who's Suspected of using LLMs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kowe Kadoma, Danaë Metaxa, Mor Naaman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly integrated into a variety of
writing tasks. While these tools can help people by generating ideas or
producing higher quality work, like many other AI tools they may risk causing a
variety of harms, disproportionately burdening historically marginalized
groups. In this work, we introduce and evaluate perceptual harm, a term for the
harm caused to users when others perceive or suspect them of using AI. We
examined perceptual harms in three online experiments, each of which entailed
human participants evaluating the profiles for fictional freelance writers. We
asked participants whether they suspected the freelancers of using AI, the
quality of their writing, and whether they should be hired. We found some
support for perceptual harms against for certain demographic groups, but that
perceptions of AI use negatively impacted writing evaluations and hiring
outcomes across the board.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Gradient of Health Data Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baihan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of digital health and artificial intelligence, the management of
patient data privacy has become increasingly complex, with significant
implications for global health equity and patient trust. This paper introduces
a novel "privacy gradient" approach to health data governance, offering a more
nuanced and adaptive framework than traditional binary privacy models. Our
multidimensional concept considers factors such as data sensitivity,
stakeholder relationships, purpose of use, and temporal aspects, allowing for
context-sensitive privacy protections. Through policy analyses, ethical
considerations, and case studies spanning adolescent health, integrated care,
and genomic research, we demonstrate how this approach can address critical
privacy challenges in diverse healthcare settings worldwide. The privacy
gradient model has the potential to enhance patient engagement, improve care
coordination, and accelerate medical research while safeguarding individual
privacy rights. We provide policy recommendations for implementing this
approach, considering its impact on healthcare systems, research
infrastructures, and global health initiatives. This work aims to inform
policymakers, healthcare leaders, and digital health innovators, contributing
to a more equitable, trustworthy, and effective global health data ecosystem in
the digital age.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aligning Human and LLM Judgments: Insights from EvalAssist on
  Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahra Ashktorab, Michael Desmond, Qian Pan, James M. Johnson, Martin Santillan Cooper, Elizabeth M. Daly, Rahul Nair, Tejaswini Pedapati, Swapnaja Achintalwar, Werner Geyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluation of large language model (LLM) outputs requires users to make
critical judgments about the best outputs across various configurations. This
process is costly and takes time given the large amounts of data. LLMs are
increasingly used as evaluators to filter training data, evaluate model
performance or assist human evaluators with detailed assessments. To support
this process, effective front-end tools are critical for evaluation. Two common
approaches for using LLMs as evaluators are direct assessment and pairwise
comparison. In our study with machine learning practitioners (n=15), each
completing 6 tasks yielding 131 evaluations, we explore how task-related
factors and assessment strategies influence criteria refinement and user
perceptions. Findings show that users performed more evaluations with direct
assessment by making criteria task-specific, modifying judgments, and changing
the evaluator model. We conclude with recommendations for how systems can
better support interactions in LLM-assisted evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "I don't trust them": Exploring Perceptions of Fact-checking Entities
  for Flagging Online Misinformation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00866v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00866v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hana Habib, Sara Elsharawy, Rifat Rahman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The spread of misinformation through online social media platforms has had
substantial societal consequences. As a result, platforms have introduced
measures to alert users of news content that may be misleading or contain
inaccuracies as a means to discourage them from sharing it. These interventions
sometimes cite external sources, such as fact-checking organizations and news
outlets, for providing assessments related to the accuracy of the content.
However, it is unclear whether users trust the assessments provided by these
entities and whether perceptions vary across different topics of news. We
conducted an online study with 655 US participants to explore user perceptions
of eight categories of fact-checking entities across two misinformation topics,
as well as factors that may impact users' perceptions. We found that
participants' opinions regarding the trustworthiness and bias of the entities
varied greatly, aligning largely with their political preference. However, just
the presence of a fact-checking label appeared to discourage participants from
sharing the headlines studied. Our results hint at the need for further
exploring fact-checking entities that may be perceived as neutral, as well as
the potential for incorporating multiple assessments in such labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Google, How Should I Vote? How Users Formulate Search Queries to Find
  Political Information on Search Engines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victoria Vziatysheva, Mykola Makhortykh, Maryna Sydorova, Vihang Jumle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Search engine results depend not only on the algorithms but also on how users
interact with them. However, factors affecting the selection of a search query
remain understudied. Using a representative survey of Swiss citizens before a
round of federal popular votes, this study examines how users formulate search
queries related to the retirement policies that were voted on in March 2024.
Contrary to existing research, we find no direct evidence of selective
exposure, or users' tendency to search for pro-attitudinal information, which
we explain by the less polarizing search topics. However, we find that the
sentiment of the query is partially aligned with the expected vote outcome. Our
results also suggest that undecided and non-voters are more likely to search
for nuanced information, such as consequences and interpretations of the
policies. The perceived importance and effect of the issue, political views,
and sociodemographics also affect query formulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Photoplethysmography-Based Sleep Staging Models by Leveraging
  Temporal Context for Wearable Devices Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph A. P. Quino, Diego A. C. Cardenas, Marcelo A. F. Toledo, Felipe M. Dias, Estela Ribeiro, Jose E. Krieger, Marco A. Gutierrez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate sleep stage classification is crucial for diagnosing sleep disorders
and evaluating sleep quality. While polysomnography (PSG) remains the gold
standard, photoplethysmography (PPG) is more practical due to its affordability
and widespread use in wearable devices. However, state-of-the-art sleep staging
methods often require prolonged continuous signal acquisition, making them
impractical for wearable devices due to high energy consumption. Shorter signal
acquisitions are more feasible but less accurate. Our work proposes an adapted
sleep staging model based on top-performing state-of-the-art methods and
evaluates its performance with different PPG segment sizes. We concatenate
30-second PPG segments over 15-minute intervals to leverage longer segment
contexts. This approach achieved an accuracy of 0.75, a Cohen's Kappa of 0.60,
an F1-Weighted score of 0.74, and an F1-Macro score of 0.60. Although reducing
segment size decreased sensitivity for deep and REM stages, our strategy
outperformed single 30-second window methods, particularly for these stages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "I Want It That Way": Enabling Interactive Decision Support Using Large
  Language Models and Constraint Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06908v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06908v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Connor Lawless, Jakob Schoeffer, Lindy Le, Kael Rowan, Shilad Sen, Cristina St. Hill, Jina Suh, Bahareh Sarrafzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A critical factor in the success of decision support systems is the accurate
modeling of user preferences. Psychology research has demonstrated that users
often develop their preferences during the elicitation process, highlighting
the pivotal role of system-user interaction in developing personalized systems.
This paper introduces a novel approach, combining Large Language Models (LLMs)
with Constraint Programming to facilitate interactive decision support. We
study this hybrid framework through the lens of meeting scheduling, a
time-consuming daily activity faced by a multitude of information workers. We
conduct three studies to evaluate the novel framework, including a diary study
(n=64) to characterize contextual scheduling preferences, a quantitative
evaluation of the system's performance, and a user study (n=10) with a
prototype system. Our work highlights the potential for a hybrid LLM and
optimization approach for iterative preference elicitation and design
considerations for building systems that support human-system collaborative
decision-making processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models and Games: A <span class="highlight-title">Survey</span> and Roadmap 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18659v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18659v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberto Gallotta, Graham Todd, Marvin Zammit, Sam Earle, Antonios Liapis, Julian Togelius, Georgios N. Yannakakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen an explosive increase in research on large language
models (LLMs), and accompanying public engagement on the topic. While starting
as a niche area within natural language processing, LLMs have shown remarkable
potential across a broad range of applications and domains, including games.
This paper surveys the current state of the art across the various applications
of LLMs in and for games, and identifies the different roles LLMs can take
within a game. Importantly, we discuss underexplored areas and promising
directions for future uses of LLMs in games and we reconcile the potential and
limitations of LLMs within the games domain. As the first comprehensive survey
and roadmap at the intersection of LLMs and games, we are hopeful that this
paper will serve as the basis for groundbreaking research and innovation in
this exciting new field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the IEEE Transactions on Games (19 pages,
  6 figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Esports Training, Periodization, and Tools -- a Scoping <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrzej Białecki, Bartłomiej Michalak, Jan Gajewski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic sports (esports) and research on this emerging field are
interdisciplinary in nature. By extension, it is essential to understand how to
standardize and structure training with the help of existing tools developed by
years of research in sports sciences and informatics. Our goal in this article
was to verify if the current body of research contains substantial evidence of
the training systems applied to training esports players. To verify the
existing sources, we have applied a framework of scoping review to address the
search from multiple scientific databases with further local processing. We
conclude that the current research on esports dealt mainly with describing and
modeling performance metrics spanned over multiple fragmented research areas
(psychology, nutrition, informatics), and yet these building blocks were not
assembled into an existing well-functioning theory of performance in esports by
providing exercise regimes, and ways of periodization for esports.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Programming Languages <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conditional Nested Pattern Matching in Interaction Net 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shinya Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interaction nets are a form of restricted graph rewrite system that can serve
as a graphical or textual programming language. As such, benefits include
one-step confluence, ease of parallelism and explicit garbage collection.
However, some of these restrictions burden the programmer, so they have been
extended in several ways, notably to include data types and conditional rules.
This paper introduces a further extension to allow nested pattern matching and
to do so in a way that preserves these benefits and fundamental properties of
interaction nets. We also show that by introducing a translation to non-nested
matching, this extension is conservative in rewriting. In addition, we propose
a new notation to express this pattern matching.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings DCM 2023, arXiv:2409.19298</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthesizing Tight Privacy and Accuracy Bounds via Weighted Model
  Counting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16982v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16982v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Oakley, Steven Holtzen, Alina Oprea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Programmatically generating tight differential privacy (DP) bounds is a hard
problem. Two core challenges are (1) finding expressive, compact, and efficient
encodings of the distributions of DP algorithms, and (2) state space explosion
stemming from the multiple quantifiers and relational properties of the DP
definition.
  We address the first challenge by developing a method for tight privacy and
accuracy bound synthesis using weighted model counting on binary decision
diagrams, a state-of-the-art technique from the artificial intelligence and
automated reasoning communities for exactly computing probability
distributions. We address the second challenge by developing a framework for
leveraging inherent symmetries in DP algorithms. Our solution benefits from
ongoing research in probabilistic programming languages, allowing us to
succinctly and expressively represent different DP algorithms with approachable
language syntax that can be used by non-experts.
  We provide a detailed case study of our solution on the binary randomized
response algorithm. We also evaluate an implementation of our solution using
the Dice probabilistic programming language for the randomized response and
truncated geometric above threshold algorithms. We compare to prior work on
exact DP verification using Markov chain probabilistic model checking and the
decision procedure DiPC. Very few existing works consider mechanized analysis
of accuracy guarantees for DP algorithms. We additionally provide a detailed
analysis using our technique for finding tight accuracy bounds for DP
algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In IEEE 37th Computer Security Foundations Symposium (CSF) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Complete Inference System for Skip-free Guarded Kleene Algebra with
  Tests 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11301v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11301v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Kappé, Todd Schmid, Alexandra Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Guarded Kleene Algebra with Tests (GKAT) is a fragment of Kleene Algebra with
Tests (KAT) that was recently introduced to reason efficiently about imperative
programs. In contrast to KAT, GKAT does not have an algebraic axiomatization,
but relies on an analogue of Salomaa's axiomatization of Kleene Algebra. In
this paper, we present an algebraic axiomatization and prove two completeness
results for a large fragment of GKAT consisting of skip-free programs.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Networking and Internet Architecture <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta Learning Based Adaptive Cooperative Perception in Nonstationary
  Vehicular Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaige Qu, Zixiong Qin, Weihua Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To accommodate high network dynamics in real-time cooperative perception
(CP), reinforcement learning (RL) based adaptive CP schemes have been proposed,
to allow adaptive switchings between CP and stand-alone perception modes among
connected and autonomous vehicles. The traditional offline-training
online-execution RL framework suffers from performance degradation under
nonstationary network conditions. To achieve fast and efficient model
adaptation, we formulate a set of Markov decision processes for adaptive CP
decisions in each stationary local vehicular network (LVN). A meta RL solution
is proposed, which trains a meta RL model that captures the general features
among LVNs, thus facilitating fast model adaptation for each LVN with the meta
RL model as an initial point. Simulation results show the superiority of meta
RL in terms of the convergence speed without reward degradation. The impact of
the customization level of meta models on the model adaptation performance has
also been evaluated.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed AI Platform for the 6G RAN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ganesh Ananthanarayanan, Xenofon Foukas, Bozidar Radunovic, Yongguang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cellular Radio Access Networks (RANs) are rapidly evolving towards 6G, driven
by the need to reduce costs and introduce new revenue streams for operators and
enterprises. In this context, AI emerges as a key enabler in solving complex
RAN problems spanning both the management and application domains.
Unfortunately, and despite the undeniable promise of AI, several practical
challenges still remain, hindering the widespread adoption of AI applications
in the RAN space. This article attempts to shed light to these challenges and
argues that existing approaches in addressing them are inadequate for realizing
the vision of a truly AI-native 6G network. Motivated by this lack of
solutions, it proposes a generic distributed AI platform architecture, tailored
to the needs of an AI-native RAN and discusses its alignment with ongoing
standardization efforts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Mathematical Theory of Hyper-simplex Fractal Network for Blockchain:
  Part I 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiwen Yang, Hao Xu, Yunqing Sun, Jiacheng Qian, Zihan Zhou, Xiaoshuai Zhang, Erwu Liu, Lei Zhang, Chih-Lin I
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blockchain technology holds promise for Web 3.0, but scalability remains a
critical challenge. Here, we present a mathematical theory for a novel
blockchain network topology based on fractal N-dimensional simplexes. This
Hyper-simplex fractal network folds one-dimensional data blocks into geometric
shapes, reflecting both underlying and overlaying network connectivities. Our
approach offers near-infinite scalability, accommodating trillions of nodes
while maintaining efficiency.
  We derive the mathematical foundations for generating and describing these
network topologies, proving key properties such as node count, connectivity
patterns, and fractal dimension. The resulting structure facilitates a
hierarchical consensus mechanism and enables deterministic address mapping for
rapid routing. This theoretical framework lays the groundwork for
next-generation blockchain architectures, potentially revolutionizing
large-scale decentralized systems. The Part I work was conducted between March
and September 2024.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large-Scale Security Analysis of Real-World Backend Deployments Speaking
  IoT-Focused Protocols <span class="chip">RAID 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09662v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09662v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlotta Tagliaro, Martina Komsic, Andrea Continella, Kevin Borgolte, Martina Lindorfer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Internet-of-Things (IoT) devices, ranging from smart home assistants to
health devices, are pervasive: Forecasts estimate their number to reach 29
billion by 2030. Understanding the security of their machine-to-machine
communication is crucial. Prior work focused on identifying devices'
vulnerabilities or proposed protocol-specific solutions. Instead, we
investigate the security of backends speaking IoT protocols, that is, the
backbone of the IoT ecosystem.
  We focus on three real-world protocols for our large-scale analysis: MQTT,
CoAP, and XMPP. We gather a dataset of over 337,000 backends, augment it with
geographical and provider data, and perform non-invasive active measurements to
investigate three major security threats: information leakage, weak
authentication, and denial of service. Our results provide quantitative
evidence of a problematic immaturity in the IoT ecosystem. Among other issues,
we find that 9.44% backends expose information, 30.38% CoAP-speaking backends
are vulnerable to denial of service attacks, and 99.84% of MQTT- and
XMPP-speaking backends use insecure transport protocols (only 0.16% adopt TLS,
of which 70.93% adopt a vulnerable version).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appeared at the 27th International Symposium on Research in Attacks,
  Intrusions and Defenses (RAID 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Near-optimal pilot assignment in cell-free massive MIMO 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03547v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03547v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphael M. Guedes, José F. de Rezende, Valmir C. Barbosa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cell-free massive MIMO systems are currently being considered as potential
enablers of future (6G) technologies for wireless communications. By combining
distributed processing and massive MIMO, they are expected to deliver improved
user coverage and efficiency. A possible source of performance degradation in
such systems is pilot contamination, which contributes to causing interference
during uplink training and affects channel estimation negatively. Contamination
occurs when the same pilot sequence is assigned to more than one user. This is
in general inevitable, as the number of mutually orthogonal pilot sequences
corresponds to only a fraction of the coherence interval. We introduce a new
algorithm for pilot assignment and analyze its performance both from a
theoretical perspective and in computational experiments. We show that it has
an approximation ratio close to 1 for a plausibly large number of orthogonal
pilot sequences, as well as low computational complexity under massive
parallelism. We also show that, on average, it outperforms other methods in
terms of per-user SINR and throughput on the uplink.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version updates metadata and expands content</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Sampling under Cost for Remote Estimation of the Wiener Process
  over a Channel with Delay 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21181v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21181v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orhan T. Yavaşcan, Süleyman Çıtır, Elif Uysal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the optimal sampling of a Wiener process under
sampling and transmission costs, with the samples being forwarded to a remote
estimator over a channel with random IID delay. The goal of the estimator is to
reconstruct an estimate of the real-time signal value from causally received
samples. Our study focuses on the optimal online strategy for both sampling and
transmission, aiming to minimize the mean square estimation error. We establish
that the optimal strategy involves threshold policies for both sampling and
transmission, and we derive the optimal thresholds. We utilize Lagrange
relaxation and backward induction as our methodology, revealing the problem of
minimizing estimation error, under the assumption that sampling and
transmission times are independent of the observed Wiener process. Our
comparative analysis demonstrates that the estimation error achieved by the
optimal joint sampling and transmission policy is significantly lower than that
of age-optimal sampling, zero-wait sampling, periodic sampling, and policies
that optimize only the sampling times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In this version only sampling policy is defined on the other hand
  solution is made for joint sampling and transmission policy, therefore
  problem formulation and solution is not consistent</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Off-Path TCP Hijacking in Wi-Fi Networks: A Packet-Size Side Channel
  Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12716v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12716v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiang Wang, Xuewei Feng, Qi Li, Kun Sun, Yuxiang Yang, Mengyuan Li, Ganqiu Du, Ke Xu, Jianping Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we unveil a fundamental side channel in Wi-Fi networks,
specifically the observable frame size, which can be exploited by attackers to
conduct TCP hijacking attacks. Despite the various security mechanisms (e.g.,
WEP and WPA2/WPA3) implemented to safeguard Wi-Fi networks, our study reveals
that an off path attacker can still extract sufficient information from the
frame size side channel to hijack the victim's TCP connection. Our side channel
attack is based on two significant findings: (i) response packets (e.g., ACK
and RST) generated by TCP receivers vary in size, and (ii) the encrypted frames
containing these response packets have consistent and distinguishable sizes. By
observing the size of the victim's encrypted frames, the attacker can detect
and hijack the victim's TCP connections. We validate the effectiveness of this
side channel attack through two case studies, i.e., SSH DoS and web traffic
manipulation. Precisely, our attack can terminate the victim's SSH session in
19 seconds and inject malicious data into the victim's web traffic within 28
seconds. Furthermore, we conduct extensive measurements to evaluate the impact
of our attack on real-world Wi-Fi networks. We test 30 popular wireless routers
from 9 well-known vendors, and none of these routers can protect victims from
our attack. Besides, we implement our attack in 80 real-world Wi-Fi networks
and successfully hijack the victim's TCP connections in 75 (93.75%) evaluated
Wi-Fi networks. We have responsibly disclosed the vulnerability to the Wi-Fi
Alliance and proposed several mitigation strategies to address this issue.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">20</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Developing a BLAS library for the AMD AI Engine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tristan Laan, Tiziano De Matteis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial (dataflow) computer architectures can mitigate the control and
performance overhead of classical von Neumann architectures such as traditional
CPUs. Driven by the popularity of Machine Learning (ML) workloads, spatial
devices are being marketed as ML inference accelerators. Despite providing a
rich software ecosystem for ML practitioners, their adoption in other
scientific domains is hindered by the steep learning curve and lack of reusable
software, which makes them inaccessible to non-experts. We present our ongoing
project AIEBLAS, an open-source, expandable implementation of Basic Linear
Algebra Routines (BLAS) for the AMD AI Engine. Numerical routines are designed
to be easily reusable, customized, and composed in dataflow programs,
leveraging the characteristics of the targeted device without requiring the
user to deeply understand the underlying hardware and programming model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Data Movement in AMD Multi-GPU Systems with Infinity
  Fabric <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabin Schieffer, Ruimin Shi, Stefano Markidis, Andreas Herten, Jennifer Faj, Ivy Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern GPU systems are constantly evolving to meet the needs of
computing-intensive applications in scientific and machine learning domains.
However, there is typically a gap between the hardware capacity and the
achievable application performance. This work aims to provide a better
understanding of the Infinity Fabric interconnects on AMD GPUs and CPUs. We
propose a test and evaluation methodology for characterizing the performance of
data movements on multi-GPU systems, stressing different communication options
on AMD MI250X GPUs, including point-to-point and collective communication, and
memory allocation strategies between GPUs, as well as the host CPU. In a
single-node setup with four GPUs, we show that direct peer-to-peer memory
accesses between GPUs and utilization of the RCCL library outperform MPI-based
solutions in terms of memory/communication latency and bandwidth. Our test and
evaluation method serves as a base for validating memory and communication
strategies on a system and improving applications on AMD multi-GPU computing
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in Workshop Proceedings of The International
  Conference for High Performance Computing Networking, Storage, and Analysis
  (SC-W '24) (2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ E-MPC: Edge-assisted Model Predictive Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan-Yao Lou, Jonathan Spencer, Kwang Taik Kim, Mung Chiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model predictive control (MPC) has become the de facto standard action space
for local planning and learning-based control in many continuous robotic
control tasks, including autonomous driving. MPC solves a long-horizon cost
optimization as a series of short-horizon optimizations based on a global
planner-supplied reference path. The primary challenge in MPC, however, is that
the computational budget for re-planning has a hard limit, which frequently
inhibits exact optimization. Modern edge networks provide low-latency
communication and heterogeneous properties that can be especially beneficial in
this situation. We propose a novel framework for edge-assisted MPC (E-MPC) for
path planning that exploits the heterogeneity of edge networks in three
important ways: 1) varying computational capacity, 2) localized sensor
information, and 3) localized observation histories. Theoretical analysis and
extensive simulations are undertaken to demonstrate quantitatively the benefits
of E-MPC in various scenarios, including maps, channel dynamics, and
availability and density of edge nodes. The results confirm that E-MPC has the
potential to reduce costs by a greater percentage than standard MPC does.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supercomputer 3D Digital Twin for User Focused Real-Time Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Bergeron, Matthew Hubbell, Daniel Mojica, Albert Reuther, William Arcand, David Bestor, Daniel Burrill,  Chansup,  Byun, Vijay Gadepally, Michael Houle, Hayden Jananthan, Michael Jones, Piotr Luszczek, Peter Michaleas, Lauren Milechin, Julie Mullen Andrew Prout, Antonio Rosa, Charles Yee, Jeremy Kepner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time supercomputing performance analysis is a critical aspect of
evaluating and optimizing computational systems in a dynamic user environment.
The operation of supercomputers produce vast quantities of analytic data from
multiple sources and of varying types so compiling this data in an efficient
matter is critical to the process. MIT Lincoln Laboratory Supercomputing Center
has been utilizing the Unity 3D game engine to create a Digital Twin of our
supercomputing systems for several years to perform system monitoring. Unity
offers robust visualization capabilities making it ideal for creating a
sophisticated representation of the computational processes. As we scale the
systems to include a diversity of resources such as accelerators and the
addition of more users, we need to implement new analysis tools for the
monitoring system. The workloads in research continuously change, as does the
capability of Unity, and this allows us to adapt our monitoring tools to scale
and incorporate features enabling efficient replay of system wide events, user
isolation, and machine level granularity. Our system fully takes advantage of
the modern capabilities of the Unity Engine in a way that intuitively
represents the real time workload performed on a supercomputer. It allows HPC
system engineers to quickly diagnose usage related errors with its responsive
user interface which scales efficiently with large data sets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PARSIR: a Package for Effective Parallel Discrete Event Simulation on
  Multi-processor Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Quaglia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article we present PARSIR (PARallel SImulation Runner), a package
that enables the effective exploitation of shared-memory multi-processor
machines for running discrete event simulation models. PARSIR is a
compile/run-time environment for discrete event simulation models developed
with the {\tt C} programming language. The architecture of PARSIR has been
designed in order to keep low the amount of CPU-cycles required for running
models. This is achieved via the combination of a set of techniques like: 1)
causally consistent batch-processing of simulation events at an individual
simulation object for caching effectiveness; 2) high likelihood of disjoint
access parallelism; 3) the favoring of memory accesses on local NUMA
(Non-Uniform-Memory-Access) nodes in the architecture, while still enabling
well balanced workload distribution via work-stealing from remote nodes; 4) the
use of RMW (Read-Modify-Write) machine instructions for fast access to
simulation engine data required by the worker threads for managing the
concurrent simulation objects and distributing the workload. Furthermore, any
architectural solution embedded in the PARSIR engine is fully transparent to
the application level code implementing the simulation model. We also provide
experimental results showing the effectiveness of PARSIR when running the
reference PHOLD benchmark on a NUMA shared-memory multi-processor machine
equipped with 40 CPUs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallel state estimation for systems with integrated measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Yaghoobi, Simo Särkkä
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents parallel-in-time state estimation methods for systems
with Slow-Rate inTegrated Measurements (SRTM). Integrated measurements are
common in various applications, and they appear in analysis of data resulting
from processes that require material collection or integration over the
sampling period. Current state estimation methods for SRTM are inherently
sequential, preventing temporal parallelization in their standard form. This
paper proposes parallel Bayesian filters and smoothers for linear Gaussian SRTM
models. For that purpose, we develop a novel smoother for SRTM models and
develop parallel-in-time filters and smoother for them using an associative
scan-based parallel formulation. Empirical experiments ran on a GPU demonstrate
the superior time complexity of the proposed methods over traditional
sequential approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Mathematical Theory of Hyper-simplex Fractal Network for Blockchain:
  Part I 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiwen Yang, Hao Xu, Yunqing Sun, Jiacheng Qian, Zihan Zhou, Xiaoshuai Zhang, Erwu Liu, Lei Zhang, Chih-Lin I
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blockchain technology holds promise for Web 3.0, but scalability remains a
critical challenge. Here, we present a mathematical theory for a novel
blockchain network topology based on fractal N-dimensional simplexes. This
Hyper-simplex fractal network folds one-dimensional data blocks into geometric
shapes, reflecting both underlying and overlaying network connectivities. Our
approach offers near-infinite scalability, accommodating trillions of nodes
while maintaining efficiency.
  We derive the mathematical foundations for generating and describing these
network topologies, proving key properties such as node count, connectivity
patterns, and fractal dimension. The resulting structure facilitates a
hierarchical consensus mechanism and enables deterministic address mapping for
rapid routing. This theoretical framework lays the groundwork for
next-generation blockchain architectures, potentially revolutionizing
large-scale decentralized systems. The Part I work was conducted between March
and September 2024.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TPI-LLM: Serving 70B-scale LLMs Efficiently on Low-resource Edge Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonghang Li, Wenjiao Feng, Mohsen Guizani, Hongfang Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large model inference is shifting from cloud to edge due to concerns about
the privacy of user interaction data. However, edge devices often struggle with
limited computing power, memory, and bandwidth, requiring collaboration across
multiple devices to run and speed up LLM inference. Pipeline parallelism, the
mainstream solution, is inefficient for single-user scenarios, while tensor
parallelism struggles with frequent communications. In this paper, we argue
that tensor parallelism can be more effective than pipeline on low-resource
devices, and present a compute- and memory-efficient tensor parallel inference
system, named TPI-LLM, to serve 70B-scale models. TPI-LLM keeps sensitive raw
data local in the users' devices and introduces a sliding window memory
scheduler to dynamically manage layer weights during inference, with disk I/O
latency overlapped with the computation and communication. This allows larger
models to run smoothly on memory-limited devices. We analyze the communication
bottleneck and find that link latency, not bandwidth, emerges as the main
issue, so a star-based allreduce algorithm is implemented. Through extensive
experiments on both emulated and real testbeds, TPI-LLM demonstrated over 80%
less time-to-first-token and token latency compared to Accelerate, and over 90%
compared to Transformers and Galaxy, while cutting the peak memory footprint of
Llama 2-70B by 90%, requiring only 3.1 GB of memory for 70B-scale models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is currently under review. Find the code at
  https://github.com/Lizonghang/TPI-LLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Grained Vectorized Merge Sorting on RISC-V: From Register to Cache 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Zhang, Jincheng Zhou, Xiang Zhang, Di Ma, Chunye Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Merge sort as a divide-sort-merge paradigm has been widely applied in
computer science fields. As modern reduced instruction set computing
architectures like the fifth generation (RISC-V) regard multiple registers as a
vector register group for wide instruction parallelism, optimizing merge sort
with this vectorized property is becoming increasingly common. In this paper,
we overhaul the divide-sort-merge paradigm, from its register-level sort to the
cache-aware merge, to develop a fine-grained RISC-V vectorized merge sort
(RVMS). From the register-level view, the inline vectorized transpose
instruction is missed in RISC-V, so implementing it efficiently is non-trivial.
Besides, the vectorized comparisons do not always work well in the merging
networks. Both issues primarily stem from the expensive data shuffle
instruction. To bypass it, RVMS strides to take register data as the proxy of
data shuffle to accelerate the transpose operation, and meanwhile replaces
vectorized comparisons with scalar cousin for more light real value swap. On
the other hand, as cache-aware merge makes larger data merge in the cache, most
merge schemes have two drawbacks: the in-cache merge usually has low cache
utilization, while the out-of-cache merging network remains an ineffectively
symmetric structure. To this end, we propose the half-merge scheme to employ
the auxiliary space of in-place merge to halve the footprint of naive merge
sort, and meanwhile copy one sequence to this space to avoid the former data
exchange. Furthermore, an asymmetric merging network is developed to adapt to
two different input sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KS+: Predicting Workflow Task Memory Usage Over Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12290v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12290v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Bader, Ansgar Lößer, Lauritz Thamsen, Björn Scheuermann, Odej Kao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific workflow management systems enable the reproducible execution of
data analysis pipelines on cluster infrastructures managed by resource managers
such as Kubernetes, Slurm, or HTCondor. These resource managers require
resource estimates for each workflow task to be executed on one of the cluster
nodes. However, task resource consumption varies significantly between
different tasks and for the same task with different inputs. Furthermore,
resource consumption also fluctuates during a task's execution. As a result,
manually configuring static memory allocations is error-prone, often leading
users to overestimate memory usage to avoid costly failures from
under-provisioning, which results in significant memory wastage. We propose
KS+, a method that predicts a task's memory consumption over time depending on
its inputs. For this, KS+ dynamically segments the task execution and predicts
the memory required for each segment. Our experimental evaluation shows an
average reduction in memory wastage of 38% compared to the best-performing
state-of-the-art baseline for two real-world workflows from the popular nf-core
repository.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted in 2024 IEEE ReWorDS, eScience</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Triangle Centrality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.00110v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.00110v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Burkhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Triangle centrality is introduced for finding important vertices in a graph
based on the concentration of triangles surrounding each vertex. It has the
distinct feature of allowing a vertex to be central if it is in many triangles
or none at all.
  We show experimentally that triangle centrality is broadly applicable to many
different types of networks. Our empirical results demonstrate that 30% of the
time triangle centrality identified central vertices that differed with those
found by five well-known centrality measures, which suggests novelty without
being overly specialized. It is also asymptotically faster to compute on sparse
graphs than all but the most trivial of these other measures.
  We introduce optimal algorithms that compute triangle centrality in
$O(m\bar\delta)$ time and $O(m+n)$ space, where $\bar\delta\le O(\sqrt{m})$ is
the $\textit{average degeneracy}$ introduced by Burkhardt, Faber, and Harris
(2020). In practical applications, $\bar\delta$ is much smaller than $\sqrt{m}$
so triangle centrality can be computed in nearly linear time. On a Concurrent
Read Exclusive Write (CREW) Parallel Random Access Machine (PRAM), we give a
near work-optimal parallel algorithm that takes $O(\log n)$ time using
$O(m\sqrt{m})$ CREW PRAM processors. In MapReduce, we show it takes four rounds
using $O(m\sqrt{m})$ communication bits and is therefore optimal. We also
derive a linear algebraic formulation of triangle centrality which can be
computed in $O(m\bar\delta)$ time on sparse graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decentralized Optimization in Time-Varying Networks with Arbitrary
  Delays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19513v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19513v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomas Ortega, Hamid Jafarkhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a decentralized optimization problem for networks affected by
communication delays. Examples of such networks include collaborative machine
learning, sensor networks, and multi-agent systems. To mimic communication
delays, we add virtual non-computing nodes to the network, resulting in
directed graphs. This motivates investigating decentralized optimization
solutions on directed graphs. Existing solutions assume nodes know their
out-degrees, resulting in limited applicability. To overcome this limitation,
we introduce a novel gossip-based algorithm, called DT-GO, that does not need
to know the out-degrees. The algorithm is applicable in general directed
networks, for example networks with delays or limited acknowledgment
capabilities. We derive convergence rates for both convex and non-convex
objectives, showing that our algorithm achieves the same complexity order as
centralized Stochastic Gradient Descent. In other words, the effects of the
graph topology and delays are confined to higher-order terms. Additionally, we
extend our analysis to accommodate time-varying network topologies. Numerical
simulations are provided to support our theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2401.11344</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PeerSwap: A Peer-Sampler with Randomness Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03829v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03829v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachid Guerraoui, Anne-Marie Kermarrec, Anastasiia Kucherenko, Rafael Pinot, Martijn de Vos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of a peer-to-peer (P2P) system to effectively host decentralized
applications often relies on the availability of a peer-sampling service, which
provides each participant with a random sample of other peers. Despite the
practical effectiveness of existing peer samplers, their ability to produce
random samples within a reasonable time frame remains poorly understood from a
theoretical standpoint. This paper contributes to bridging this gap by
introducing PeerSwap, a peer-sampling protocol with provable randomness
guarantees. We establish execution time bounds for PeerSwap, demonstrating its
ability to scale effectively with the network size. We prove that PeerSwap
maintains the fixed structure of the communication graph while allowing
sequential peer position swaps within this graph. We do so by showing that
PeerSwap is a specific instance of an interchange process, a renowned model for
particle movement analysis. Leveraging this mapping, we derive execution time
bounds, expressed as a function of the network size N. Depending on the network
structure, this time can be as low as a polylogarithmic function of N,
highlighting the efficiency of PeerSwap. We implement PeerSwap and conduct
numerical evaluations using regular graphs with varying connectivity and
containing up to 32768 (2^15) peers. Our evaluation demonstrates that PeerSwap
quickly provides peers with uniform random samples of other peers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLeNS: Federated Learning with Enhanced Nesterov-Newton Sketch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunny Gupta, Mohit Jindal, Pankhi Kashyap, Pranav Jeevan, Amit Sethi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning faces a critical challenge in balancing communication
efficiency with rapid convergence, especially for second-order methods. While
Newton-type algorithms achieve linear convergence in communication rounds,
transmitting full Hessian matrices is often impractical due to quadratic
complexity. We introduce Federated Learning with Enhanced Nesterov-Newton
Sketch (FLeNS), a novel method that harnesses both the acceleration
capabilities of Nesterov's method and the dimensionality reduction benefits of
Hessian sketching. FLeNS approximates the centralized Newton's method without
relying on the exact Hessian, significantly reducing communication overhead. By
combining Nesterov's acceleration with adaptive Hessian sketching, FLeNS
preserves crucial second-order information while preserving the rapid
convergence characteristics. Our theoretical analysis, grounded in statistical
learning, demonstrates that FLeNS achieves super-linear convergence rates in
communication rounds - a notable advancement in federated optimization. We
provide rigorous convergence guarantees and characterize tradeoffs between
acceleration, sketch size, and convergence speed. Extensive empirical
evaluation validates our theoretical findings, showcasing FLeNS's
state-of-the-art performance with reduced communication requirements,
particularly in privacy-sensitive and edge-computing scenarios. The code is
available at https://github.com/sunnyinAI/FLeNS
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, 2 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Data Assimilation with Message Passing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12968v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12968v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar Key, So Takao, Daniel Giles, Marc Peter Deisenroth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data assimilation is a core component of numerical weather prediction
systems. The large quantity of data processed during assimilation requires the
computation to be distributed across increasingly many compute nodes, yet
existing approaches suffer from synchronisation overhead in this setting. In
this paper, we exploit the formulation of data assimilation as a Bayesian
inference problem and apply a message-passing algorithm to solve the spatial
inference problem. Since message passing is inherently based on local
computations, this approach lends itself to parallel and distributed
computation. In combination with a GPU-accelerated implementation, we can scale
the algorithm to very large grid sizes while retaining good accuracy and
compute and memory requirements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asynchronous Approximate Agreement with Quadratic Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05495v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05495v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mose Mizrahi Erbes, Roger Wattenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider an asynchronous network of $n$ message-sending parties, up to $t$
of which are byzantine. We study approximate agreement, where the parties
obtain approximately equal outputs in the convex hull of their inputs. In their
seminal work, Abraham, Amit and Dolev [OPODIS '04] achieve this with the
optimal resilience $t < \frac{n}{3}$ with a protocol where each party reliably
broadcasts its input every iteration. This takes $\Theta(n^2)$ messages per
reliable broadcast, or $\Theta(n^3)$ messages per iteration.
  In this work, we present optimally resilient asynchronous approximate
agreement protocols where we forgo reliable broadcast to require communication
proportional to $n^2$ instead of $n^3$. We begin with a protocol for
$\omega$-dimensional barycentric agreement with $\mathcal{O}(\omega n^2)$ small
messages that does not use reliable broadcast. Then, we achieve edge agreement
in a tree of diameter $D$ with $\lceil \log_2 D \rceil$ iterations of a
multivalued graded consensus variant. This results in a
$\mathcal{O}(\log\frac{1}{\varepsilon})$-round protocol for
$\varepsilon$-agreement in $[0, 1]$ with
$\mathcal{O}(n^2\log\frac{1}{\varepsilon})$ messages and
$\mathcal{O}(n^2\log\frac{1}{\varepsilon}\log\log\frac{1}{\varepsilon})$ bits
of communication, improving over the state of the art which matches this
complexity only when the inputs are all either $0$ or $1$. Finally, we extend
our edge agreement protocol for edge agreement in $\mathbb{Z}$ and thus
$\varepsilon$-agreement in $\mathbb{R}$ with quadratic communication, in
$\mathcal{O}(\log\frac{M}{\varepsilon})$ rounds where $M$ is the maximum honest
input magnitude.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 3 figures, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Massively parallel CMA-ES with increasing population 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11765v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11765v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Redon, Pierre Fortin, Bilel Derbel, Miwako Tsuji, Mitsuhisa Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Increasing Population Covariance Matrix Adaptation Evolution Strategy
(IPOP-CMA-ES) algorithm is a reference stochastic optimizer dedicated to
blackbox optimization, where no prior knowledge about the underlying problem
structure is available. This paper aims at accelerating IPOP-CMA-ES thanks to
high performance computing and parallelism when solving large optimization
problems. We first show how BLAS and LAPACK routines can be introduced in
linear algebra operations, and we then propose two strategies for deploying
IPOP-CMA-ES efficiently on large-scale parallel architectures with thousands of
CPU cores. The first parallel strategy processes the multiple searches in the
same ordering as the sequential IPOP-CMA-ES, while the second one processes
concurrently these multiple searches. These strategies are implemented in
MPI+OpenMP and compared on 6144 cores of the supercomputer Fugaku. We manage to
obtain substantial speedups (up to several thousand) and even super-linear
ones, and we provide an in-depth analysis of our results to understand
precisely the superior performance of our second strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Communication in Deep Learning Recommendation Model
  Training with Dual-Level Adaptive Lossy Compression <span class="chip">SC '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04272v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04272v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Feng, Boyuan Zhang, Fanjiang Ye, Min Si, Ching-Hsiang Chu, Jiannan Tian, Chunxing Yin, Summer Deng, Yuchen Hao, Pavan Balaji, Tong Geng, Dingwen Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DLRM is a state-of-the-art recommendation system model that has gained
widespread adoption across various industry applications. The large size of
DLRM models, however, necessitates the use of multiple devices/GPUs for
efficient training. A significant bottleneck in this process is the
time-consuming all-to-all communication required to collect embedding data from
all devices. To mitigate this, we introduce a method that employs error-bounded
lossy compression to reduce the communication data size and accelerate DLRM
training. We develop a novel error-bounded lossy compression algorithm,
informed by an in-depth analysis of embedding data features, to achieve high
compression ratios. Moreover, we introduce a dual-level adaptive strategy for
error-bound adjustment, spanning both table-wise and iteration-wise aspects, to
balance the compression benefits with the potential impacts on accuracy. We
further optimize our compressor for PyTorch tensors on GPUs, minimizing
compression overhead. Evaluation shows that our method achieves a 1.38$\times$
training speedup with a minimal accuracy impact.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>camera-ready version for SC '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ cuSZ-$i$: High-Ratio Scientific Lossy Compression on GPUs with Optimized
  Multi-Level Interpolation <span class="chip">SC '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05492v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05492v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyang Liu, Jiannan Tian, Shixun Wu, Sheng Di, Boyuan Zhang, Robert Underwood, Yafan Huang, Jiajun Huang, Kai Zhao, Guanpeng Li, Dingwen Tao, Zizhong Chen, Franck Cappello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Error-bounded lossy compression is a critical technique for significantly
reducing scientific data volumes. Compared to CPU-based compressors, GPU-based
compressors exhibit substantially higher throughputs, fitting better for
today's HPC applications. However, the critical limitations of existing
GPU-based compressors are their low compression ratios and qualities, severely
restricting their applicability. To overcome these, we introduce a new
GPU-based error-bounded scientific lossy compressor named cuSZ-$i$, with the
following contributions: (1) A novel GPU-optimized interpolation-based
prediction method significantly improves the compression ratio and
decompression data quality. (2) The Huffman encoding module in cuSZ-$i$ is
optimized for better efficiency. (3) cuSZ-$i$ is the first to integrate the
NVIDIA Bitcomp-lossless as an additional compression-ratio-enhancing module.
Evaluations show that cuSZ-$i$ significantly outperforms other latest GPU-based
lossy compressors in compression ratio under the same error bound (hence, the
desired quality), showcasing a 476% advantage over the second-best. This leads
to cuSZ-$i$'s optimized performance in several real-world use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>camera-ready version for SC '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A High-Quality Workflow for Multi-Resolution Scientific Data Reduction
  and Visualization <span class="chip">SC '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04267v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04267v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daoce Wang, Pascal Grosset, Jesus Pulido, Tushar M. Athawale, Jiannan Tian, Kai Zhao, Zarija Lukić, Axel Huebl, Zhe Wang, James Ahrens, Dingwen Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-resolution methods such as Adaptive Mesh Refinement (AMR) can enhance
storage efficiency for HPC applications generating vast volumes of data.
However, their applicability is limited and cannot be universally deployed
across all applications. Furthermore, integrating lossy compression with
multi-resolution techniques to further boost storage efficiency encounters
significant barriers. To this end, we introduce an innovative workflow that
facilitates high-quality multi-resolution data compression for both uniform and
AMR simulations. Initially, to extend the usability of multi-resolution
techniques, our workflow employs a compression-oriented Region of Interest
(ROI) extraction method, transforming uniform data into a multi-resolution
format. Subsequently, to bridge the gap between multi-resolution techniques and
lossy compressors, we optimize three distinct compressors, ensuring their
optimal performance on multi-resolution data. Lastly, we incorporate an
advanced uncertainty visualization method into our workflow to understand the
potential impacts of lossy compression. Experimental evaluation demonstrates
that our workflow achieves significant compression quality improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>camera-ready version for SC '24</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-10-09T05:31:06.803733066Z">
            2024-10-09 05:31:06 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
